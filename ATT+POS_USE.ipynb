{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "94944109-44ee-4916-9a57-e592d05d4253",
        "id": "nsSmrtK06JSh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.2\n",
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.2 in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (1.16.1)\n",
            "Requirement already satisfied: keras-applications==1.0.4 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (1.0.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing==1.0.2 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.2) (1.3.0)\n",
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "1a1d7ed8-6a72-45c1-a5a7-a14df4749604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import text\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 16000\n",
        "# longest output sentence\n",
        "maxlen = 5\n",
        "batch_size = 32\n",
        "deduplicate_haiku=True\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_base=\"/content/model_haiku_5_\"\n",
        "model_file=model_base + \"{}.h5\".format(int(time.time()))\n",
        "for file in glob.glob(model_base + '*'):\n",
        "    print('found: ', file)\n",
        "    model_file=file\n",
        "\n",
        "print(model_file)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "found:  /content/model_haiku_5_1562532704.h5\n",
            "/content/model_haiku_5_1562532704.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "15ab1920-9f31-4bcb-8296-9d170534a087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!date\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul  7 21:11:22 UTC 2019\n",
            "1812 N EH P\n",
            "# features:  15088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "e51dcadc-e4f9-41a2-9422-12289146aa7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "big_text = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            continue\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            if word in word2sylls:\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "        if len(_sylls) != 5:\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        big_text.append(_text)\n",
        "        big_haiku.append(_haiku)\n",
        "        big_data.append(_data)\n",
        "        #if len(big_text) == max_data * 2:\n",
        "        #    break\n",
        "\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "shuffle = np.arange(len(big_text))\n",
        "print(shuffle)\n",
        "np.random.shuffle(shuffle)\n",
        "shuffle = shuffle[0:max_data]\n",
        "big_text = big_text[shuffle]\n",
        "big_haiku = big_haiku[shuffle]\n",
        "big_data = big_data[shuffle]\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a white sink and door -> a white sink and door : [ 6885  8370  6835 11925  3535]\n",
            "[     0      1      2 ... 668545 668546 668547]\n",
            "with a boat nearby -> with a boat nearby : [11495  6885 13535 13406  4642]\n",
            "Full length clauses:  100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "45b6f311-2304-4aa4-966f-85d65ea4b84d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "big_data = np.expand_dims(big_data, -1)\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(big_text, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print(x_train[0], y_train[0])\n",
        "print(x_test[0], y_test[0])\n",
        "print(x_train[-1], y_train[-1])\n",
        "print(x_test[-1], y_test[-1])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "with heavy traffic [[11495]\n",
            " [ 6547]\n",
            " [ 1338]\n",
            " [ 6701]\n",
            " [ 8919]]\n",
            "a man with some kind of mask on next to a blue wall [[ 4121]\n",
            " [10447]\n",
            " [ 6885]\n",
            " [ 2431]\n",
            " [11213]]\n",
            "another person [[ 6885]\n",
            " [ 7114]\n",
            " [13856]\n",
            " [10984]\n",
            " [ 5346]]\n",
            "a herd of zebra [[ 6885]\n",
            " [14535]\n",
            " [ 7024]\n",
            " [ 3988]\n",
            " [13028]]\n",
            "x_train shape: (74976,)\n",
            "x_test shape: (24992,)\n",
            "y_train shape: (74976, 5, 1)\n",
            "y_test shape: (24992, 5, 1)\n",
            "[4121]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "41146b0f-abef-4bda-c49b-151fd636a68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size//3\n",
        "units=256\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "# model loading bug\n",
        "metric_list = []\n",
        "metric_names = []\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "x = layers.RepeatVector(maxlen)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "if False:\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "#x = layers.Dense(units*2, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=x)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=True\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    model_file=model_base + \"{}.h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=2,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=5)],\n",
        "          ModelCheckpoint(model_file, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 5, 256)            788480    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5, 16000)          4112000   \n",
            "=================================================================\n",
            "Total params: 4,900,480\n",
            "Trainable params: 4,900,480\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  names = metric_names + ['loss']\n",
        "  # summarize history for accuracy\n",
        "  for m in names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(names, loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "outputId": "1e764eb0-d94f-41b5-b6a2-0d7182cf29c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=x)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "x_short = x_test[0:10]\n",
        "y_short = y_test[0:10]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "  print('history: ', history)\n",
        "  predicts = model.predict(x_short, batch_size=32)\n",
        "  print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24992/24992 [==============================] - 21s 823us/step\n",
            "model.evaluate on short:  ['loss', 'sparse', 'perfect'] [2.6300575353119346, 0.4895326517684512, 0.17073463508322664]\n",
            "history:  None\n",
            "shape: (10, 5, 16000)\n",
            "5\n",
            "16000\n",
            "[[1.0114858e-09 3.0746247e-10 3.3483705e-10 ... 2.6608812e-10\n",
            "  3.1714192e-10 2.9931990e-10]\n",
            " [1.2086245e-07 4.3382371e-08 4.6079723e-08 ... 4.1182204e-08\n",
            "  4.5244953e-08 4.2868486e-08]\n",
            " [1.2839904e-07 2.4326885e-08 2.6398778e-08 ... 2.3249296e-08\n",
            "  2.5133065e-08 2.4284326e-08]\n",
            " [1.4472708e-07 2.2513087e-08 2.6005987e-08 ... 2.1366352e-08\n",
            "  2.3653508e-08 2.4029321e-08]\n",
            " [1.2009273e-06 2.9496806e-07 3.0472228e-07 ... 2.7028142e-07\n",
            "  2.9136589e-07 3.0457988e-07]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "outputId": "5709e967-eda8-433c-a3ea-22f349084fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "(mini_vals, mini_preds) = topk(predicts, top_k=5)\n",
        "print(mini_preds[0][0])\n",
        "print(mini_preds.shape)\n",
        "fs = FullSearch(5 * 5, 5, 5)\n",
        "fs.mainloop(mini_preds[0])\n",
        "sentences = decoder.get_sentences(fs.scorepaths)\n",
        "print(sentences)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6885 11495  2173  5356  7024]\n",
            "(10, 5, 5)\n",
            "[[[101352, 121836, 77027, 77027, 101352]], [[101352, 121836, 121836, 77027, 90111]], [[101352, 121836, 121836, 77027, 90111]], [[101352, 101352, 121836, 77027, 101352]], [[101352, 101352, 121836, 77027, 101352]], [[101352, 101352, 121836, 77027, 101352]], [[101352, 101352, 121836, 77027, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 121836, 101352]], [[101352, 121836, 121836, 77027, 101352]], [[101352, 121836, 121836, 77027, 101352]], [[101352, 121836, 121836, 77027, 101352]], [[101352, 121836, 121836, 77027, 101352]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a215a3a0-2291-4c75-ba7e-f09e491a96ab"
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000/100000 [==============================] - 69s 688us/step\n",
            "model.evaluate on haiku clauses:  ['loss'] 2.217849191894531\n",
            "history:  None\n",
            "100000/100000 [==============================] - 69s 689us/step\n",
            "model.evaluate on long clauses:  ['loss'] 2.5600720119476317\n",
            "history:  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c30OLFH9kvtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "944bd80d-1ee2-4eb6-d135-a6a34fc27e68"
      },
      "source": [
        "def printhistory(history):\n",
        "    print(','.join(model.metrics_names))\n",
        "    out = ''\n",
        "    for i in range(len(model.metrics_names)):\n",
        "        out += str(history[i]) + ','\n",
        "    print(out[0:-1])\n",
        "    \n",
        "printhistory(eval_small)\n",
        "printhistory(eval_big)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bc4dce3f2a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprinthistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprinthistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_big\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-bc4dce3f2a64>\u001b[0m in \u001b[0;36mprinthistory\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogOac-7KoPYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "5d1c464c-94c9-49e2-ba46-ecfb0ff4a4d9"
      },
      "source": [
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=x)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "short {'sparse': 0.5751953125000002, 'perfect': 0.2294921875, 'sparse5': 0.7501953125000003, 'perfect5': 0.4462890625}\n",
            "[0] {'sparse': 0.5050781249999992, 'perfect': 0.185546875, 'sparse5': 0.7070312500000001, 'perfect5': 0.3818359375}\n",
            "[1024] {'sparse': 0.4955078124999991, 'perfect': 0.1767578125, 'sparse5': 0.7126953125000011, 'perfect5': 0.3857421875}\n",
            "[2048] {'sparse': 0.5070312499999992, 'perfect': 0.1845703125, 'sparse5': 0.7214843750000001, 'perfect5': 0.3974609375}\n",
            "[3072] {'sparse': 0.49785156249999957, 'perfect': 0.1650390625, 'sparse5': 0.7062499999999989, 'perfect5': 0.3828125}\n",
            "[4096] {'sparse': 0.48828124999999956, 'perfect': 0.169921875, 'sparse5': 0.7105468750000007, 'perfect5': 0.3857421875}\n",
            "[5120] {'sparse': 0.5074218749999996, 'perfect': 0.173828125, 'sparse5': 0.7162109374999995, 'perfect5': 0.3916015625}\n",
            "[6144] {'sparse': 0.48554687499999927, 'perfect': 0.162109375, 'sparse5': 0.7099609374999994, 'perfect5': 0.3798828125}\n",
            "[7168] {'sparse': 0.4863281249999987, 'perfect': 0.1611328125, 'sparse5': 0.7066406250000004, 'perfect5': 0.3857421875}\n",
            "[8192] {'sparse': 0.483007812499999, 'perfect': 0.158203125, 'sparse5': 0.70625, 'perfect5': 0.375}\n",
            "[9216] {'sparse': 0.4917968749999986, 'perfect': 0.1796875, 'sparse5': 0.7148437499999996, 'perfect5': 0.4013671875}\n",
            "[10240] {'sparse': 0.5050781249999992, 'perfect': 0.16796875, 'sparse5': 0.7162109374999996, 'perfect5': 0.3935546875}\n",
            "[11264] {'sparse': 0.4843749999999992, 'perfect': 0.1669921875, 'sparse5': 0.6923828125000002, 'perfect5': 0.3623046875}\n",
            "[12288] {'sparse': 0.5074218749999995, 'perfect': 0.177734375, 'sparse5': 0.7226562499999991, 'perfect5': 0.4072265625}\n",
            "[13312] {'sparse': 0.5029296874999988, 'perfect': 0.1728515625, 'sparse5': 0.7232421875000009, 'perfect5': 0.3974609375}\n",
            "[14336] {'sparse': 0.47929687499999946, 'perfect': 0.169921875, 'sparse5': 0.6962890625000011, 'perfect5': 0.3544921875}\n",
            "[15360] {'sparse': 0.48828124999999944, 'perfect': 0.15625, 'sparse5': 0.7101562499999985, 'perfect5': 0.390625}\n",
            "[16384] {'sparse': 0.49609375000000017, 'perfect': 0.1865234375, 'sparse5': 0.7091796875000003, 'perfect5': 0.3955078125}\n",
            "[17408] {'sparse': 0.495117187499999, 'perfect': 0.1826171875, 'sparse5': 0.7085937499999998, 'perfect5': 0.3984375}\n",
            "[18432] {'sparse': 0.5058593749999996, 'perfect': 0.189453125, 'sparse5': 0.7251953125000002, 'perfect5': 0.4091796875}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-38fd57849409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiglen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[{}] {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtext5arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perfect5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-897cecd1a27e>\u001b[0m in \u001b[0;36mreport\u001b[0;34m(data, prediction)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(len(short[n]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mgood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-897cecd1a27e>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(data, prediction)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtopind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtopind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mgood\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \"\"\"\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}