{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "8049af4c-48f2-4962-c9c1-ebf8fb9471ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "5dc43012-bb76-4a5b-cb79-531f5d965762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "7a9d1e73-2b85-4d0c-8924-9ed806ecdfbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 35000\n",
        "# longest output sentence\n",
        "max_len = 5\n",
        "# longest input sentence\n",
        "max_words = 7\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=True\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "c4c18bc2-b403-4312-dd0f-17a7b4b6099a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!date\n",
        "word2sylls = trim_homynyms(word2sylls)\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 15 04:04:50 UTC 2019\n",
            "2443 0\n",
            "# features:  15098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "35f96577-92de-493f-c01b-8c79b8d94ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "big_text = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "textwordset = set()\n",
        "haikuwordset = set()\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            continue\n",
        "        words = list(text.text_to_word_sequence(_haiku))\n",
        "        if len(words) > max_words:\n",
        "            continue\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            if word in word2sylls:\n",
        "                haikuwordset.add(word)\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "        for textword in list(text.text_to_word_sequence(_text)):\n",
        "            textwordset.add(textword)\n",
        "        if len(_sylls) != 5:\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        big_text.append(_text)\n",
        "        big_haiku.append(_haiku)\n",
        "        big_data.append(_data)\n",
        "        #if len(big_text) == max_data * 2:\n",
        "        #    break\n",
        "\n",
        "        \n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "big_data = np.expand_dims(big_data, -1)\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "shuffle = np.arange(len(big_text))\n",
        "print(shuffle)\n",
        "np.random.shuffle(shuffle)\n",
        "shuffle = shuffle[0:max_data]\n",
        "big_text = big_text[shuffle]\n",
        "big_haiku = big_haiku[shuffle]\n",
        "big_data = big_data[shuffle]\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "office equipment -> office equipment : [[ 211]\n",
            " [2732]\n",
            " [4968]\n",
            " [6573]\n",
            " [7383]]\n",
            "[    0     1     2 ... 49711 49712 49713]\n",
            "this motorcycle is up on a table so it's easy to work on -> this motorcycle : [[ 2461]\n",
            " [ 7685]\n",
            " [12823]\n",
            " [ 9899]\n",
            " [ 5519]]\n",
            "Full length clauses:  49714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "dd73c32b-83db-47d5-9b21-2d7dcaf6b96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(big_haiku, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print(x_test[0], y_test[0])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "their motorcycles [[ 2443]\n",
            " [ 7685]\n",
            " [12823]\n",
            " [ 9899]\n",
            " [ 5525]]\n",
            "x_train shape: (37280,)\n",
            "x_test shape: (12416,)\n",
            "y_train shape: (37280, 5, 1)\n",
            "y_test shape: (12416, 5, 1)\n",
            "[2443]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "dda93ec5-5d42-41f6-c564-6242110b46ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "def sparse_loss(y_true, y_pred):\n",
        "    return scc(y_true, y_pred)\n",
        "\n",
        "def perfect_loss(y_true, y_pred):\n",
        "    return scct(y_true, y_pred, scale=1.0)\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.0\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.RepeatVector(max_len)(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', sparse_loss, metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=75,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0715 04:05:18.342360 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0715 04:05:18.344087 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0715 04:05:19.839753 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0715 04:05:20.567161 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0715 04:05:20.591366 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras_stuff/loss.py:58: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_1 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5, 35000)          23905000  \n",
            "=================================================================\n",
            "Total params: 26,618,770\n",
            "Trainable params: 26,618,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0715 04:05:23.793616 140449186654080 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0715 04:05:24.105883 140449186654080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 37280 samples, validate on 12416 samples\n",
            "Epoch 1/75\n",
            " - 76s - loss: 3.4291 - sparse: 0.4094 - perfect: 0.1948 - val_loss: 1.9917 - val_sparse: 0.6157 - val_perfect: 0.3520\n",
            "\n",
            "Epoch 00001: val_perfect improved from -inf to 0.35205, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 2/75\n",
            " - 71s - loss: 1.4642 - sparse: 0.6991 - perfect: 0.4337 - val_loss: 1.2416 - val_sparse: 0.7420 - val_perfect: 0.4853\n",
            "\n",
            "Epoch 00002: val_perfect improved from 0.35205 to 0.48534, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 3/75\n",
            " - 71s - loss: 0.8848 - sparse: 0.8050 - perfect: 0.5532 - val_loss: 0.9014 - val_sparse: 0.8082 - val_perfect: 0.5656\n",
            "\n",
            "Epoch 00003: val_perfect improved from 0.48534 to 0.56564, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 4/75\n",
            " - 71s - loss: 0.5827 - sparse: 0.8685 - perfect: 0.6460 - val_loss: 0.7044 - val_sparse: 0.8532 - val_perfect: 0.6269\n",
            "\n",
            "Epoch 00004: val_perfect improved from 0.56564 to 0.62685, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 5/75\n",
            " - 71s - loss: 0.4166 - sparse: 0.9061 - perfect: 0.7164 - val_loss: 0.5863 - val_sparse: 0.8820 - val_perfect: 0.6902\n",
            "\n",
            "Epoch 00005: val_perfect improved from 0.62685 to 0.69016, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 6/75\n",
            " - 71s - loss: 0.3171 - sparse: 0.9295 - perfect: 0.7720 - val_loss: 0.5361 - val_sparse: 0.8994 - val_perfect: 0.7299\n",
            "\n",
            "Epoch 00006: val_perfect improved from 0.69016 to 0.72995, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 7/75\n",
            " - 71s - loss: 0.2671 - sparse: 0.9405 - perfect: 0.7971 - val_loss: 0.4843 - val_sparse: 0.9114 - val_perfect: 0.7539\n",
            "\n",
            "Epoch 00007: val_perfect improved from 0.72995 to 0.75395, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 8/75\n",
            " - 71s - loss: 0.2127 - sparse: 0.9541 - perfect: 0.8384 - val_loss: 0.4375 - val_sparse: 0.9237 - val_perfect: 0.7883\n",
            "\n",
            "Epoch 00008: val_perfect improved from 0.75395 to 0.78834, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 9/75\n",
            " - 71s - loss: 0.1808 - sparse: 0.9618 - perfect: 0.8621 - val_loss: 0.4452 - val_sparse: 0.9234 - val_perfect: 0.7874\n",
            "\n",
            "Epoch 00009: val_perfect did not improve from 0.78834\n",
            "Epoch 10/75\n",
            " - 70s - loss: 0.1684 - sparse: 0.9644 - perfect: 0.8700 - val_loss: 0.4227 - val_sparse: 0.9286 - val_perfect: 0.8018\n",
            "\n",
            "Epoch 00010: val_perfect improved from 0.78834 to 0.80179, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 11/75\n",
            " - 71s - loss: 0.1460 - sparse: 0.9701 - perfect: 0.8873 - val_loss: 0.3848 - val_sparse: 0.9420 - val_perfect: 0.8453\n",
            "\n",
            "Epoch 00011: val_perfect improved from 0.80179 to 0.84528, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 12/75\n",
            " - 71s - loss: 0.1430 - sparse: 0.9705 - perfect: 0.8907 - val_loss: 0.4043 - val_sparse: 0.9382 - val_perfect: 0.8333\n",
            "\n",
            "Epoch 00012: val_perfect did not improve from 0.84528\n",
            "Epoch 13/75\n",
            " - 71s - loss: 0.1203 - sparse: 0.9762 - perfect: 0.9096 - val_loss: 0.3870 - val_sparse: 0.9408 - val_perfect: 0.8436\n",
            "\n",
            "Epoch 00013: val_perfect did not improve from 0.84528\n",
            "Epoch 14/75\n",
            " - 71s - loss: 0.1180 - sparse: 0.9772 - perfect: 0.9133 - val_loss: 0.3822 - val_sparse: 0.9443 - val_perfect: 0.8627\n",
            "\n",
            "Epoch 00014: val_perfect improved from 0.84528 to 0.86268, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 15/75\n",
            " - 71s - loss: 0.1100 - sparse: 0.9791 - perfect: 0.9204 - val_loss: 0.3689 - val_sparse: 0.9472 - val_perfect: 0.8680\n",
            "\n",
            "Epoch 00015: val_perfect improved from 0.86268 to 0.86799, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 16/75\n",
            " - 71s - loss: 0.1058 - sparse: 0.9802 - perfect: 0.9241 - val_loss: 0.3666 - val_sparse: 0.9499 - val_perfect: 0.8777\n",
            "\n",
            "Epoch 00016: val_perfect improved from 0.86799 to 0.87774, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 17/75\n",
            " - 71s - loss: 0.0921 - sparse: 0.9834 - perfect: 0.9372 - val_loss: 0.3552 - val_sparse: 0.9541 - val_perfect: 0.8944\n",
            "\n",
            "Epoch 00017: val_perfect improved from 0.87774 to 0.89441, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 18/75\n",
            " - 71s - loss: 0.0883 - sparse: 0.9846 - perfect: 0.9421 - val_loss: 0.4079 - val_sparse: 0.9428 - val_perfect: 0.8511\n",
            "\n",
            "Epoch 00018: val_perfect did not improve from 0.89441\n",
            "Epoch 19/75\n",
            " - 71s - loss: 0.0996 - sparse: 0.9822 - perfect: 0.9325 - val_loss: 0.3620 - val_sparse: 0.9542 - val_perfect: 0.8931\n",
            "\n",
            "Epoch 00019: val_perfect did not improve from 0.89441\n",
            "Epoch 20/75\n",
            " - 71s - loss: 0.0890 - sparse: 0.9848 - perfect: 0.9427 - val_loss: 0.3427 - val_sparse: 0.9573 - val_perfect: 0.9040\n",
            "\n",
            "Epoch 00020: val_perfect improved from 0.89441 to 0.90399, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 21/75\n",
            " - 71s - loss: 0.0779 - sparse: 0.9874 - perfect: 0.9530 - val_loss: 0.3640 - val_sparse: 0.9533 - val_perfect: 0.8934\n",
            "\n",
            "Epoch 00021: val_perfect did not improve from 0.90399\n",
            "Epoch 22/75\n",
            " - 71s - loss: 0.0864 - sparse: 0.9848 - perfect: 0.9416 - val_loss: 0.3614 - val_sparse: 0.9550 - val_perfect: 0.8980\n",
            "\n",
            "Epoch 00022: val_perfect did not improve from 0.90399\n",
            "Epoch 23/75\n",
            " - 71s - loss: 0.0763 - sparse: 0.9872 - perfect: 0.9501 - val_loss: 0.3512 - val_sparse: 0.9584 - val_perfect: 0.9100\n",
            "\n",
            "Epoch 00023: val_perfect improved from 0.90399 to 0.91004, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 24/75\n",
            " - 71s - loss: 0.0878 - sparse: 0.9843 - perfect: 0.9404 - val_loss: 0.3405 - val_sparse: 0.9597 - val_perfect: 0.9145\n",
            "\n",
            "Epoch 00024: val_perfect improved from 0.91004 to 0.91455, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 25/75\n",
            " - 71s - loss: 0.0637 - sparse: 0.9911 - perfect: 0.9677 - val_loss: 0.3430 - val_sparse: 0.9601 - val_perfect: 0.9168\n",
            "\n",
            "Epoch 00025: val_perfect improved from 0.91455 to 0.91680, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 26/75\n",
            " - 71s - loss: 0.0693 - sparse: 0.9901 - perfect: 0.9633 - val_loss: 0.3550 - val_sparse: 0.9584 - val_perfect: 0.9076\n",
            "\n",
            "Epoch 00026: val_perfect did not improve from 0.91680\n",
            "Epoch 27/75\n",
            " - 71s - loss: 0.0817 - sparse: 0.9860 - perfect: 0.9451 - val_loss: 0.3793 - val_sparse: 0.9556 - val_perfect: 0.8992\n",
            "\n",
            "Epoch 00027: val_perfect did not improve from 0.91680\n",
            "Epoch 28/75\n",
            " - 71s - loss: 0.0664 - sparse: 0.9903 - perfect: 0.9623 - val_loss: 0.3683 - val_sparse: 0.9547 - val_perfect: 0.8936\n",
            "\n",
            "Epoch 00028: val_perfect did not improve from 0.91680\n",
            "Epoch 29/75\n",
            " - 71s - loss: 0.0656 - sparse: 0.9902 - perfect: 0.9635 - val_loss: 0.3552 - val_sparse: 0.9595 - val_perfect: 0.9157\n",
            "\n",
            "Epoch 00029: val_perfect did not improve from 0.91680\n",
            "Epoch 30/75\n",
            " - 71s - loss: 0.0692 - sparse: 0.9887 - perfect: 0.9559 - val_loss: 0.3646 - val_sparse: 0.9568 - val_perfect: 0.8992\n",
            "\n",
            "Epoch 00030: val_perfect did not improve from 0.91680\n",
            "Epoch 31/75\n",
            " - 71s - loss: 0.0659 - sparse: 0.9899 - perfect: 0.9608 - val_loss: 0.3606 - val_sparse: 0.9595 - val_perfect: 0.9154\n",
            "\n",
            "Epoch 00031: val_perfect did not improve from 0.91680\n",
            "Epoch 32/75\n",
            " - 71s - loss: 0.0690 - sparse: 0.9900 - perfect: 0.9627 - val_loss: 0.3590 - val_sparse: 0.9585 - val_perfect: 0.9072\n",
            "\n",
            "Epoch 00032: val_perfect did not improve from 0.91680\n",
            "Epoch 33/75\n",
            " - 71s - loss: 0.0689 - sparse: 0.9891 - perfect: 0.9570 - val_loss: 0.3450 - val_sparse: 0.9620 - val_perfect: 0.9209\n",
            "\n",
            "Epoch 00033: val_perfect improved from 0.91680 to 0.92091, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 34/75\n",
            " - 71s - loss: 0.0583 - sparse: 0.9919 - perfect: 0.9687 - val_loss: 0.3597 - val_sparse: 0.9594 - val_perfect: 0.9127\n",
            "\n",
            "Epoch 00034: val_perfect did not improve from 0.92091\n",
            "Epoch 35/75\n",
            " - 71s - loss: 0.0662 - sparse: 0.9894 - perfect: 0.9585 - val_loss: 0.4220 - val_sparse: 0.9446 - val_perfect: 0.8541\n",
            "\n",
            "Epoch 00035: val_perfect did not improve from 0.92091\n",
            "Epoch 36/75\n",
            " - 71s - loss: 0.0669 - sparse: 0.9891 - perfect: 0.9559 - val_loss: 0.3555 - val_sparse: 0.9598 - val_perfect: 0.9144\n",
            "\n",
            "Epoch 00036: val_perfect did not improve from 0.92091\n",
            "Epoch 37/75\n",
            " - 71s - loss: 0.0530 - sparse: 0.9932 - perfect: 0.9725 - val_loss: 0.3557 - val_sparse: 0.9621 - val_perfect: 0.9255\n",
            "\n",
            "Epoch 00037: val_perfect improved from 0.92091 to 0.92550, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 38/75\n",
            " - 71s - loss: 0.0616 - sparse: 0.9921 - perfect: 0.9707 - val_loss: 0.3943 - val_sparse: 0.9548 - val_perfect: 0.9041\n",
            "\n",
            "Epoch 00038: val_perfect did not improve from 0.92550\n",
            "Epoch 39/75\n",
            " - 71s - loss: 0.0642 - sparse: 0.9906 - perfect: 0.9620 - val_loss: 0.3528 - val_sparse: 0.9620 - val_perfect: 0.9197\n",
            "\n",
            "Epoch 00039: val_perfect did not improve from 0.92550\n",
            "Epoch 40/75\n",
            " - 71s - loss: 0.0687 - sparse: 0.9890 - perfect: 0.9551 - val_loss: 0.3620 - val_sparse: 0.9608 - val_perfect: 0.9168\n",
            "\n",
            "Epoch 00040: val_perfect did not improve from 0.92550\n",
            "Epoch 41/75\n",
            " - 71s - loss: 0.0499 - sparse: 0.9940 - perfect: 0.9755 - val_loss: 0.3504 - val_sparse: 0.9638 - val_perfect: 0.9294\n",
            "\n",
            "Epoch 00041: val_perfect improved from 0.92550 to 0.92945, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 42/75\n",
            " - 71s - loss: 0.0574 - sparse: 0.9924 - perfect: 0.9701 - val_loss: 0.3631 - val_sparse: 0.9611 - val_perfect: 0.9204\n",
            "\n",
            "Epoch 00042: val_perfect did not improve from 0.92945\n",
            "Epoch 43/75\n",
            " - 70s - loss: 0.0603 - sparse: 0.9916 - perfect: 0.9676 - val_loss: 0.3652 - val_sparse: 0.9605 - val_perfect: 0.9179\n",
            "\n",
            "Epoch 00043: val_perfect did not improve from 0.92945\n",
            "Epoch 44/75\n",
            " - 71s - loss: 0.0537 - sparse: 0.9929 - perfect: 0.9722 - val_loss: 0.3633 - val_sparse: 0.9620 - val_perfect: 0.9220\n",
            "\n",
            "Epoch 00044: val_perfect did not improve from 0.92945\n",
            "Epoch 45/75\n",
            " - 71s - loss: 0.0607 - sparse: 0.9915 - perfect: 0.9666 - val_loss: 0.3663 - val_sparse: 0.9608 - val_perfect: 0.9158\n",
            "\n",
            "Epoch 00045: val_perfect did not improve from 0.92945\n",
            "Epoch 46/75\n",
            " - 71s - loss: 0.0635 - sparse: 0.9902 - perfect: 0.9593 - val_loss: 0.3921 - val_sparse: 0.9525 - val_perfect: 0.8802\n",
            "\n",
            "Epoch 00046: val_perfect did not improve from 0.92945\n",
            "Epoch 47/75\n",
            " - 71s - loss: 0.0525 - sparse: 0.9926 - perfect: 0.9690 - val_loss: 0.3472 - val_sparse: 0.9644 - val_perfect: 0.9315\n",
            "\n",
            "Epoch 00047: val_perfect improved from 0.92945 to 0.93154, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 48/75\n",
            " - 71s - loss: 0.0468 - sparse: 0.9951 - perfect: 0.9810 - val_loss: 0.3560 - val_sparse: 0.9633 - val_perfect: 0.9274\n",
            "\n",
            "Epoch 00048: val_perfect did not improve from 0.93154\n",
            "Epoch 49/75\n",
            " - 71s - loss: 0.0549 - sparse: 0.9930 - perfect: 0.9725 - val_loss: 0.3777 - val_sparse: 0.9592 - val_perfect: 0.9136\n",
            "\n",
            "Epoch 00049: val_perfect did not improve from 0.93154\n",
            "Epoch 50/75\n",
            " - 71s - loss: 0.0638 - sparse: 0.9905 - perfect: 0.9611 - val_loss: 0.3651 - val_sparse: 0.9607 - val_perfect: 0.9191\n",
            "\n",
            "Epoch 00050: val_perfect did not improve from 0.93154\n",
            "Epoch 51/75\n",
            " - 71s - loss: 0.0510 - sparse: 0.9935 - perfect: 0.9725 - val_loss: 0.3487 - val_sparse: 0.9637 - val_perfect: 0.9304\n",
            "\n",
            "Epoch 00051: val_perfect did not improve from 0.93154\n",
            "Epoch 52/75\n",
            " - 71s - loss: 0.0547 - sparse: 0.9926 - perfect: 0.9694 - val_loss: 0.3463 - val_sparse: 0.9648 - val_perfect: 0.9339\n",
            "\n",
            "Epoch 00052: val_perfect improved from 0.93154 to 0.93388, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 53/75\n",
            " - 71s - loss: 0.0504 - sparse: 0.9945 - perfect: 0.9775 - val_loss: 0.3565 - val_sparse: 0.9630 - val_perfect: 0.9278\n",
            "\n",
            "Epoch 00053: val_perfect did not improve from 0.93388\n",
            "Epoch 54/75\n",
            " - 71s - loss: 0.0526 - sparse: 0.9936 - perfect: 0.9742 - val_loss: 0.3627 - val_sparse: 0.9631 - val_perfect: 0.9290\n",
            "\n",
            "Epoch 00054: val_perfect did not improve from 0.93388\n",
            "Epoch 55/75\n",
            " - 71s - loss: 0.0563 - sparse: 0.9929 - perfect: 0.9715 - val_loss: 0.3645 - val_sparse: 0.9619 - val_perfect: 0.9236\n",
            "\n",
            "Epoch 00055: val_perfect did not improve from 0.93388\n",
            "Epoch 56/75\n",
            " - 71s - loss: 0.0554 - sparse: 0.9933 - perfect: 0.9741 - val_loss: 0.3648 - val_sparse: 0.9622 - val_perfect: 0.9249\n",
            "\n",
            "Epoch 00056: val_perfect did not improve from 0.93388\n",
            "Epoch 57/75\n",
            " - 71s - loss: 0.0589 - sparse: 0.9916 - perfect: 0.9650 - val_loss: 0.3589 - val_sparse: 0.9641 - val_perfect: 0.9280\n",
            "\n",
            "Epoch 00057: val_perfect did not improve from 0.93388\n",
            "Epoch 58/75\n",
            " - 70s - loss: 0.0448 - sparse: 0.9951 - perfect: 0.9799 - val_loss: 0.3764 - val_sparse: 0.9617 - val_perfect: 0.9207\n",
            "\n",
            "Epoch 00058: val_perfect did not improve from 0.93388\n",
            "Epoch 59/75\n",
            " - 71s - loss: 0.0503 - sparse: 0.9936 - perfect: 0.9734 - val_loss: 0.3690 - val_sparse: 0.9618 - val_perfect: 0.9215\n",
            "\n",
            "Epoch 00059: val_perfect did not improve from 0.93388\n",
            "Epoch 60/75\n",
            " - 71s - loss: 0.0559 - sparse: 0.9926 - perfect: 0.9685 - val_loss: 0.3700 - val_sparse: 0.9621 - val_perfect: 0.9195\n",
            "\n",
            "Epoch 00060: val_perfect did not improve from 0.93388\n",
            "Epoch 61/75\n",
            " - 71s - loss: 0.0524 - sparse: 0.9931 - perfect: 0.9704 - val_loss: 0.3631 - val_sparse: 0.9642 - val_perfect: 0.9288\n",
            "\n",
            "Epoch 00061: val_perfect did not improve from 0.93388\n",
            "Epoch 62/75\n",
            " - 71s - loss: 0.0431 - sparse: 0.9957 - perfect: 0.9823 - val_loss: 0.3694 - val_sparse: 0.9635 - val_perfect: 0.9313\n",
            "\n",
            "Epoch 00062: val_perfect did not improve from 0.93388\n",
            "Epoch 00062: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "8aa33d80-9c73-4f3d-9d0f-e0d9600e70bc"
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "5b89e67d-3d34-4d69-8054-745ee0b98445"
      },
      "source": [
        "\n",
        "\n",
        "x_short = big_text[0:500]\n",
        "y_short = big_data[0:500]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "    #K.manual_variable_initialization(True)  \n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # need this?\n",
        "    session.run(tf.tables_initializer())\n",
        "    #model = load_model(model_file)  \n",
        "    model.load_weights(model_file)  \n",
        "    eval = model.evaluate(x_train, y_train)\n",
        "    print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "    predicts = model.predict(x_short, batch_size=32)\n",
        "    print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   96/37280 [..............................] - ETA: 6:05 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fbc7009f9b0>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37280/37280 [==============================] - 32s 858us/step\n",
            "model.evaluate on short:  ['loss', 'sparse', 'perfect'] [0.04819829654180377, 0.9953808962019728, 0.9822424892703863]\n",
            "shape: (500, 5, 35000)\n",
            "5\n",
            "35000\n",
            "[[4.1623979e-12 4.6073284e-12 3.7596818e-12 ... 4.7173168e-12\n",
            "  3.3191861e-12 4.2004490e-12]\n",
            " [1.4523964e-11 1.6287334e-11 1.4776987e-11 ... 1.3450083e-11\n",
            "  1.5727921e-11 1.8178743e-11]\n",
            " [5.6683849e-11 5.5600947e-11 4.6750735e-11 ... 4.9953250e-11\n",
            "  4.8861831e-11 5.5308709e-11]\n",
            " [2.0887402e-10 2.6223415e-10 1.9486138e-10 ... 2.7913563e-10\n",
            "  1.9245568e-10 2.3429217e-10]\n",
            " [6.5210450e-12 5.3756079e-12 5.6565551e-12 ... 4.8422872e-12\n",
            "  5.3085743e-12 5.6953556e-12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmYm4s4AJ6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y_short)):\n",
        "    rep = report([y_short[i]], [predicts[i]])\n",
        "    if rep['perfect5'] == 1.0:\n",
        "        f = find_top_k_match(y_short[i], predicts[i], 5)\n",
        "        print('[{}]: {} -> {}'.format(i, x_short[i], [decoder.idx2syll[j] for j in f]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBZfxMLgpRA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k=5\n",
        "\n",
        "fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "def decodem(predict, top_k):\n",
        "    global fs\n",
        "    (top_vals, top_paths) = get_top_k(np.array([predict]), top_k=top_k)\n",
        "    #print('top_paths: ' + str(top_paths))\n",
        "    #print(top_paths.shape)\n",
        "    fs.mainloop(top_paths[0])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print('scorepaths[{}]: {}'.format(j, fs.scorepaths[j]))\n",
        "        #print('predict.shape: ', predict.shape)\n",
        "        #print('top_paths.shape: ', top_paths.shape)\n",
        "        #print('top_paths[{}]: {}'.format(j, top_paths))\n",
        "        #print('top_paths[{}][]: {}'.format(j, top_paths[0][np.arange(max_len), fs.scorepaths[j]]))\n",
        "        morepaths[j] = top_paths[0][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print('morepaths: ' + str(morepaths))\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    sentences = {}\n",
        "    if len(encoded) > 0:\n",
        "        #print(encoded)\n",
        "        decoded = []\n",
        "        for e1 in encoded:\n",
        "            if len(e1) > 0 and len(e1[0]) > 0:\n",
        "                dec = decoder.decode_sentences([e1])\n",
        "                decoded.append(dec)\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    for d4 in d3:\n",
        "                        go = True\n",
        "                        for w in d4:\n",
        "                            if not w in haikuwordset:\n",
        "                                go = False\n",
        "                        if go:\n",
        "                            key = ' '.join(d4)\n",
        "                            sentences[key] = d4\n",
        "                    #print('d3: ', d3)\n",
        "                    #key = ' '.join(d3)\n",
        "                    #sentences[key] = d3\n",
        "    return sentences\n",
        "\n",
        "# return N possible sentences with the fewest words\n",
        "def short_sentences(sentences):\n",
        "    out = {}\n",
        "    for i in range(1, max_len + 1):\n",
        "        for (k, v) in sentences.items():\n",
        "            if len(v) == i:\n",
        "                out[k] = v\n",
        "        if len(out) > 0:\n",
        "            return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e706c52d-25b3-45d2-ca77-54036578f4cb"
      },
      "source": [
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          sentences = decodem(predicts[j], 5)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences):\n",
        "                    print('{} -> {}'.format(x_test[i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fbc2a06f048>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "on his motorbike -> through some motorbikes\n",
            "on his motorbike -> through some motorbike\n",
            "wildlife animal -> wildlife ewe species\n",
            "fruit and vegetables -> pears shelves vegetables\n",
            "yellow equipment -> yellow ewe t tools\n",
            "yellow equipment -> yellow ewe t t\n",
            "some people sitting -> small people sitting\n",
            "some people sitting -> small people setting\n",
            "some people sitting -> some people sitting\n",
            "white and yellow dress -> yellow yellow toe\n",
            "multiple busses -> multiple laces\n",
            "another language -> setter sta making\n",
            "orange and yellow -> yellow white yellow\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "several glass displays -> several shops glass stands\n",
            "separate leg rest -> separate leg rest\n",
            "at west union street -> right west union street\n",
            "another language -> setter sta making\n",
            "black and white picture -> white e white pictures\n",
            "black and white picture -> white e white picture\n",
            "assorted objects -> some sorted pieces\n",
            "and sweet potato -> that three potatoes\n",
            "and sweet potato -> that three potato\n",
            "and sweet potato -> that sweet potatoes\n",
            "and sweet potato -> that sweet potato\n",
            "and sweet potato -> big three potato\n",
            "and sweet potato -> as three potatoes\n",
            "and sweet potato -> as three potato\n",
            "and sweet potato -> and three potatoes\n",
            "and sweet potato -> and three potato\n",
            "and sweet potato -> on three potatoes\n",
            "and sweet potato -> on three potato\n",
            "and sweet potato -> big sweet potatoes\n",
            "and sweet potato -> big sweet potato\n",
            "and sweet potato -> that some potatoes\n",
            "and sweet potato -> that some potato\n",
            "and sweet potato -> as sweet potatoes\n",
            "and sweet potato -> as sweet potato\n",
            "and sweet potato -> and sweet potatoes\n",
            "and sweet potato -> and sweet potato\n",
            "this messy pizza -> some thick style pizza\n",
            "on walking through leaves -> through walking through tops\n",
            "on walking through leaves -> through walking tree tops\n",
            "upright position -> low right position\n",
            "some transit buses -> some transit pieces\n",
            "colorful bluebird -> patty cover has\n",
            "black and white picture -> white e white pictures\n",
            "black and white picture -> white e white picture\n",
            "green motorcycle -> white toy motor top\n",
            "that muddy water -> some water water\n",
            "that muddy water -> some pretty water\n",
            "several used toilets -> several used toilets\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "watch television -> watch television\n",
            "at baseball practice -> during they practice\n",
            "at baseball practice -> during their practice\n",
            "at baseball practice -> during base practice\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "dry vegetation -> thick run ware run ware\n",
            "steam locomotive -> letter steam motive\n",
            "eating broccoli -> sitting cutting tick\n",
            "eating broccoli -> sitting cutting sprouts\n",
            "eating broccoli -> sitting ca leaving\n",
            "eating broccoli -> eating cutting tick\n",
            "eating broccoli -> preening cutting tick\n",
            "that looks delicious -> straw looks delicious\n",
            "that looks delicious -> straw has delicious\n",
            "family pictures -> toy toy par couple\n",
            "family pictures -> some toy par couple\n",
            "family pictures -> toy toy par pictures\n",
            "family pictures -> toy toy parka friends\n",
            "family pictures -> some toy par pictures\n",
            "various people -> vary some places\n",
            "various people -> vary some pieces\n",
            "even big giraffes -> even small giraffes\n",
            "his basketball skills -> his skate trick play skills\n",
            "his basketball skills -> his skate trick play skier\n",
            "small water closet -> small water small tub\n",
            "small water closet -> some water small tub\n",
            "small water closet -> large water small tub\n",
            "different pastries -> pace pastries pastries\n",
            "different pastries -> pace pastries desserts\n",
            "going i. right now -> sitting pointing stands\n",
            "going i. right now -> sitting pointing now\n",
            "going i. right now -> sitting pay weighing\n",
            "going i. right now -> sitting now weighing\n",
            "going i. right now -> getting pointing stands\n",
            "going i. right now -> sitting tribal stands\n",
            "this kitchen cooking -> some kitchen mic watch\n",
            "this kitchen cooking -> kitchen meal mic watch\n",
            "oranges and limes -> walling run run limes\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "various stages -> varying stating\n",
            "some people and cars -> three people train cars\n",
            "cartoon characters -> small pay characters\n",
            "silver and yellow -> yellowstone yellow\n",
            "on pillows on bed -> lay pillows pillows\n",
            "on pillows on bed -> lay pillows pickles\n",
            "yellow and white bird -> yellow white white dog\n",
            "yellow and white bird -> white yellow white dog\n",
            "behind each other -> between each ware ware\n",
            "something interesting -> what thing interesting\n",
            "on overcast day -> at over versa\n",
            "on overcast day -> and over versa\n",
            "on overcast day -> on over versa\n",
            "on overcast day -> during sun versa\n",
            "at birthday party -> just three t party\n",
            "at birthday party -> just three t cc\n",
            "at birthday party -> just six t party\n",
            "at birthday party -> just six t cc\n",
            "at birthday party -> at three t party\n",
            "at birthday party -> on three t party\n",
            "at birthday party -> at three t cc\n",
            "at birthday party -> on three t cc\n",
            "passenger loading -> low way sun sun stored\n",
            "passenger loading -> low way sun low stored\n",
            "pictures above them -> posted atop top\n",
            "matching white towels -> white white white towels\n",
            "thick vegetation -> thick vegetation\n",
            "on kitchen counter -> through pink stick counter\n",
            "which are flying low -> which smoke playing low\n",
            "which are flying low -> which flying which low\n",
            "which are flying low -> which smoke playing them\n",
            "which are flying low -> which smoke flying low\n",
            "leaving their aircraft -> leaving plain sun wings\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "many tubes and pipes -> three tub and toy cuts\n",
            "black motorcycles -> white motorcycles\n",
            "black motorcycles -> white motorcycle\n",
            "uses his laptop -> using too laptop\n",
            "uses his laptop -> uses too laptop\n",
            "uses his laptop -> using too cellphone\n",
            "bluish gray water -> white stream right water\n",
            "bluish gray water -> white stream gray water\n",
            "bluish gray water -> white gray right water\n",
            "bluish gray water -> white stream grater smoke\n",
            "coming towards them -> meeting towards ware\n",
            "coming towards them -> meeting toward ware\n",
            "coming towards them -> cutting towards ware\n",
            "coming towards them -> cutting toward ware\n",
            "coming towards them -> eating towards ware\n",
            "coming towards them -> eating toward ware\n",
            "camping gear and food -> watch watch watch food ping\n",
            "camping gear and food -> watch watch watch i ping\n",
            "camping gear and food -> watch watch watch and ping\n",
            "camping gear and food -> watch watch watch food gear\n",
            "camping gear and food -> play watch watch food ping\n",
            "camping gear and food -> watch watch watch food food\n",
            "camping gear and food -> watch ping watch food ping\n",
            "camping gear and food -> watch watch meal food ping\n",
            "camping gear and food -> watch watch watch i gear\n",
            "camping gear and food -> watch watch watch and gear\n",
            "camping gear and food -> play watch watch i ping\n",
            "camping gear and food -> watch watch watch i food\n",
            "camping gear and food -> watch ping watch i ping\n",
            "camping gear and food -> play watch watch and ping\n",
            "camping gear and food -> watch watch watch and food\n",
            "are playing soccer -> play play ware tennis\n",
            "under construction -> par low construction\n",
            "colorful feathers -> some t colored plane's\n",
            "another watches -> three nother tree way\n",
            "participating -> participating\n",
            "black and white picture -> white e white pictures\n",
            "black and white picture -> white e white picture\n",
            "roasted broccoli -> roasted toe toe beef\n",
            "slightly different -> slightly circa thing\n",
            "heavy equipment -> metal vehicles\n",
            "older black and white -> dirty white white white\n",
            "on wooden benches -> through small wooden shelves\n",
            "on wooden benches -> through six wooden shelves\n",
            "some white cabinets -> several white ca shelves\n",
            "black appliances -> white ply ply tv\n",
            "black appliances -> stove ply ply tv\n",
            "black appliances -> white ply county ware\n",
            "black and white cover -> white white white cover\n",
            "black and white cover -> white t white cover\n",
            "black and white cover -> wooding white white white\n",
            "that has strong currents -> which too three oval\n",
            "that has strong currents -> which too strong oval\n",
            "that has strong currents -> which too some oval\n",
            "that has strong currents -> some too three oval\n",
            "that has strong currents -> some too strong oval\n",
            "that has strong currents -> some too some oval\n",
            "people are on bikes -> people holding thing\n",
            "people flying kites -> people cutting tricks\n",
            "underneath and bench -> out tub us lay logs\n",
            "underneath and bench -> out tub up lay logs\n",
            "underneath and bench -> out tub and lay logs\n",
            "underneath and bench -> on tub us lay logs\n",
            "underneath and bench -> on tub up lay logs\n",
            "on street and stoplight -> through three cars street street\n",
            "on street and stoplight -> through three cars road street\n",
            "on street and stoplight -> through three cars street lights\n",
            "on street and stoplight -> through three and street street\n",
            "on street and stoplight -> through three cars street cars\n",
            "another giraffe -> three nother circle\n",
            "some tables and chairs -> three picnic sea trays\n",
            "some tables and chairs -> three picnic sea shelves\n",
            "some tables and chairs -> three tables sea trays\n",
            "some tables and chairs -> three table sea trays\n",
            "some tables and chairs -> some picnic sea trays\n",
            "some tables and chairs -> three tables sea shelves\n",
            "some tables and chairs -> three table sea shelves\n",
            "some tables and chairs -> some picnic sea shelves\n",
            "oversized glasses -> oversized titles\n",
            "flying over top -> railing over top\n",
            "another giraffe -> three nother circle\n",
            "young soccer players -> youth soccer youth play\n",
            "young soccer players -> young soccer youth play\n",
            "standing and staring -> staring ring staring\n",
            "standing and staring -> staring ring smiling\n",
            "resting on branches -> waiting tree thin tree\n",
            "resting on branches -> waiting tree tree tree\n",
            "resting on branches -> waiting sun thin tree\n",
            "resting on branches -> waiting sun tree tree\n",
            "resting on branches -> resting tree thin tree\n",
            "resting on branches -> waiting tree thin logs\n",
            "resting on branches -> resting tree tree tree\n",
            "resting on branches -> waiting tree tree logs\n",
            "resting on branches -> lighting tree thin tree\n",
            "resting on branches -> lighting tree tree tree\n",
            "resting on branches -> resting sun thin tree\n",
            "resting on branches -> waiting sun thin logs\n",
            "resting on branches -> resting sun tree tree\n",
            "resting on branches -> waiting sun tree logs\n",
            "resting on branches -> lighting sun thin tree\n",
            "resting on branches -> lighting sun tree tree\n",
            "resting on branches -> waiting tree thin branch\n",
            "resting on branches -> waiting tree tree branch\n",
            "resting on branches -> resting tree thin logs\n",
            "resting on branches -> waiting up thin tree\n",
            "resting on branches -> resting tree tree logs\n",
            "resting on branches -> waiting on thin tree\n",
            "resting on branches -> waiting up tree tree\n",
            "resting on branches -> waiting on tree tree\n",
            "resting on branches -> lighting tree thin logs\n",
            "old weathered train cars -> old train train train lights\n",
            "different toppings -> three day lives toppings\n",
            "another giraffe -> three nother circle\n",
            "on tree top foliage -> out tree top tree top\n",
            "on tree top foliage -> at tree top tree top\n",
            "on tree top foliage -> on tree top tree top\n",
            "on tree top foliage -> out tree top tree stairs\n",
            "on tree top foliage -> at tree top tree stairs\n",
            "on tree top foliage -> on tree top tree stairs\n",
            "soccer uniforms -> tennis ewe t forms\n",
            "laptops on their couch -> laptops their tv\n",
            "laptops on their couch -> laptops off tv\n",
            "laptops on their couch -> laptops and tv\n",
            "laptops on their couch -> laptops on tv\n",
            "another giraffe -> three nother circle\n",
            "passengers waiting -> way way sun waiting\n",
            "passengers waiting -> way sun sun waiting\n",
            "motorcyclist -> road motor cover\n",
            "bathroom vanity -> toy t vanity\n",
            "plain and colorful -> plain some color rides\n",
            "plain and colorful -> ca some color rides\n",
            "plain and colorful -> plain some ca colors\n",
            "plain and colorful -> plain some ca color\n",
            "several skateboard ramps -> several skate skate ramps\n",
            "stands on his skateboard -> stands stands skateboard which\n",
            "stands on his skateboard -> stands stands skateboard top\n",
            "stands on his skateboard -> rides stands skateboard which\n",
            "large and violent -> metal large citi\n",
            "black and white image -> stunning white white white\n",
            "black and white image -> stunning tan white white\n",
            "black and white image -> stunning white pay white\n",
            "black and white image -> stunning tan pay white\n",
            "black and white image -> stunning white ca white\n",
            "different colors -> ewe couple colors\n",
            "black and gray spotted -> white looks white spotted\n",
            "black and gray spotted -> white lights white spotted\n",
            "black and gray spotted -> white black white spotted\n",
            "black and gray spotted -> white and white spotted\n",
            "black and gray spotted -> looks looks white spotted\n",
            "black and gray spotted -> looks lights white spotted\n",
            "black and gray spotted -> white lights white teddy\n",
            "black and gray spotted -> white looks gray spotted\n",
            "black and gray spotted -> white lights gray spotted\n",
            "black and gray spotted -> white lights white local\n",
            "black and gray spotted -> that looks white spotted\n",
            "potato salad -> potato toe t\n",
            "looking at some shrubs -> looking through shrubs shrubs\n",
            "looking at some shrubs -> looking through some shrubs\n",
            "looking at some shrubs -> looking through shrubs road\n",
            "entering parked train -> level through winter\n",
            "entering parked train -> level train winter\n",
            "entering parked train -> letting through winter\n",
            "entering parked train -> letter through winter\n",
            "entering parked train -> letting train winter\n",
            "entering parked train -> letter train winter\n",
            "pictures on outside -> pick pick thing pick low\n",
            "under construction -> par low construction\n",
            "flowered wallpaper -> ridden white pay t\n",
            "leaving smoke behind -> leaving smoke sprouts which\n",
            "leaving smoke behind -> letting smoke sprouts which\n",
            "leaving smoke behind -> leaving smoke sprouts sinks\n",
            "leaving smoke behind -> letting smoke sprouts sinks\n",
            "leaving smoke behind -> leaving smoke hind which\n",
            "leaving smoke behind -> bitting smoke sprouts which\n",
            "leaving smoke behind -> letting smoke hind which\n",
            "leaving smoke behind -> leaking smoke sprouts which\n",
            "small appliances -> small small ply uses\n",
            "small appliances -> small ply ply uses\n",
            "small appliances -> small small e uses\n",
            "small appliances -> large small ply uses\n",
            "small appliances -> small ply e uses\n",
            "small appliances -> large ply ply uses\n",
            "enjoying some life -> having ring ware scape\n",
            "enjoying some life -> having ring ware life\n",
            "enjoying some life -> having ring some scape\n",
            "black and white cattle -> white white white cattle\n",
            "walking behind them -> walling beside watch\n",
            "walking behind them -> walling upside watch\n",
            "walking behind them -> walking beside watch\n",
            "walking behind them -> walking upside watch\n",
            "already swimming -> swims utter water\n",
            "many zebras that -> too some some zebras\n",
            "ugly tile flooring -> black tile tile tile tile\n",
            "ugly tile flooring -> des tile tile tile ring\n",
            "ugly tile flooring -> black tile tile tile ring\n",
            "his ruffled feathers -> his face tail tail wings\n",
            "his ruffled feathers -> his des tail tail wings\n",
            "hugging each other -> smiling someone its\n",
            "fruit and vegetables -> pears shelves vegetables\n",
            "various colors -> vary t covers\n",
            "crumpled newspapers -> pay pay too pay e\n",
            "three different kinds -> something which spring types\n",
            "three different kinds -> three three which silos\n",
            "three different kinds -> three thing which silos\n",
            "three different kinds -> something which kinds types\n",
            "other ideas -> thing nothing thing de\n",
            "orange counter tops -> tan counter tops tops\n",
            "black appliances -> white ply ply tv\n",
            "black appliances -> stove ply ply tv\n",
            "black appliances -> white ply county ware\n",
            "are lined up ready -> some cutting setting\n",
            "are lined up ready -> some cutting setter\n",
            "sydney, australia -> sydney venice ware\n",
            "sydney, australia -> sunny venice ware\n",
            "sydney, australia -> sydney venice sick\n",
            "sydney, australia -> sunny venice sick\n",
            "towards its next stop -> past way way its way\n",
            "towards its next stop -> past way way his way\n",
            "towards its next stop -> its way way its way\n",
            "towards its next stop -> its way way his way\n",
            "people on their bikes -> people bikes some use\n",
            "people on their bikes -> people out some use\n",
            "people on their bikes -> people off some use\n",
            "people on their bikes -> people at some use\n",
            "people on their bikes -> people on some use\n",
            "people on their bikes -> siding bikes some use\n",
            "people on their bikes -> siding out some use\n",
            "people on their bikes -> siding off some use\n",
            "people on their bikes -> siding at some use\n",
            "people on their bikes -> siding on some use\n",
            "open container -> ewe taken taper\n",
            "open container -> ewe stick container\n",
            "open container -> ewe taken taken\n",
            "open container -> stick taken taper\n",
            "swimmers and coaches -> swimmers ewe run cuts\n",
            "many other types -> many weather white\n",
            "are playing baseball -> playing play poker\n",
            "are playing baseball -> play playing poker\n",
            "several small airplanes -> several small small plane's\n",
            "many bananas -> metal small run sheep\n",
            "many bananas -> many small run sheep\n",
            "flying over top -> railing over top\n",
            "standing and pointing -> stand standing sitting\n",
            "standing and pointing -> stand standing pointing\n",
            "underneath something -> some hind thing someone\n",
            "underneath something -> some hind thing something\n",
            "underneath something -> some from thing someone\n",
            "underneath something -> par hind thing someone\n",
            "underneath something -> some from thing something\n",
            "underneath something -> par hind thing something\n",
            "underneath something -> beneath thing thing thing\n",
            "underneath something -> par from thing someone\n",
            "underneath something -> lay hind thing someone\n",
            "underneath something -> par from thing something\n",
            "underneath something -> lay hind thing something\n",
            "underneath something -> lay from thing someone\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "on three bananas -> three three pine stacked stacked\n",
            "on three bananas -> three stacked pine stacked stacked\n",
            "on three bananas -> three six pine stacked stacked\n",
            "on three bananas -> three peeled pine stacked stacked\n",
            "are playing soccer -> play play ware tennis\n",
            "plaid patterned bed spreads -> three patterned turned spreads\n",
            "plaid patterned bed spreads -> three patterned turned north\n",
            "plaid patterned bed spreads -> six patterned turned spreads\n",
            "plaid patterned bed spreads -> plaid patterned turned spreads\n",
            "various prizes -> vary come prizes\n",
            "drinking some water -> sitting some water\n",
            "drinking some water -> walking some water\n",
            "drinking some water -> sipping some water\n",
            "drinking some water -> eating some water\n",
            "drinking some water -> getting some water\n",
            "drinking some water -> sitting food water\n",
            "drinking some water -> sitting some walking\n",
            "perfectly painted -> painted stick painting\n",
            "perfectly painted -> painted stick painted\n",
            "little stuffed monkees -> little stuffed people\n",
            "various people -> vary some places\n",
            "various people -> vary some pieces\n",
            "beside small houses -> through large small laces\n",
            "beside small houses -> through hind small laces\n",
            "beside small houses -> through large small houses\n",
            "on bed and curtains -> right tub lutz tree ground\n",
            "on bed and curtains -> right tub lutz tub ground\n",
            "on bed and curtains -> right tub come tree ground\n",
            "on bed and curtains -> right tub come tub ground\n",
            "on bed and curtains -> right tub lutz tree chairs\n",
            "on bed and curtains -> right six lutz tree ground\n",
            "on bed and curtains -> right tub lutz tub chairs\n",
            "on bed and curtains -> right six lutz tub ground\n",
            "on bed and curtains -> right tub lutz tree bed\n",
            "on bed and curtains -> right tub come tree chairs\n",
            "on bed and curtains -> right six come tree ground\n",
            "on bed and curtains -> right tub lutz tub bed\n",
            "on bed and curtains -> right tub come tub chairs\n",
            "on bed and curtains -> right six come tub ground\n",
            "on bed and curtains -> right tub come tree bed\n",
            "on bed and curtains -> right six lutz tree chairs\n",
            "on bed and curtains -> right tub come tub bed\n",
            "on bed and curtains -> right six lutz tub chairs\n",
            "on bed and curtains -> right six lutz tree bed\n",
            "on bed and curtains -> right tub bed tree ground\n",
            "on bed and curtains -> right six come tree chairs\n",
            "on bed and curtains -> right six lutz tub bed\n",
            "on bed and curtains -> right tub lutz q ground\n",
            "on bed and curtains -> right tub bed tub ground\n",
            "on bed and curtains -> right six come tub chairs\n",
            "on bed and curtains -> right six come tree bed\n",
            "on bed and curtains -> right six come tub bed\n",
            "on bed and curtains -> right tub come q ground\n",
            "on bed and curtains -> right tub bed tree chairs\n",
            "on bed and curtains -> right six bed tree ground\n",
            "on bed and curtains -> right tub lutz q chairs\n",
            "on bed and curtains -> right six lutz q ground\n",
            "on bed and curtains -> at tub lutz tree ground\n",
            "on bed and curtains -> right tub bed tub chairs\n",
            "on bed and curtains -> right six bed tub ground\n",
            "on bed and curtains -> on tub lutz tree ground\n",
            "on bed and curtains -> right tub bed tree bed\n",
            "uniformed people -> server ewe people\n",
            "uniformed people -> server ewe purple\n",
            "large white airliner -> white white white liner\n",
            "large white airliner -> white white tan liner\n",
            "large white airliner -> small white white liner\n",
            "american flags -> ewe rustic controls\n",
            "bikes and motor bikes -> stay toy motor cars\n",
            "other toiletries -> toy toy toy toy toy\n",
            "other toiletries -> toy toy t toy toy\n",
            "other toiletries -> toy t toy toy toy\n",
            "other toiletries -> toy t t toy toy\n",
            "some white cabinets -> several white ca shelves\n",
            "carrying cargo -> carry somewhere sea\n",
            "several silver spoons -> several silver spoons\n",
            "parked motorcycle -> motor t t t\n",
            "parked motorcycle -> motor t t tank\n",
            "parked motorcycle -> rides motor t t\n",
            "parked motorcycle -> rides motor t tank\n",
            "parked motorcycle -> parked motor t t\n",
            "baby eating food -> title eating plates\n",
            "baby eating food -> title eating plate\n",
            "baby eating food -> title eating cake\n",
            "baby eating food -> title beating plates\n",
            "baby eating food -> title beating plate\n",
            "baby eating food -> title eating food\n",
            "baby eating food -> title beating cake\n",
            "this tiny bathroom -> small tiny toy tick\n",
            "this tiny bathroom -> small tiny toy stick\n",
            "this tiny bathroom -> small tiny toy rooms\n",
            "uses their cellphone -> using watch seated\n",
            "uses their cellphone -> using t seated\n",
            "uses their cellphone -> uses watch seated\n",
            "uses their cellphone -> uses t seated\n",
            "three suitcases that -> three three suitcases\n",
            "three suitcases that -> three six suitcases\n",
            "three suitcases that -> six three suitcases\n",
            "looking over edge -> seeking merry shelves\n",
            "looking over edge -> looking merry shelves\n",
            "looking over edge -> reading merry shelves\n",
            "various people -> vary some places\n",
            "various people -> vary some pieces\n",
            "meat and brussel sprouts -> raw sprouts sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> meat sprouts sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> large sprouts sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> raw meat sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> meat meat sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> large meat sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> cheese sprouts sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> raw sprouts fried sprouts sprouts\n",
            "meat and brussel sprouts -> meat sprouts fried sprouts sprouts\n",
            "meat and brussel sprouts -> raw sprouts sprouts cheese sprouts\n",
            "meat and brussel sprouts -> raw cheese sprouts sprouts sprouts\n",
            "meat and brussel sprouts -> large sprouts fried sprouts sprouts\n",
            "street lights street number -> what street three street watch\n",
            "street lights street number -> three street three street watch\n",
            "street lights street number -> what street street street watch\n",
            "street lights street number -> three street street street watch\n",
            "street lights street number -> street street three street watch\n",
            "street lights street number -> six street three street watch\n",
            "street lights street number -> street street street street watch\n",
            "street lights street number -> what street three lights watch\n",
            "street lights street number -> what lights three street watch\n",
            "street lights street number -> what street three street post\n",
            "street lights street number -> three lights three street watch\n",
            "street lights street number -> three street three lights watch\n",
            "street lights street number -> three street three street post\n",
            "street lights street number -> six street street street watch\n",
            "street lights street number -> what street street lights watch\n",
            "street lights street number -> what lights street street watch\n",
            "street lights street number -> what street street street post\n",
            "street lights street number -> what street three street lights\n",
            "street lights street number -> three street street lights watch\n",
            "three cement benches -> three supple kites shelves\n",
            "three cement benches -> three cement kites shelves\n",
            "three cement benches -> three supple kites pipes\n",
            "through grassy water -> through some sea water\n",
            "serrated edges -> serrated scissors\n",
            "under construction -> par low construction\n",
            "being unloaded -> ewe t would leaky\n",
            "being unloaded -> ewe t would loaded\n",
            "being unloaded -> ewe low would leaky\n",
            "being unloaded -> ewe t low leaky\n",
            "his hands out and mouth -> he mouth soap leaguer\n",
            "his hands out and mouth -> his mouth soap leaguer\n",
            "motorcyclist -> road motor cover\n",
            "baseball uniforms -> their t ewe t t\n",
            "baseball uniforms -> bra t ewe t t\n",
            "baseball uniforms -> base t ewe t t\n",
            "motorized scooter -> ewe motor scooter\n",
            "another giraffe -> three nother circle\n",
            "has stained glass windows -> sinks shovel windows\n",
            "has stained glass windows -> sinks shovel window\n",
            "pick up passengers -> waking sun way sun\n",
            "pick up passengers -> waking sun way parked\n",
            "around his shoulders -> over some shoulders\n",
            "denim overalls -> cover over tops\n",
            "denim overalls -> cover over hats\n",
            "some people waiting -> some people waiting\n",
            "its yellow train cars -> yellow train train plane's\n",
            "orange safety vests -> yellow ewe t vests\n",
            "orange safety vests -> yellow ewe t vest\n",
            "behind his surfboard -> across top toy top\n",
            "behind his surfboard -> behind top toy top\n",
            "behind his surfboard -> across right toy top\n",
            "behind his surfboard -> across top right top\n",
            "behind his surfboard -> under top toy top\n",
            "at parking meters -> par par par meters\n",
            "many bananas -> metal small run sheep\n",
            "many bananas -> many small run sheep\n",
            "various grilled foods -> various ta meals\n",
            "over three people -> three three three pieces\n",
            "over three people -> three three three person\n",
            "over three people -> three three three people\n",
            "over three people -> three three three purple\n",
            "over three people -> three three three laces\n",
            "over three people -> three three small pieces\n",
            "over three people -> three three small person\n",
            "over three people -> six three three pieces\n",
            "looking at his leg -> pointing through its leg\n",
            "looking at his leg -> pointing through his leg\n",
            "looking at his leg -> pointing through its hind\n",
            "looking at his leg -> pointing through this leg\n",
            "looking at his leg -> pointing through his hind\n",
            "some wavy water -> some wavy waters\n",
            "some wavy water -> some wavy water\n",
            "young soccer players -> youth soccer youth play\n",
            "young soccer players -> young soccer youth play\n",
            "laptop computer -> raising top top top\n",
            "people are waiting -> passengers waiting\n",
            "brightly colored chairs -> white turned colored tops\n",
            "brightly colored chairs -> tiled turned colored tops\n",
            "tasty soup and milk -> tasty soup and milk\n",
            "from something orange -> something thing yellow\n",
            "at its reflection -> through what reflection\n",
            "reading sidewalk closed -> siting sidewalk t\n",
            "looking behind back -> staring ring hind way\n",
            "on wooden table -> through small stick counter\n",
            "on wooden table -> stick small stick counter\n",
            "on wooden table -> through old stick counter\n",
            "on street and stoplight -> through three cars street street\n",
            "on street and stoplight -> through three cars road street\n",
            "on street and stoplight -> through three cars street lights\n",
            "on street and stoplight -> through three and street street\n",
            "on street and stoplight -> through three cars street cars\n",
            "many oranges -> several small run ware\n",
            "many oranges -> several run run ware\n",
            "many oranges -> metal small run ware\n",
            "many oranges -> metal run run ware\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "desk top computer -> well top seater top\n",
            "desk top computer -> top top seater top\n",
            "kicking up dirt as -> showing ware ware dirt\n",
            "kicking up dirt as -> showing ware ware as\n",
            "kicking up dirt as -> showing ware ware at\n",
            "different toilets -> past lutz toy toilets\n",
            "different toilets -> past lutz toy toilet\n",
            "some parked motor bikes -> several turtle ware\n",
            "some parked motor bikes -> several motor ware\n",
            "some parked motor bikes -> several turtle e\n",
            "on holding something -> wholesome ware something\n",
            "on holding something -> held somewhere something\n",
            "on holding something -> wholesome something thing\n",
            "on holding something -> wholesome thing something\n",
            "on holding something -> held something something\n",
            "on motorcycle -> through motor motor\n",
            "uniformed people -> server ewe people\n",
            "uniformed people -> server ewe purple\n",
            "beautiful giraffes -> pretty t tree sheep\n",
            "beautiful giraffes -> beauty t tree sheep\n",
            "beautiful giraffes -> pretty flee tree sheep\n",
            "surfing through water -> walling through waters\n",
            "surfing through water -> walling through water\n",
            "surfing through water -> walking through waters\n",
            "surfing through water -> walking through water\n",
            "surfing through water -> walling through seater\n",
            "motorcycles parked -> parked motor parked space\n",
            "motorcycles parked -> parked meters parked space\n",
            "motorcycles parked -> parked meter parked space\n",
            "motorcycle rides -> rides motor siting\n",
            "motorcycle rides -> ride motor siting\n",
            "motorcycle rides -> rides motor writing\n",
            "motorcycle rides -> ride motor writing\n",
            "others looking on -> some peeking way walks\n",
            "others looking on -> some looking way walks\n",
            "others looking on -> some peeking way past\n",
            "banana clusters -> oval shaped stirs shapes\n",
            "banana clusters -> oval shaped stirs stirs\n",
            "banana clusters -> oval stirs stirs shapes\n",
            "banana clusters -> oval stirs stirs stirs\n",
            "motorcyclist -> road motor cover\n",
            "subdued colored bird -> ewe t people toe\n",
            "subdued colored bird -> ewe t couple toe\n",
            "subdued colored bird -> ewe t colored toe\n",
            "subdued colored bird -> sub t people toe\n",
            "this tiny bathroom -> small tiny toy tick\n",
            "this tiny bathroom -> small tiny toy stick\n",
            "this tiny bathroom -> small tiny toy rooms\n",
            "little league baseball -> little tank league low\n",
            "little league baseball -> little tank de low\n",
            "little league baseball -> little league league low\n",
            "following purchase -> par way sta bye town\n",
            "another giraffe -> three nother circle\n",
            "handicap toilet -> ewe tree cap toy toy\n",
            "handicap toilet -> ewe tree cap toy stall\n",
            "handicap toilet -> ewe tree cap pay toy\n",
            "handicap toilet -> ewe tree off toy toy\n",
            "handicap toilet -> ewe tree cap pay stall\n",
            "dirty brown and white -> dirty white toe white\n",
            "dirty brown and white -> dirty white t white\n",
            "dirty brown and white -> dirty white toe t\n",
            "dirty brown and white -> dirty t toe white\n",
            "dirty brown and white -> dirty white t t\n",
            "dirty brown and white -> dirty t t white\n",
            "different sizes -> varying sizes\n",
            "some computer screens -> three computer tops\n",
            "sun speckle pattern -> white sun tick patting\n",
            "sun speckle pattern -> white sun tick pattern\n",
            "fasten their seat belts -> take seat seat stop seat\n",
            "fasten their seat belts -> take seat seat stop sinks\n",
            "fasten their seat belts -> take seat sun stop seat\n",
            "fasten their seat belts -> take sun seat stop seat\n",
            "fasten their seat belts -> take sun seat stop sinks\n",
            "fasten their seat belts -> take seat sun stop sinks\n",
            "fasten their seat belts -> take sun sun stop seat\n",
            "fasten their seat belts -> take sun sun stop sinks\n",
            "fasten their seat belts -> take seat seat seat seat\n",
            "fasten their seat belts -> take seat seat seat sinks\n",
            "fasten their seat belts -> take seat sun seat seat\n",
            "fasten their seat belts -> take seat seat sun seat\n",
            "fasten their seat belts -> take sun seat seat seat\n",
            "fasten their seat belts -> take seat sun seat sinks\n",
            "fasten their seat belts -> take seat seat sun sinks\n",
            "fasten their seat belts -> take sun seat seat sinks\n",
            "fasten their seat belts -> stop seat seat stop seat\n",
            "fasten their seat belts -> stop seat seat stop sinks\n",
            "fasten their seat belts -> take seat sun sun seat\n",
            "fasten their seat belts -> take sun sun seat seat\n",
            "fasten their seat belts -> take sun seat sun seat\n",
            "fasten their seat belts -> take sun sun seat sinks\n",
            "fasten their seat belts -> take sun seat sun sinks\n",
            "fasten their seat belts -> take seat sun sun sinks\n",
            "fasten their seat belts -> stop sun seat stop seat\n",
            "fasten their seat belts -> stop seat sun stop seat\n",
            "fasten their seat belts -> stop sun seat stop sinks\n",
            "fasten their seat belts -> stop seat sun stop sinks\n",
            "fasten their seat belts -> take sun sun sun seat\n",
            "fasten their seat belts -> take sun sun sun sinks\n",
            "fasten their seat belts -> stop sun sun stop seat\n",
            "fasten their seat belts -> stop sun sun stop sinks\n",
            "fasten their seat belts -> stop seat seat seat seat\n",
            "fasten their seat belts -> stop seat seat seat sinks\n",
            "fasten their seat belts -> stop sun seat seat seat\n",
            "fasten their seat belts -> stop seat sun seat seat\n",
            "fasten their seat belts -> stop seat seat sun seat\n",
            "fasten their seat belts -> take leaves seat stop seat\n",
            "fasten their seat belts -> stop seat seat sun sinks\n",
            "fasten their seat belts -> stop seat sun seat sinks\n",
            "fasten their seat belts -> stop sun seat seat sinks\n",
            "fasten their seat belts -> take leaves seat stop sinks\n",
            "fasten their seat belts -> stop sun seat sun seat\n",
            "fasten their seat belts -> stop sun sun seat seat\n",
            "fasten their seat belts -> stop seat sun sun seat\n",
            "fasten their seat belts -> take leaves sun stop seat\n",
            "fasten their seat belts -> stop seat sun sun sinks\n",
            "fasten their seat belts -> stop sun sun seat sinks\n",
            "fasten their seat belts -> stop sun seat sun sinks\n",
            "fasten their seat belts -> take leaves sun stop sinks\n",
            "fasten their seat belts -> stop sun sun sun seat\n",
            "on concrete structure -> over stick struck shelves\n",
            "over barriers -> over and scissors\n",
            "over barriers -> pretty and scissors\n",
            "westward ho motel -> slept ca street street time\n",
            "westward ho motel -> slept ca street street street\n",
            "westward ho motel -> slept ca street sun time\n",
            "spectators look on -> spectators pieces\n",
            "laying on toilet -> sitting toy toilet\n",
            "laying on toilet -> sitting toilet tub\n",
            "laying on toilet -> sitting toilet sink\n",
            "standing on some ice -> walling through water\n",
            "standing on some ice -> walking through water\n",
            "standing on some ice -> walling some water\n",
            "standing on some ice -> pulling through water\n",
            "standing on some ice -> walking some water\n",
            "standing on some ice -> floating through water\n",
            "different sweet treats -> three pace lar sweet treats\n",
            "different sweet treats -> three pace lar spring treats\n",
            "different sweet treats -> three pace lar sweet ping\n",
            "different sweet treats -> three pace lar spring ping\n",
            "on snowy mountain -> through snowy mountain\n",
            "tied around his head -> tied overhead squares\n",
            "tied around his head -> tied overhead neck\n",
            "tied around his head -> tied over skateboard\n",
            "various pieces -> vary some vases\n",
            "various pieces -> some sorted vases\n",
            "various pieces -> vary some reese's\n",
            "sitting and standing -> sitting rest sea way\n",
            "sitting and standing -> sitting rest sun way\n",
            "sitting on sofa -> seating t way top\n",
            "sitting on sofa -> sitting t way top\n",
            "sitting on sofa -> seating t way hood\n",
            "sitting on sofa -> sitting t way hood\n",
            "sitting on sofa -> seeing t way top\n",
            "umbrellas and chairs -> taken ware chairs ware\n",
            "umbrellas and chairs -> taken ware and ware\n",
            "umbrellas and chairs -> table ware chairs ware\n",
            "umbrellas and chairs -> taken ware chairs pick\n",
            "umbrellas and chairs -> table ware and ware\n",
            "umbrellas and chairs -> taken ware and pick\n",
            "umbrellas and chairs -> table ware chairs pick\n",
            "umbrellas and chairs -> table ware and pick\n",
            "different colors -> ewe couple colors\n",
            "space and oranges -> space tub soap run cups\n",
            "space and oranges -> space tub small run cups\n",
            "posing on his bike -> toting some he leg\n",
            "posing on his bike -> toting some his leg\n",
            "posing on his bike -> toting some he he\n",
            "posing on his bike -> toting some his he\n",
            "posing on his bike -> toting some dirt leg\n",
            "posing on his bike -> writing some he leg\n",
            "posing on his bike -> writing some his leg\n",
            "posing on his bike -> toting some bike leg\n",
            "posing on his bike -> toting some dirt he\n",
            "posing on his bike -> toting his he leg\n",
            "posing on his bike -> toting his his leg\n",
            "posing on his bike -> writing some he he\n",
            "posing on his bike -> writing some his he\n",
            "posing on his bike -> writing some dirt leg\n",
            "posing on his bike -> toting some he bike\n",
            "posing on his bike -> toting some bike he\n",
            "posing on his bike -> toting some his bike\n",
            "posing on his bike -> toting some he back\n",
            "posing on his bike -> toting some his back\n",
            "posing on his bike -> toting some he as\n",
            "posing on his bike -> toting some his as\n",
            "posing on his bike -> toting his he he\n",
            "graduation robes -> specialty robes robes\n",
            "traffic lights and cars -> pavement street street shelves\n",
            "traffic lights and cars -> traffic street street shelves\n",
            "traffic lights and cars -> pavement sky street shelves\n",
            "traffic lights and cars -> traffic sky street shelves\n",
            "traffic lights and cars -> pavement street street plates\n",
            "traffic lights and cars -> traffic street street plates\n",
            "using his laptop -> using laptop tops\n",
            "using his laptop -> using top laptops\n",
            "using his laptop -> using top laptop\n",
            "using his laptop -> using laptop top\n",
            "on wooden table -> through small stick counter\n",
            "on wooden table -> stick small stick counter\n",
            "on wooden table -> through old stick counter\n",
            "black and white photo -> white white white photo\n",
            "black and white photo -> white white tan photo\n",
            "black and white photo -> sta white white photo\n",
            "black and white photo -> sta white tan photo\n",
            "pick up passengers -> waking sun way sun\n",
            "pick up passengers -> waking sun way parked\n",
            "people flying kites -> people cutting tricks\n",
            "behind each other -> between each ware ware\n",
            "cooking over coals -> putting over thing\n",
            "cooking over coals -> mating over thing\n",
            "living spaces are -> living spaces stirs\n",
            "different colors -> ewe couple colors\n",
            "at flag and building -> through long town standing\n",
            "large white and orange -> small yellow yellow\n",
            "large white and orange -> long yellow yellow\n",
            "large white and orange -> large yellow yellow\n",
            "white water rafting -> white water water\n",
            "white water rafting -> tan water water\n",
            "on something yellow -> from thing thing something\n",
            "on something yellow -> from thing thing yellow\n",
            "different pastries -> pace pastries pastries\n",
            "different pastries -> pace pastries desserts\n",
            "several shopping bags -> several shopping thick\n",
            "several shopping bags -> vary shopping thick\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "#print('top preds: ', mini_preds[0])\n",
        "#print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    print('{} -> {}'.format(x_short[i], [decoder.idx2syll[x] for x in fs.scorepaths[0]]))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    print(mini_preds[i].shape)\n",
        "    print(morepaths.shape)\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print(fs.scorepaths[j])\n",
        "        #z = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "        #print(z)\n",
        "        morepaths[j] = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print(morepaths[0])\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    #d = []\n",
        "    # for x in encoded:\n",
        "    #   for y in x:\n",
        "    #        if len(y) > 0:\n",
        "    #           d.append(y[0])  \n",
        "    #d = np.array(d)\n",
        "    d = encoded\n",
        "    if len(d) > 0:\n",
        "        print('encoded sentences: ', d)\n",
        "        print(x_short[i], ':')\n",
        "        #print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(encoded)\n",
        "        #print('len(decoded): ', len(decoded))\n",
        "        ##print('len(decoded[0]): ', len(decoded[0]))\n",
        "        #print('len(decoded[0][0]): ', len(decoded[0][0]))\n",
        "        #print('len(decoded[0][0][0]): ', len(decoded[0][0][0]))\n",
        "        #print('decoded[0][0][0]: ', decoded[0][0][0])\n",
        "        sentences = {}\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    print('d3: ', d3)\n",
        "                    break\n",
        "                    key = ' '.join(d3)\n",
        "                    sentences[key] = d3\n",
        "                    break\n",
        "                     \n",
        "        print('[{}]  -> {}', i,list(sentences.keys())[0:10])\n",
        "    break\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_list = [sparse, perfect, sparse5, perfect5]\n",
        "metric_names = ['sparse', 'perfect', 'sparse5', 'perfect5']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6aZfdAnkCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}