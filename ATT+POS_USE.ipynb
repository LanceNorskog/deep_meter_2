{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYd5dVdmTaK8",
        "colab_type": "code",
        "outputId": "7d049361-c3f9-4329-ac33-f812dd91048a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "#!pip install keras==2.1.2\n",
        "!pip install git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model, Sequential\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import text\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# cut texts after this number of words\n",
        "# (among top max_features most common words)\n",
        "max_features = 2000\n",
        "# longest output sentence\n",
        "maxlen = 5\n",
        "batch_size = 32\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5_same.txt\n",
        "haiku_data = 'haiku_5_same.txt'\n",
        "haiku_text = []\n",
        "with open(haiku_data) as f:\n",
        "    for line in f.readlines():\n",
        "        haiku_text.append(line.split('\\t')[0])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: deepmeter from git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "['DH EH R', 'F AO R']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File ‘haiku_5_same.txt’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer_from_json(json_string):\n",
        "    \"\"\"Parses a JSON tokenizer configuration file and returns a\n",
        "    tokenizer instance.\n",
        "    # Arguments\n",
        "        json_string: JSON string encoding a tokenizer configuration.\n",
        "    # Returns\n",
        "        A Keras Tokenizer instance\n",
        "    \"\"\"\n",
        "    tokenizer_config = json.loads(json_string)\n",
        "    config = tokenizer_config.get('config')\n",
        "\n",
        "    word_counts = json.loads(config.pop('word_counts'))\n",
        "    word_docs = json.loads(config.pop('word_docs'))\n",
        "    index_docs = json.loads(config.pop('index_docs'))\n",
        "    # Integer indexing gets converted to strings with json.dumps()\n",
        "    index_docs = {int(k): v for k, v in index_docs.items()}\n",
        "    index_word = json.loads(config.pop('index_word'))\n",
        "    index_word = {int(k): v for k, v in index_word.items()}\n",
        "    word_index = json.loads(config.pop('word_index'))\n",
        "\n",
        "    tokenizer = text.Tokenizer(**config)\n",
        "    tokenizer.word_counts = word_counts\n",
        "    tokenizer.word_docs = word_docs\n",
        "    tokenizer.index_docs = index_docs\n",
        "    tokenizer.word_index = word_index\n",
        "    tokenizer.index_word = index_word\n",
        "\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "19a23b50-2dac-4d16-923d-efa01d2cdb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!date\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5_same.txt\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/toki_cmu.json\n",
        "haiku_data = 'haiku_5_same.txt'\n",
        "toki_file = 'toki_cmu.json'\n",
        "haiku_text = []\n",
        "with open(haiku_data) as f:\n",
        "    for line in f.readlines():\n",
        "        haiku_text.append(line.split('\\t')[0])\n",
        "!date\n",
        "print('haiku_text: ', len(haiku_text))\n",
        "print('one: ', haiku_text[0])\n",
        "with open(toki_file, \"r\") as f:\n",
        "        conf = f.read()\n",
        "        toki = tokenizer_from_json(conf)\n",
        "        conf = None\n",
        "!date\n",
        "data_all = [[]] * len(haiku_text)\n",
        "for i in range(len(haiku_text)):    \n",
        "    seq = text.text_to_word_sequence(haiku_text[i])\n",
        "    sylls = []\n",
        "    for word in seq:\n",
        "        if word in word2sylls:\n",
        "            for syll in word2sylls[word]:\n",
        "                sylls.append(syll)\n",
        "    data_all[i] = toki.texts_to_sequences(sylls)\n",
        "\n",
        "!date   \n",
        "print(haiku_text[0])\n",
        "print(data_all[0])\n",
        "print(haiku_text[2999])\n",
        "print(data_all[2999])\n",
        "!date\n",
        "#(x_train, x_test, y_train, y_test) = train_test_split(haiku_text, data_all)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jun 26 05:14:18 UTC 2019\n",
            "File ‘haiku_5_same.txt’ already there; not retrieving.\n",
            "\n",
            "File ‘toki_cmu.json’ already there; not retrieving.\n",
            "\n",
            "Wed Jun 26 05:14:21 UTC 2019\n",
            "haiku_text:  95622\n",
            "one:  a abandon church\n",
            "Wed Jun 26 05:14:21 UTC 2019\n",
            "Wed Jun 26 05:14:30 UTC 2019\n",
            "a abandon church\n",
            "[[1], [1], [11, 10, 3], [8, 1, 3], [32, 19, 32]]\n",
            "a cardboard cutout\n",
            "[[1], [9, 14, 5, 8], [11, 25, 5, 8], [9, 1, 2], [31, 2]]\n",
            "Wed Jun 26 05:14:30 UTC 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "936a3054-9119-4542-ce65-f90273982208"
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(haiku_text, data_all)\n",
        "print(x_train[0][0], y_train[0][0])\n",
        "print(x_test[0][0], y_test[0][0])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "def pack(indexes, maxlen):\n",
        "    print(type(indexes))\n",
        "    print(type(indexes[0]))\n",
        "    print(type(indexes[0][0]))\n",
        "    print(len(indexes[0][0]))\n",
        "    ilen = len(indexes)\n",
        "    out = np.zeros((ilen, maxlen))\n",
        "    for i in range(ilen):\n",
        "        for j in range(len(indexes[i])):\n",
        "            if j >= maxlen:\n",
        "                continue\n",
        "            #print(i,j, len(indexes[i]))  \n",
        "            if len(indexes[i][j]) > 0:\n",
        "                out[i][j] = indexes[i][j][0]\n",
        "    return out\n",
        "    \n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.expand_dims(pack(y_train, maxlen), -1)\n",
        "y_test = np.expand_dims(pack(y_test, maxlen), -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a [1]\n",
            "h [30, 28, 7]\n",
            "Pad sequences (samples x time)\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "1\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "3\n",
            "x_train shape: (71716,)\n",
            "x_test shape: (23906,)\n",
            "y_train shape: (71716, 5, 1)\n",
            "y_test shape: (23906, 5, 1)\n",
            "[30.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "outputId": "512406cf-0424-4711-88ad-0d8151bad879"
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size//3\n",
        "units=1024\n",
        "\n",
        "metric_list = [sparse, sparse1, perfect, perfect1, sparse5, perfect5]\n",
        "metric_names = ['sparse', 'sparse1', 'perfect', 'perfect1', 'sparse5', 'perfect5']\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "x = layers.RepeatVector(maxlen)(x)\n",
        "if False:\n",
        "    #x = layers.Dropout(0.1)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(max_features, activation='softmax')(x)\n",
        "#x = layers.Reshape((-1, max_features * maxlen))(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=x)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=30,\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 05:14:33.827944 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0626 05:14:33.830021 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0626 05:14:35.606917 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0626 05:14:38.980397 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0626 05:14:38.993509 140564896233344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0626 05:14:39.046564 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0626 05:14:39.073428 140564896233344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 1024)           6299648   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 2000)           2050000   \n",
            "=================================================================\n",
            "Total params: 8,349,648\n",
            "Trainable params: 8,349,648\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0626 05:14:42.364511 140564896233344 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 71716 samples, validate on 23906 samples\n",
            "Epoch 1/30\n",
            " - 149s - loss: 2.5075 - sparse: 0.3035 - sparse1: 0.3035 - perfect: 0.0051 - perfect1: 0.0000e+00 - sparse5: 0.6237 - perfect5: 0.0000e+00 - val_loss: 2.0752 - val_sparse: 0.4110 - val_sparse1: 0.4110 - val_perfect: 0.0207 - val_perfect1: 0.0000e+00 - val_sparse5: 0.7290 - val_perfect5: 0.0000e+00\n",
            "Epoch 2/30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPojf4nRip3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(metric_names, loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_short = x_test[0:1000]\n",
        "y_short = y_test[0:1000]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  #eval = model.evaluate(x_test, y_test)\n",
        "  #print('model.evaluate on val holdout: ' ,model.metrics_names, eval)\n",
        "  print('history: ', history)\n",
        "  predicts = model.predict(x_short, batch_size=32)\n",
        "  print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFy2tfyIYOtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxindx(pred):\n",
        "    maxi=-1\n",
        "    maxv=-1.0\n",
        "    for x in range(len(pred)):\n",
        "        if pred[x] > maxv:\n",
        "            maxv = pred[x]\n",
        "            maxi = x\n",
        "    return (maxi, maxv)\n",
        "\n",
        "def match(data, prediction):\n",
        "    good = 0\n",
        "    total = 0\n",
        "    for i in range(len(data)):\n",
        "        #print(data[i], np.argmax(prediction[i]))\n",
        "        if data[i] == np.argmax(prediction[i]):\n",
        "            good += 1\n",
        "        total += 1\n",
        "    #print('{}, {}'.format(good, total))\n",
        "    if (total == 0):\n",
        "        return 0\n",
        "    return good / total\n",
        "\n",
        "parallel = 0.0\n",
        "serial = 0.0\n",
        "total = 0\n",
        "for n in range(len(y_short)):\n",
        "    #print(len(short[n]))\n",
        "    check = match(y_short[n], predicts[n])\n",
        "    parallel += check\n",
        "    if check > 0.9999:\n",
        "        serial += 1\n",
        "    total += 1\n",
        "\n",
        "print('Parallel, serial: ', parallel / total, serial / total)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWVxMyKd3P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}