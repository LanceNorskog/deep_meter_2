{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "3327f6a9-81ff-4164-8098-4f5c65c94776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I14yvHKbdZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm /content/model_haiku_5*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "0fee066d-1c55-4d39-ef58-c369a703c5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 35000\n",
        "# longest output sentence\n",
        "maxlen = 5\n",
        "batch_size = 32\n",
        "deduplicate_haiku=True\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_base=\"/content/model_haiku_5_\"\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "88abc5cc-fc0c-4187-a709-638193649ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!date\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul  9 03:44:59 UTC 2019\n",
            "['!exclamation-point', '\"close-quote', '\"double-quote', '\"end-of-quote', '\"end-quote', '\"in-quotes', '\"quote', '\"unquote', '%percent', '&ampersand']\n",
            "['AA', 'AA B', 'AA B S T', 'AA CH', 'AA D', 'AA D Z', 'AA F', 'AA F S', 'AA F S K', 'AA G']\n",
            "19433 0\n",
            "# features:  32088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "8d641500-ba21-4310-b94c-bdce891deadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "big_text = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            continue\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            if word in word2sylls:\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "        if len(_sylls) != 5:\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        big_text.append(_text)\n",
        "        big_haiku.append(_haiku)\n",
        "        big_data.append(_data)\n",
        "        #if len(big_text) == max_data * 2:\n",
        "        #    break\n",
        "\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "big_data = np.expand_dims(big_data, -1)\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "shuffle = np.arange(len(big_text))\n",
        "print(shuffle)\n",
        "np.random.shuffle(shuffle)\n",
        "shuffle = shuffle[0:max_data]\n",
        "big_text = big_text[shuffle]\n",
        "big_haiku = big_haiku[shuffle]\n",
        "big_data = big_data[shuffle]\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a white sink and door -> a white sink and door : [[17146]\n",
            " [31228]\n",
            " [27105]\n",
            " [17115]\n",
            " [18834]]\n",
            "[     0      1      2 ... 668545 668546 668547]\n",
            "a remote control has all black and white buttons except for a yellow light \" button -> a remote control : [[17146]\n",
            " [26501]\n",
            " [24697]\n",
            " [22523]\n",
            " [30217]]\n",
            "Full length clauses:  100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "e4ce1698-70c1-450f-c562-8d8d364f4952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(big_text, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print('{} -> {}'.format(big_text[0], [idx2syll[big_data[0][x][0]] for x in range(len(big_data[0]))]))\n",
        "print(x_test[0], y_test[0])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a remote control has all black and white buttons except for a yellow light \" button -> ['AH', 'R IH', 'M OW T', 'K AH N', 'T R OW L']\n",
            "a dirt walking path [[17146]\n",
            " [18972]\n",
            " [31128]\n",
            " [22768]\n",
            " [25454]]\n",
            "x_train shape: (74976,)\n",
            "x_test shape: (24992,)\n",
            "y_train shape: (74976, 5, 1)\n",
            "y_test shape: (24992, 5, 1)\n",
            "[17146]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "ad50e88d-e455-424f-de09-e1c59564cc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "# model loading bug\n",
        "metric_list = []\n",
        "metric_names = []\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "x = layers.RepeatVector(maxlen)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "if False:\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "#x = layers.Dense(units*2, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=50,\n",
        "          callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5),\n",
        "            ModelCheckpoint(model_file, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0709 03:45:15.618144 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0709 03:45:15.619668 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0709 03:45:16.650156 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0709 03:45:16.659564 139753104537472 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0709 03:45:17.122013 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0709 03:45:17.710000 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0709 03:45:17.731807 139753104537472 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 35000)          17955000  \n",
            "=================================================================\n",
            "Total params: 20,056,248\n",
            "Trainable params: 20,056,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0709 03:45:20.777346 139753104537472 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 74976 samples, validate on 24992 samples\n",
            "Epoch 1/50\n",
            " - 121s - loss: 4.3641 - val_loss: 2.9491\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.94912, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 2/50\n",
            " - 116s - loss: 2.8316 - val_loss: 2.3398\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.94912 to 2.33982, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 3/50\n",
            " - 116s - loss: 2.4331 - val_loss: 2.0836\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.33982 to 2.08361, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 4/50\n",
            " - 116s - loss: 2.2158 - val_loss: 1.9377\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.08361 to 1.93772, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 5/50\n",
            " - 116s - loss: 2.0706 - val_loss: 1.8423\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.93772 to 1.84229, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 6/50\n",
            " - 116s - loss: 1.9643 - val_loss: 1.7701\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.84229 to 1.77008, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 7/50\n",
            " - 116s - loss: 1.8815 - val_loss: 1.7229\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.77008 to 1.72294, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 8/50\n",
            " - 116s - loss: 1.8117 - val_loss: 1.6790\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.72294 to 1.67895, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 9/50\n",
            " - 119s - loss: 1.7517 - val_loss: 1.6474\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.67895 to 1.64738, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 10/50\n",
            " - 116s - loss: 1.7012 - val_loss: 1.6168\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.64738 to 1.61680, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 11/50\n",
            " - 116s - loss: 1.6605 - val_loss: 1.5935\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.61680 to 1.59347, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 12/50\n",
            " - 116s - loss: 1.6248 - val_loss: 1.5804\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.59347 to 1.58043, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 13/50\n",
            " - 116s - loss: 1.5877 - val_loss: 1.5592\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.58043 to 1.55922, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 14/50\n",
            " - 116s - loss: 1.5592 - val_loss: 1.5541\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.55922 to 1.55412, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 15/50\n",
            " - 116s - loss: 1.5338 - val_loss: 1.5413\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.55412 to 1.54126, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 16/50\n",
            " - 116s - loss: 1.5062 - val_loss: 1.5293\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.54126 to 1.52928, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 17/50\n",
            " - 116s - loss: 1.4851 - val_loss: 1.5214\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.52928 to 1.52137, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 18/50\n",
            " - 116s - loss: 1.4637 - val_loss: 1.5101\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.52137 to 1.51014, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 19/50\n",
            " - 116s - loss: 1.4489 - val_loss: 1.5140\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.51014\n",
            "Epoch 20/50\n",
            " - 116s - loss: 1.4262 - val_loss: 1.5065\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51014 to 1.50651, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 21/50\n",
            " - 119s - loss: 1.4113 - val_loss: 1.5046\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.50651 to 1.50457, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 22/50\n",
            " - 115s - loss: 1.3941 - val_loss: 1.4975\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.50457 to 1.49750, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 23/50\n",
            " - 116s - loss: 1.3820 - val_loss: 1.4969\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.49750 to 1.49693, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 24/50\n",
            " - 116s - loss: 1.3692 - val_loss: 1.4990\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.49693\n",
            "Epoch 25/50\n",
            " - 115s - loss: 1.3583 - val_loss: 1.4940\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.49693 to 1.49403, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 26/50\n",
            " - 116s - loss: 1.3458 - val_loss: 1.4904\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.49403 to 1.49045, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 27/50\n",
            " - 116s - loss: 1.3350 - val_loss: 1.4880\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.49045 to 1.48802, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 28/50\n",
            " - 116s - loss: 1.3265 - val_loss: 1.4856\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.48802 to 1.48559, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 29/50\n",
            " - 116s - loss: 1.3180 - val_loss: 1.4871\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.48559\n",
            "Epoch 30/50\n",
            " - 115s - loss: 1.3077 - val_loss: 1.4817\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.48559 to 1.48174, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 31/50\n",
            " - 115s - loss: 1.3018 - val_loss: 1.4847\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.48174\n",
            "Epoch 32/50\n",
            " - 116s - loss: 1.2954 - val_loss: 1.4796\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.48174 to 1.47965, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 33/50\n",
            " - 119s - loss: 1.2824 - val_loss: 1.4810\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.47965\n",
            "Epoch 34/50\n",
            " - 115s - loss: 1.2764 - val_loss: 1.4888\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.47965\n",
            "Epoch 35/50\n",
            " - 115s - loss: 1.2681 - val_loss: 1.4849\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.47965\n",
            "Epoch 36/50\n",
            " - 115s - loss: 1.2651 - val_loss: 1.4785\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.47965 to 1.47852, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 37/50\n",
            " - 116s - loss: 1.2596 - val_loss: 1.4790\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.47852\n",
            "Epoch 38/50\n",
            " - 115s - loss: 1.2523 - val_loss: 1.4821\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.47852\n",
            "Epoch 39/50\n",
            " - 115s - loss: 1.2458 - val_loss: 1.4854\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.47852\n",
            "Epoch 40/50\n",
            " - 115s - loss: 1.2377 - val_loss: 1.4773\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.47852 to 1.47729, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 41/50\n",
            " - 115s - loss: 1.2382 - val_loss: 1.4690\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.47729 to 1.46897, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n",
            "Epoch 42/50\n",
            " - 115s - loss: 1.2296 - val_loss: 1.4844\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.46897\n",
            "Epoch 43/50\n",
            " - 115s - loss: 1.2265 - val_loss: 1.4804\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.46897\n",
            "Epoch 44/50\n",
            " - 115s - loss: 1.2204 - val_loss: 1.4863\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.46897\n",
            "Epoch 45/50\n",
            " - 119s - loss: 1.2160 - val_loss: 1.4843\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.46897\n",
            "Epoch 46/50\n",
            " - 114s - loss: 1.2161 - val_loss: 1.4867\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.46897\n",
            "Epoch 00046: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "ef15d313-9a2c-4ee4-ae3d-a031eaf0487e"
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  names = metric_names + ['loss']\n",
        "  # summarize history for accuracy\n",
        "  for m in names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(names, loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "b03cd653-9a3d-429d-dd88-b4a3c0376d09"
      },
      "source": [
        "\n",
        "\n",
        "x_short = big_text[0:500]\n",
        "y_short = big_data[0:500]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "    #K.manual_variable_initialization(True)  \n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # need this?\n",
        "    session.run(tf.tables_initializer())\n",
        "    #model = load_model(model_file)  \n",
        "    model.load_weights(model_file)  \n",
        "    #eval = model.evaluate(x_test, y_test)\n",
        "    #print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "    predicts = model.predict(x_short, batch_size=32)\n",
        "    print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape: (500, 5, 35000)\n",
            "5\n",
            "35000\n",
            "[[1.33180915e-08 1.32330884e-08 1.32735094e-08 ... 1.34967637e-08\n",
            "  1.33160079e-08 1.34514044e-08]\n",
            " [4.67115058e-09 4.62579175e-09 4.57032145e-09 ... 4.59637262e-09\n",
            "  4.68835992e-09 4.60506877e-09]\n",
            " [1.02781028e-09 9.94346494e-10 1.02036435e-09 ... 1.00451625e-09\n",
            "  1.00161401e-09 1.01579345e-09]\n",
            " [3.50427846e-11 3.45523089e-11 3.48043122e-11 ... 3.51791304e-11\n",
            "  3.44400619e-11 3.50992430e-11]\n",
            " [1.21886834e-10 1.19110569e-10 1.20558521e-10 ... 1.21527011e-10\n",
            "  1.20202168e-10 1.21144456e-10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmYm4s4AJ6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "5424299e-9e4c-492a-81f5-8b387f06f639"
      },
      "source": [
        "for i in range(20):\n",
        "    rep = report([y_short[i]], [predicts[i]])\n",
        "    print(rep)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sparse': 1.0, 'perfect': 1.0, 'sparse5': 1.0, 'perfect5': 1.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.2, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.2, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n",
            "{'sparse': 0.0, 'perfect': 0.0, 'sparse5': 0.0, 'perfect5': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "546d6829-7ae3-4e6e-de3f-adf6d1f8aabf"
      },
      "source": [
        "top_k=10\n",
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "print('top preds: ', mini_preds[0])\n",
        "print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    encoded = decoder.get_sentences(fs.scorepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    d = []\n",
        "    for x in encoded[0]:\n",
        "        for y in x:\n",
        "            if len(y) > 0:\n",
        "                d.append(y)   \n",
        "    if len(d) > 0:\n",
        "        print(x_short[i], ':')\n",
        "        print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(d)\n",
        "        sentences = {}\n",
        "        for d in decoded:\n",
        "            key = ' '.join(d)\n",
        "            sentences[key] = d\n",
        "        print('[{}]  -> ', i,list(sentences.keys()))\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top preds:  [[17146 17113 19415 30278 17198 17708 31345 25322 17021 31119]\n",
            " [25165 28201 19746 21980 31119 17214 17146 30278 30590 31313]\n",
            " [31313 23888 26903 29813 31119 28201 26402 18957 17146 19919]\n",
            " [28201 31313 26903 19481 19919 29824 26612 28673 26606 24391]\n",
            " [26903 28201 29025 29030 26905 31313 29824 22509 19413 23874]]\n",
            "(1024, 5, 10)\n",
            "a remote control has all black and white buttons except for a yellow light \" button -> ['AH', 'N OW', 'W EY', 'S T AA P', 'S AY N']\n",
            "encoded[0]:  []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "a38931ac-dfe7-4baf-932a-718cb8103411"
      },
      "source": [
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   160/100000 [..............................] - ETA: 9:02 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f1a67fadf60>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100000/100000 [==============================] - 66s 659us/step\n",
            "model.evaluate on haiku clauses:  ['loss'] 0.6009694620466233\n",
            "history:  <keras.callbacks.History object at 0x7f1a68955c18>\n",
            "100000/100000 [==============================] - 67s 674us/step\n",
            "model.evaluate on long clauses:  ['loss'] 0.8417476171016693\n",
            "history:  <keras.callbacks.History object at 0x7f1a68955c18>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50a619c2-7c2a-41de-e0ef-a7e962457552"
      },
      "source": [
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "short {'sparse': 0.867578124999999, 'perfect': 0.662109375, 'sparse5': 0.9503906249999989, 'perfect5': 0.853515625}\n",
            "[0] {'sparse': 0.8023437499999986, 'perfect': 0.53515625, 'sparse5': 0.9369140624999993, 'perfect5': 0.8056640625}\n",
            "[1024] {'sparse': 0.8035156249999977, 'perfect': 0.5263671875, 'sparse5': 0.9304687499999987, 'perfect5': 0.794921875}\n",
            "[2048] {'sparse': 0.8087890624999998, 'perfect': 0.5537109375, 'sparse5': 0.9359374999999988, 'perfect5': 0.80859375}\n",
            "[3072] {'sparse': 0.7818359374999992, 'perfect': 0.4892578125, 'sparse5': 0.9236328124999993, 'perfect5': 0.7822265625}\n",
            "[4096] {'sparse': 0.8068359374999994, 'perfect': 0.533203125, 'sparse5': 0.9363281249999988, 'perfect5': 0.791015625}\n",
            "[5120] {'sparse': 0.8005859374999991, 'perfect': 0.5166015625, 'sparse5': 0.9285156249999992, 'perfect5': 0.798828125}\n",
            "[6144] {'sparse': 0.8093749999999987, 'perfect': 0.5419921875, 'sparse5': 0.938476562499999, 'perfect5': 0.8193359375}\n",
            "[7168] {'sparse': 0.7951171874999992, 'perfect': 0.5244140625, 'sparse5': 0.928710937499999, 'perfect5': 0.7822265625}\n",
            "[8192] {'sparse': 0.8021484374999982, 'perfect': 0.517578125, 'sparse5': 0.9349609374999984, 'perfect5': 0.8037109375}\n",
            "[9216] {'sparse': 0.813476562499999, 'perfect': 0.560546875, 'sparse5': 0.9374999999999991, 'perfect5': 0.8154296875}\n",
            "[10240] {'sparse': 0.8052734374999994, 'perfect': 0.5302734375, 'sparse5': 0.9406249999999992, 'perfect5': 0.8134765625}\n",
            "[11264] {'sparse': 0.8080078124999992, 'perfect': 0.5439453125, 'sparse5': 0.9374999999999989, 'perfect5': 0.8154296875}\n",
            "[12288] {'sparse': 0.8115234374999992, 'perfect': 0.560546875, 'sparse5': 0.9353515624999991, 'perfect5': 0.8173828125}\n",
            "[13312] {'sparse': 0.7968749999999988, 'perfect': 0.51171875, 'sparse5': 0.9382812499999983, 'perfect5': 0.8046875}\n",
            "[14336] {'sparse': 0.806054687499999, 'perfect': 0.529296875, 'sparse5': 0.9410156249999977, 'perfect5': 0.80859375}\n",
            "[15360] {'sparse': 0.7816406249999992, 'perfect': 0.4951171875, 'sparse5': 0.9292968749999986, 'perfect5': 0.779296875}\n",
            "[16384] {'sparse': 0.794531249999999, 'perfect': 0.5068359375, 'sparse5': 0.9275390624999984, 'perfect5': 0.7841796875}\n",
            "[17408] {'sparse': 0.8138671874999993, 'perfect': 0.548828125, 'sparse5': 0.9357421874999983, 'perfect5': 0.806640625}\n",
            "[18432] {'sparse': 0.7970703124999986, 'perfect': 0.5244140625, 'sparse5': 0.9306640624999988, 'perfect5': 0.787109375}\n",
            "[19456] {'sparse': 0.8029296874999996, 'perfect': 0.5244140625, 'sparse5': 0.9310546874999986, 'perfect5': 0.79296875}\n",
            "[20480] {'sparse': 0.8060546874999996, 'perfect': 0.5283203125, 'sparse5': 0.9363281249999995, 'perfect5': 0.8076171875}\n",
            "[21504] {'sparse': 0.8070312499999982, 'perfect': 0.541015625, 'sparse5': 0.935546874999999, 'perfect5': 0.8056640625}\n",
            "[22528] {'sparse': 0.8113281249999981, 'perfect': 0.521484375, 'sparse5': 0.9402343749999982, 'perfect5': 0.81640625}\n",
            "[23552] {'sparse': 0.7857421874999989, 'perfect': 0.4990234375, 'sparse5': 0.9324218749999983, 'perfect5': 0.7919921875}\n",
            "[24576] {'sparse': 0.7986328124999993, 'perfect': 0.5185546875, 'sparse5': 0.9330078124999986, 'perfect5': 0.7919921875}\n",
            "[25600] {'sparse': 0.7992187499999994, 'perfect': 0.513671875, 'sparse5': 0.9324218749999993, 'perfect5': 0.7998046875}\n",
            "[26624] {'sparse': 0.8042968749999992, 'perfect': 0.5166015625, 'sparse5': 0.9341796874999987, 'perfect5': 0.8037109375}\n",
            "[27648] {'sparse': 0.7869140624999993, 'perfect': 0.494140625, 'sparse5': 0.9330078124999986, 'perfect5': 0.810546875}\n",
            "[28672] {'sparse': 0.7908203124999997, 'perfect': 0.525390625, 'sparse5': 0.9277343749999989, 'perfect5': 0.7939453125}\n",
            "[29696] {'sparse': 0.8072265624999991, 'perfect': 0.5380859375, 'sparse5': 0.9335937499999993, 'perfect5': 0.80078125}\n",
            "[30720] {'sparse': 0.8068359374999988, 'perfect': 0.5380859375, 'sparse5': 0.9363281249999982, 'perfect5': 0.8115234375}\n",
            "[31744] {'sparse': 0.7949218749999984, 'perfect': 0.5146484375, 'sparse5': 0.9292968749999989, 'perfect5': 0.7998046875}\n",
            "[32768] {'sparse': 0.796289062499999, 'perfect': 0.5302734375, 'sparse5': 0.9337890624999992, 'perfect5': 0.80078125}\n",
            "[33792] {'sparse': 0.8083984374999992, 'perfect': 0.541015625, 'sparse5': 0.9349609374999989, 'perfect5': 0.8125}\n",
            "[34816] {'sparse': 0.802148437499999, 'perfect': 0.5234375, 'sparse5': 0.9345703124999989, 'perfect5': 0.8037109375}\n",
            "[35840] {'sparse': 0.7955078124999985, 'perfect': 0.51953125, 'sparse5': 0.9257812499999984, 'perfect5': 0.794921875}\n",
            "[36864] {'sparse': 0.8105468749999988, 'perfect': 0.537109375, 'sparse5': 0.9394531249999986, 'perfect5': 0.8056640625}\n",
            "[37888] {'sparse': 0.7982421874999989, 'perfect': 0.49609375, 'sparse5': 0.9378906249999983, 'perfect5': 0.8056640625}\n",
            "[38912] {'sparse': 0.8037109374999993, 'perfect': 0.53515625, 'sparse5': 0.9339843749999988, 'perfect5': 0.8076171875}\n",
            "[39936] {'sparse': 0.8005859374999993, 'perfect': 0.5419921875, 'sparse5': 0.9333984374999991, 'perfect5': 0.806640625}\n",
            "[40960] {'sparse': 0.8130859374999991, 'perfect': 0.548828125, 'sparse5': 0.9394531249999987, 'perfect5': 0.806640625}\n",
            "[41984] {'sparse': 0.7947265624999992, 'perfect': 0.521484375, 'sparse5': 0.9363281249999994, 'perfect5': 0.8134765625}\n",
            "[43008] {'sparse': 0.7906249999999989, 'perfect': 0.5009765625, 'sparse5': 0.931640624999999, 'perfect5': 0.8017578125}\n",
            "[44032] {'sparse': 0.8111328124999989, 'perfect': 0.55078125, 'sparse5': 0.9363281249999988, 'perfect5': 0.8212890625}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e013d850fca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiglen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbigbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[{}] {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtext5arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perfect5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-897cecd1a27e>\u001b[0m in \u001b[0;36mreport\u001b[0;34m(data, prediction)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(len(short[n]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mgood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-897cecd1a27e>\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(data, prediction)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtopind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtopind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mgood\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \"\"\"\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}