{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "4cfff930-4ae5-48be-8527-f8c342aa6e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "a8cd7cd7-ee70-4148-86e8-4fc932d3f967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 35000\n",
        "# longest output sentence\n",
        "maxlen = 5\n",
        "batch_size = 32\n",
        "deduplicate_haiku=True\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_base=\"/content/model_haiku_5_\"\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "59cf5aa4-0fae-49a6-ef34-ecb71bf6ebde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!date\n",
        "word2sylls = trim_homynyms(word2sylls)\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 10 04:45:13 UTC 2019\n",
            "19433 0\n",
            "# features:  32088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "a179109d-a9de-470d-f5aa-961397074230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "big_text = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            continue\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            if word in word2sylls:\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "        if len(_sylls) != 5:\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        big_text.append(_text)\n",
        "        big_haiku.append(_haiku)\n",
        "        big_data.append(_data)\n",
        "        #if len(big_text) == max_data * 2:\n",
        "        #    break\n",
        "\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "big_data = np.expand_dims(big_data, -1)\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "shuffle = np.arange(len(big_text))\n",
        "print(shuffle)\n",
        "np.random.shuffle(shuffle)\n",
        "shuffle = shuffle[0:max_data]\n",
        "big_text = big_text[shuffle]\n",
        "big_haiku = big_haiku[shuffle]\n",
        "big_data = big_data[shuffle]\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "office equipment -> office equipment : [[17201]\n",
            " [19722]\n",
            " [21958]\n",
            " [23563]\n",
            " [24373]]\n",
            "[    0     1     2 ... 49711 49712 49713]\n",
            "at outdoor event -> at outdoor event : [[17140]\n",
            " [17275]\n",
            " [18834]\n",
            " [21958]\n",
            " [30812]]\n",
            "Full length clauses:  49714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "7675c427-ce0c-4456-a05e-700a26c057ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(big_text, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print('{} -> {}'.format(big_text[0], [idx2syll[big_data[0][x][0]] for x in range(len(big_data[0]))]))\n",
        "print(x_test[0], y_test[0])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at outdoor event -> ['AE T', 'AW T', 'D AO R', 'IH', 'V EH N T']\n",
            "to pick up passengers [[25674]\n",
            " [17182]\n",
            " [25418]\n",
            " [26821]\n",
            " [22231]]\n",
            "x_train shape: (37280,)\n",
            "x_test shape: (12416,)\n",
            "y_train shape: (37280, 5, 1)\n",
            "y_test shape: (12416, 5, 1)\n",
            "[25674]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "8fa53234-10b4-4531-8e34-ac664c6386a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "# model loading bug\n",
        "metric_list = []\n",
        "metric_names = []\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "x = layers.RepeatVector(maxlen)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "if False:\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "#x = layers.Dense(units*2, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=True\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=50,\n",
        "          callbacks=[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5),\n",
        "            ModelCheckpoint(model_file, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0710 04:45:33.445542 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0710 04:45:33.447738 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0710 04:45:34.654676 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0710 04:45:34.667347 139644452747136 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0710 04:45:35.230900 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0710 04:45:35.986230 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0710 04:45:36.014534 139644452747136 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 35000)          17955000  \n",
            "=================================================================\n",
            "Total params: 20,056,248\n",
            "Trainable params: 20,056,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  names = metric_names + ['loss']\n",
        "  # summarize history for accuracy\n",
        "  for m in names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(names, loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "outputId": "2f398849-97d9-4e17-bf79-0073e9447ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "\n",
        "\n",
        "x_short = big_text[0:500]\n",
        "y_short = big_data[0:500]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "    #K.manual_variable_initialization(True)  \n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # need this?\n",
        "    session.run(tf.tables_initializer())\n",
        "    #model = load_model(model_file)  \n",
        "    model.load_weights(model_file)  \n",
        "    #eval = model.evaluate(x_test, y_test)\n",
        "    #print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "    predicts = model.predict(x_short, batch_size=32)\n",
        "    print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f0126b1a898>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape: (500, 5, 35000)\n",
            "5\n",
            "35000\n",
            "[[4.5495265e-09 4.5562132e-09 4.6079331e-09 ... 4.6187392e-09\n",
            "  4.5530855e-09 4.5695461e-09]\n",
            " [1.5056644e-09 1.4802953e-09 1.4802557e-09 ... 1.4648727e-09\n",
            "  1.4978360e-09 1.4740406e-09]\n",
            " [1.3513289e-10 1.3676700e-10 1.3562866e-10 ... 1.3446491e-10\n",
            "  1.3682595e-10 1.3748520e-10]\n",
            " [3.3589448e-13 3.4021128e-13 3.4355122e-13 ... 3.5025905e-13\n",
            "  3.2947627e-13 3.3955133e-13]\n",
            " [4.5031624e-13 4.4497568e-13 4.4666273e-13 ... 4.4437604e-13\n",
            "  4.3640737e-13 4.3142670e-13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmYm4s4AJ6l",
        "colab_type": "code",
        "outputId": "35160312-c6a6-4342-f523-fa4c846f427f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(len(y_short)):\n",
        "    rep = report([y_short[i]], [predicts[i]])\n",
        "    #print(rep)\n",
        "    if rep['perfect5'] == 1.0:\n",
        "        f = find_top_k_match(y_short[i], predicts[i], 5)\n",
        "        print('{} -> {}'.format(x_short[i], [decoder.idx2syll[j] for j in f]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "at outdoor event -> ['AE T', 'AW T', 'D AO R', 'IH', 'V EH N T']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "outputId": "66601be5-1557-4adf-b8d9-202f6ab0a185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "source": [
        "top_k=5\n",
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "#print('top preds: ', mini_preds[0])\n",
        "#print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    print('{} -> {}'.format(x_short[i], [decoder.idx2syll[x] for x in fs.scorepaths[0]]))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    print(mini_preds[i].shape)\n",
        "    print(morepaths.shape)\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print(fs.scorepaths[j])\n",
        "        #z = mini_preds[i][np.arange(maxlen), fs.scorepaths[j]]\n",
        "        #print(z)\n",
        "        morepaths[j] = mini_preds[i][np.arange(maxlen), fs.scorepaths[j]]\n",
        "    #print(morepaths[0])\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    #d = []\n",
        "    # for x in encoded:\n",
        "    #   for y in x:\n",
        "    #        if len(y) > 0:\n",
        "    #           d.append(y[0])  \n",
        "    #d = np.array(d)\n",
        "    d = encoded\n",
        "    if len(d) > 0:\n",
        "        print('encoded sentences: ', d)\n",
        "        print(x_short[i], ':')\n",
        "        #print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(encoded)\n",
        "        #print('len(decoded): ', len(decoded))\n",
        "        ##print('len(decoded[0]): ', len(decoded[0]))\n",
        "        #print('len(decoded[0][0]): ', len(decoded[0][0]))\n",
        "        #print('len(decoded[0][0][0]): ', len(decoded[0][0][0]))\n",
        "        #print('decoded[0][0][0]: ', decoded[0][0][0])\n",
        "        sentences = {}\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    #print('d3: ', d3)\n",
        "                    #break\n",
        "                    key = ' '.join(d3)\n",
        "                    sentences[key] = d3\n",
        "                    break\n",
        "                     \n",
        "        print('[{}]  -> {}', i,list(sentences.keys())[0:10])\n",
        "    break\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top preds:  [[21980 17140 17021 17275 17146]\n",
            " [17275 18834 17146 17113 21980]\n",
            " [18834 21958 17275 30812 29894]\n",
            " [21958 23563 29025 18834 26934]\n",
            " [30812 24373 29348 29025 17175]]\n",
            "(500, 5, 5)\n",
            "at outdoor event -> ['IH N', 'AW T', 'D AO R', 'IH', 'V EH N T']\n",
            "at outdoor event -> ['syll0', 'syll4', 'syll3', 'syll2', 'syll0']\n",
            "(5, 5)\n",
            "(125, 5)\n",
            "encoded[0]:  [[[148358, 148358, 204333, 190142, 204333]], [[148358, 150159, 190142, 204333]]]\n",
            "encoded sentences:  [[[[148358, 148358, 204333, 190142, 204333]], [[148358, 150159, 190142, 204333]]], [[[148358, 148358, 199396, 190142, 204333]]], [[[148358, 148358, 204333, 190142, 189994]], [[148358, 150159, 190142, 189994]]], [[[148358, 148358, 204333, 190142, 190142]], [[148358, 150159, 190142, 190142]]], [], [[[148358, 148358, 199396, 190142, 189994]]], [[[148358, 148358, 199396, 190142, 190142]]], [], [[[148358, 127883, 204333, 190142, 204333]], [[148766, 204333, 190142, 204333]]], [], [[[148358, 148358, 204333, 188983]], [[148358, 150159, 188983]]], [[[148358, 127883, 199396, 190142, 204333]], [[148766, 199396, 190142, 204333]]], [], [[[148358, 127883, 204333, 190142, 189994]], [[148766, 204333, 190142, 189994]]], [[[171343, 148358, 204333, 190142, 204333]], [[171343, 150159, 190142, 204333]]], [[[148358, 171343, 204333, 190142, 204333]]], [[[148358, 148358, 199396, 188983]]], [[[148358, 201871, 204333, 190142, 204333]]], [[[201871, 148358, 204333, 190142, 204333]], [[201871, 150159, 190142, 204333]]], [[[105419, 148358, 204333, 190142, 204333]], [[105419, 150159, 190142, 204333]]], [[[148358, 103576, 204333, 190142, 204333]]], [[[148358, 127883, 204333, 190142, 190142]], [[148766, 204333, 190142, 190142]]], [[[170472, 148358, 204333, 190142, 204333]], [[170472, 150159, 190142, 204333]]], [], [[[148358, 148358, 204333, 179313, 204333]], [[148358, 150159, 179313, 204333]]], [[[148358, 127883, 199396, 190142, 189994]], [[148766, 199396, 190142, 189994]]], [[[171343, 148358, 199396, 190142, 204333]]], [[[148358, 171343, 199396, 190142, 204333]]], [[[148358, 201871, 199396, 190142, 204333]]], [[[105419, 148358, 199396, 190142, 204333]]], [[[148358, 103576, 199396, 190142, 204333]]], [[[148358, 127883, 199396, 190142, 190142]], [[148766, 199396, 190142, 190142]]], [[[170472, 148358, 199396, 190142, 204333]]], [], [[[171343, 148358, 204333, 190142, 189994]], [[171343, 150159, 190142, 189994]]], [[[148358, 171343, 204333, 190142, 189994]]], [[[148358, 201871, 204333, 190142, 189994]]], [[[105419, 148358, 204333, 190142, 189994]], [[105419, 150159, 190142, 189994]]], [[[148358, 103576, 204333, 190142, 189994]]], [[[148358, 148358, 199396, 179313, 204333]]], [[[170472, 148358, 204333, 190142, 189994]], [[170472, 150159, 190142, 189994]]], [[[148358, 148358, 204333, 190142, 166748]], [[148358, 150159, 190142, 166748]]], [[[171343, 148358, 204333, 190142, 190142]], [[171343, 150159, 190142, 190142]]], [[[148358, 171343, 204333, 190142, 190142]]], [[[148358, 201871, 204333, 190142, 190142]]], [[[105419, 148358, 204333, 190142, 190142]], [[105419, 150159, 190142, 190142]]], [[[148358, 103576, 204333, 190142, 190142]]], [], [[[170472, 148358, 204333, 190142, 190142]], [[170472, 150159, 190142, 190142]]], [], [], [], [[[148358, 148358, 204333, 179313, 189994]], [[148358, 150159, 179313, 189994]]], [], [], [[[148358, 127883, 204333, 188983]], [[148766, 204333, 188983]]], [], [[[148358, 148358, 204333, 132570]], [[148358, 150159, 132570]]], [[[148358, 171343, 199396, 190142, 189994]]], [[[171343, 148358, 199396, 190142, 189994]]], [[[148358, 201871, 199396, 190142, 189994]]], [[[105419, 148358, 199396, 190142, 189994]]], [[[148358, 103576, 199396, 190142, 189994]]], [[[148358, 148358, 204333, 179313, 190142]], [[148358, 150159, 179313, 190142]]], [[[170472, 148358, 199396, 190142, 189994]]], [[[148358, 148358, 199396, 190142, 166748]]], [[[148358, 171343, 199396, 190142, 190142]]], [[[171343, 148358, 199396, 190142, 190142]]], [[[148358, 201871, 199396, 190142, 190142]]], [[[105419, 148358, 199396, 190142, 190142]]], [[[148358, 103576, 199396, 190142, 190142]]], [], [[[170472, 148358, 199396, 190142, 190142]]], [], [], [], [[[148358, 148358, 199396, 179313, 189994]]], [], [[[171343, 127883, 204333, 190142, 204333]], [[171376, 204333, 190142, 204333]]], [], [[[148358, 127883, 199396, 188983]], [[148766, 199396, 188983]]], [], [[[101109, 204333, 190142, 204333]], [[201871, 127883, 204333, 190142, 204333]]], [[[148358, 148358, 199396, 132570]]], [[[105419, 127883, 204333, 190142, 204333]]], [[[170472, 127883, 204333, 190142, 204333]]], [[[148358, 148358, 199396, 179313, 190142]]], [], [], [], [], [], [], [], [], [[[171343, 148358, 204333, 188983]], [[171343, 150159, 188983]]], [[[148358, 171343, 204333, 188983]]], [[[148358, 127883, 204333, 179313, 204333]], [[148766, 204333, 179313, 204333]]], [[[148358, 201871, 204333, 188983]]], [[[105419, 148358, 204333, 188983]], [[105419, 150159, 188983]]], [[[148358, 103576, 204333, 188983]]], [[[171343, 127883, 199396, 190142, 204333]], [[171376, 199396, 190142, 204333]]], [[[170472, 148358, 204333, 188983]], [[170472, 150159, 188983]]], [], [], [[[101109, 199396, 190142, 204333]], [[201871, 127883, 199396, 190142, 204333]]], [[[105419, 127883, 199396, 190142, 204333]]], [[[170472, 127883, 199396, 190142, 204333]]], [], [], [], [], [[[171343, 127883, 204333, 190142, 189994]], [[171376, 204333, 190142, 189994]]], [], [[[171343, 171343, 204333, 190142, 204333]]], [], [[[101109, 204333, 190142, 189994]], [[201871, 127883, 204333, 190142, 189994]]], [], [], [[[105419, 127883, 204333, 190142, 189994]]], [[[148358, 171343, 199396, 188983]]], [[[171343, 148358, 199396, 188983]]], [[[148358, 127883, 199396, 179313, 204333]], [[148766, 199396, 179313, 204333]]], [[[201871, 171343, 204333, 190142, 204333]]], [[[171343, 201871, 204333, 190142, 204333]]]]\n",
            "at outdoor event :\n",
            "[[21980 17140 17021 17275 17146]\n",
            " [17275 18834 17146 17113 21980]\n",
            " [18834 21958 17275 30812 29894]\n",
            " [21958 23563 29025 18834 26934]\n",
            " [30812 24373 29348 29025 17175]]\n",
            "len(decoded):  125\n",
            "len(decoded[0]):  2\n",
            "len(decoded[0][0]):  1\n",
            "len(decoded[0][0][0]):  5\n",
            "decoded[0][0][0]:  ['in.', 'in.', 'vent', 'shun', 'vent']\n",
            "[{}]  ->  0 ['in. in. vent shun vent', 'in. invent shun vent', 'in. in. ting shun vent', 'in. in. vent shun show', 'in. invent shun show', 'in. in. vent shun shun', 'in. invent shun shun', 'in. in. ting shun show', 'in. in. ting shun shun', 'in. dore vent shun vent']\n",
            "Total decoded: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "outputId": "0fe28333-b2c9-465f-bb00-7cc5f12e32aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "kwith tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-24ed0189a01e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    kwith tf.Session() as session:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}