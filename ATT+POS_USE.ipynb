{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+POS USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYd5dVdmTaK8",
        "colab_type": "code",
        "outputId": "8e8d009e-00dc-4a3c-862e-9ede4c261b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "#!pip install keras==2.1.2\n",
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model, Sequential\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.datasets import reuters\n",
        "from keras.preprocessing import text\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "max_features = 2000\n",
        "# cut texts after this number of words\n",
        "# (among top max_features most common words)\n",
        "maxlen = 30\n",
        "batch_size = 32\n",
        "\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5_same.txt\n",
        "haiku_data = 'haiku_5_same.txt'\n",
        "haiku_text = []\n",
        "with open(haiku_data) as f:\n",
        "    for line in f.readlines():\n",
        "        haiku_text.append(line.split('\\t')[0])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File ‘haiku_5_same.txt’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "4b23d2fc-80c2-4b35-fa68-545fbd1d2a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!date\n",
        "# haiku_text = haiku_text[0:3000]\n",
        "toki_file = './toki.json'\n",
        "train_toki = False\n",
        "try:\n",
        "    with open(toki_file, \"r\") as f:\n",
        "        conf = f.read()\n",
        "        toki = text.tokenizer_from_json(conf)\n",
        "        conf = None\n",
        "except:\n",
        "    toki = text.Tokenizer(max_features)\n",
        "    for h in haiku_text:\n",
        "        toki.fit_on_texts(text.text_to_word_sequence(h))\n",
        "    with open(toki_file, \"w\") as f:\n",
        "        f.write(toki.to_json())\n",
        "!date\n",
        "data_all = [[]] * len(haiku_text)\n",
        "for i in range(len(haiku_text)):    \n",
        "    seq = text.text_to_word_sequence(haiku_text[i])\n",
        "    data_all[i] = toki.texts_to_sequences(seq)\n",
        "\n",
        "!date   \n",
        "print(haiku_text[0])\n",
        "print(data_all[0])\n",
        "print(haiku_text[2999])\n",
        "print(data_all[2999])\n",
        "!date\n",
        "#(x_train, x_test, y_train, y_test) = train_test_split(haiku_text, data_all)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun 22 05:31:50 UTC 2019\n",
            "Sat Jun 22 05:45:27 UTC 2019\n",
            "Sat Jun 22 05:45:35 UTC 2019\n",
            "a abandon church\n",
            "[[1], [], [374]]\n",
            "a cardboard cutout\n",
            "[[1], [1117], []]\n",
            "Sat Jun 22 05:45:37 UTC 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "e8747c8e-f977-468d-abac-7473b3f94e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(haiku_text, data_all)\n",
        "print(x_train[0][0], y_train[0][0])\n",
        "print(x_test[0][0], y_test[0][0])\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "def pack(indexes, maxlen):\n",
        "    print(type(indexes))\n",
        "    print(type(indexes[0]))\n",
        "    print(type(indexes[0][0]))\n",
        "    print(len(indexes[0][0]))\n",
        "    ilen = len(indexes)\n",
        "    out = np.zeros((ilen, maxlen))\n",
        "    for i in range(ilen):\n",
        "        for j in range(len(indexes[i])):\n",
        "            #print(i,j, len(indexes[i]))  \n",
        "            if len(indexes[i][j]) > 0:\n",
        "                out[i][j] = indexes[i][j][0]\n",
        "    return out\n",
        "    \n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "y_train = np.expand_dims(pack(y_train, maxlen), -1)\n",
        "y_test = np.expand_dims(pack(y_test, maxlen), -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "o [3]\n",
            "a [1]\n",
            "Pad sequences (samples x time)\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "1\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "1\n",
            "x_train shape: (71716,)\n",
            "x_test shape: (23906,)\n",
            "y_train shape: (71716, 30, 1)\n",
            "y_test shape: (23906, 30, 1)\n",
            "[1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2104
        },
        "outputId": "3c2c382d-2061-474a-9205-0bc290bf0e13"
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size/3\n",
        "units=32\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "x = layers.RepeatVector(maxlen)(x)\n",
        "#x = Dropout(0.1)\n",
        "if False:\n",
        "    x = AttentionBlock(units*2, k_size=units_k, v_size=units_v)(x)\n",
        "    # x = layers.Dropout(0.1)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "# x = layers.Dropout(0.1)(x)\n",
        "output_matrix = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_matrix)\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=[sparse, perfect])\n",
        "model.summary()\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 30, 512)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 30, 32)            69888     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 30, 2000)          66000     \n",
            "=================================================================\n",
            "Total params: 135,888\n",
            "Trainable params: 135,888\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Train on 71716 samples, validate on 23906 samples\n",
            "Epoch 1/50\n",
            "71716/71716 [==============================] - 36s 509us/step - loss: 0.9247 - sparse: 0.8862 - perfect: 0.0026 - val_loss: 0.6681 - val_sparse: 0.8916 - val_perfect: 0.0031\n",
            "Epoch 2/50\n",
            "71716/71716 [==============================] - 34s 471us/step - loss: 0.6149 - sparse: 0.8938 - perfect: 0.0053 - val_loss: 0.5679 - val_sparse: 0.8973 - val_perfect: 0.0068\n",
            "Epoch 3/50\n",
            "71716/71716 [==============================] - 34s 472us/step - loss: 0.5348 - sparse: 0.8997 - perfect: 0.0079 - val_loss: 0.5051 - val_sparse: 0.9031 - val_perfect: 0.0092\n",
            "Epoch 4/50\n",
            "71716/71716 [==============================] - 34s 470us/step - loss: 0.4797 - sparse: 0.9053 - perfect: 0.0111 - val_loss: 0.4606 - val_sparse: 0.9081 - val_perfect: 0.0134\n",
            "Epoch 5/50\n",
            "71716/71716 [==============================] - 34s 469us/step - loss: 0.4401 - sparse: 0.9099 - perfect: 0.0153 - val_loss: 0.4291 - val_sparse: 0.9116 - val_perfect: 0.0171\n",
            "Epoch 6/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.4114 - sparse: 0.9132 - perfect: 0.0194 - val_loss: 0.4055 - val_sparse: 0.9147 - val_perfect: 0.0198\n",
            "Epoch 7/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.3897 - sparse: 0.9158 - perfect: 0.0227 - val_loss: 0.3872 - val_sparse: 0.9167 - val_perfect: 0.0234\n",
            "Epoch 8/50\n",
            "71716/71716 [==============================] - 34s 470us/step - loss: 0.3725 - sparse: 0.9180 - perfect: 0.0265 - val_loss: 0.3735 - val_sparse: 0.9183 - val_perfect: 0.0272\n",
            "Epoch 9/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.3584 - sparse: 0.9200 - perfect: 0.0305 - val_loss: 0.3609 - val_sparse: 0.9203 - val_perfect: 0.0307\n",
            "Epoch 10/50\n",
            "71716/71716 [==============================] - 33s 466us/step - loss: 0.3463 - sparse: 0.9216 - perfect: 0.0347 - val_loss: 0.3518 - val_sparse: 0.9215 - val_perfect: 0.0328\n",
            "Epoch 11/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.3359 - sparse: 0.9231 - perfect: 0.0387 - val_loss: 0.3415 - val_sparse: 0.9228 - val_perfect: 0.0379\n",
            "Epoch 12/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.3268 - sparse: 0.9245 - perfect: 0.0428 - val_loss: 0.3341 - val_sparse: 0.9243 - val_perfect: 0.0413\n",
            "Epoch 13/50\n",
            "71716/71716 [==============================] - 33s 461us/step - loss: 0.3189 - sparse: 0.9257 - perfect: 0.0479 - val_loss: 0.3279 - val_sparse: 0.9250 - val_perfect: 0.0447\n",
            "Epoch 14/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.3119 - sparse: 0.9267 - perfect: 0.0506 - val_loss: 0.3226 - val_sparse: 0.9258 - val_perfect: 0.0474\n",
            "Epoch 15/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.3056 - sparse: 0.9277 - perfect: 0.0539 - val_loss: 0.3162 - val_sparse: 0.9269 - val_perfect: 0.0514\n",
            "Epoch 16/50\n",
            "71716/71716 [==============================] - 33s 461us/step - loss: 0.3000 - sparse: 0.9286 - perfect: 0.0584 - val_loss: 0.3124 - val_sparse: 0.9272 - val_perfect: 0.0548\n",
            "Epoch 17/50\n",
            "71716/71716 [==============================] - 34s 468us/step - loss: 0.2949 - sparse: 0.9293 - perfect: 0.0621 - val_loss: 0.3075 - val_sparse: 0.9280 - val_perfect: 0.0574\n",
            "Epoch 18/50\n",
            "71716/71716 [==============================] - 35s 481us/step - loss: 0.2903 - sparse: 0.9302 - perfect: 0.0651 - val_loss: 0.3038 - val_sparse: 0.9285 - val_perfect: 0.0600\n",
            "Epoch 19/50\n",
            "71716/71716 [==============================] - 35s 481us/step - loss: 0.2861 - sparse: 0.9309 - perfect: 0.0685 - val_loss: 0.3037 - val_sparse: 0.9285 - val_perfect: 0.0587\n",
            "Epoch 20/50\n",
            "71716/71716 [==============================] - 34s 472us/step - loss: 0.2822 - sparse: 0.9315 - perfect: 0.0712 - val_loss: 0.2978 - val_sparse: 0.9297 - val_perfect: 0.0648\n",
            "Epoch 21/50\n",
            "71716/71716 [==============================] - 34s 470us/step - loss: 0.2786 - sparse: 0.9321 - perfect: 0.0746 - val_loss: 0.2940 - val_sparse: 0.9304 - val_perfect: 0.0681\n",
            "Epoch 22/50\n",
            "71716/71716 [==============================] - 34s 472us/step - loss: 0.2753 - sparse: 0.9328 - perfect: 0.0774 - val_loss: 0.2922 - val_sparse: 0.9309 - val_perfect: 0.0708\n",
            "Epoch 23/50\n",
            "71716/71716 [==============================] - 33s 467us/step - loss: 0.2723 - sparse: 0.9332 - perfect: 0.0793 - val_loss: 0.2895 - val_sparse: 0.9311 - val_perfect: 0.0720\n",
            "Epoch 24/50\n",
            "71716/71716 [==============================] - 34s 468us/step - loss: 0.2694 - sparse: 0.9337 - perfect: 0.0838 - val_loss: 0.2873 - val_sparse: 0.9315 - val_perfect: 0.0744\n",
            "Epoch 25/50\n",
            "71716/71716 [==============================] - 34s 467us/step - loss: 0.2669 - sparse: 0.9342 - perfect: 0.0862 - val_loss: 0.2860 - val_sparse: 0.9316 - val_perfect: 0.0753\n",
            "Epoch 26/50\n",
            "71716/71716 [==============================] - 33s 466us/step - loss: 0.2642 - sparse: 0.9347 - perfect: 0.0884 - val_loss: 0.2831 - val_sparse: 0.9322 - val_perfect: 0.0765\n",
            "Epoch 27/50\n",
            "71716/71716 [==============================] - 33s 467us/step - loss: 0.2619 - sparse: 0.9351 - perfect: 0.0905 - val_loss: 0.2814 - val_sparse: 0.9327 - val_perfect: 0.0808\n",
            "Epoch 28/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2596 - sparse: 0.9356 - perfect: 0.0938 - val_loss: 0.2793 - val_sparse: 0.9329 - val_perfect: 0.0822\n",
            "Epoch 29/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2577 - sparse: 0.9360 - perfect: 0.0966 - val_loss: 0.2773 - val_sparse: 0.9335 - val_perfect: 0.0831\n",
            "Epoch 30/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2556 - sparse: 0.9364 - perfect: 0.0979 - val_loss: 0.2761 - val_sparse: 0.9333 - val_perfect: 0.0851\n",
            "Epoch 31/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2538 - sparse: 0.9367 - perfect: 0.0997 - val_loss: 0.2747 - val_sparse: 0.9339 - val_perfect: 0.0885\n",
            "Epoch 32/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2519 - sparse: 0.9370 - perfect: 0.1030 - val_loss: 0.2740 - val_sparse: 0.9338 - val_perfect: 0.0879\n",
            "Epoch 33/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2503 - sparse: 0.9374 - perfect: 0.1043 - val_loss: 0.2730 - val_sparse: 0.9342 - val_perfect: 0.0896\n",
            "Epoch 34/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2487 - sparse: 0.9376 - perfect: 0.1060 - val_loss: 0.2720 - val_sparse: 0.9345 - val_perfect: 0.0932\n",
            "Epoch 35/50\n",
            "71716/71716 [==============================] - 33s 465us/step - loss: 0.2472 - sparse: 0.9379 - perfect: 0.1084 - val_loss: 0.2696 - val_sparse: 0.9347 - val_perfect: 0.0938\n",
            "Epoch 36/50\n",
            "71716/71716 [==============================] - 34s 470us/step - loss: 0.2456 - sparse: 0.9383 - perfect: 0.1103 - val_loss: 0.2692 - val_sparse: 0.9352 - val_perfect: 0.0963\n",
            "Epoch 37/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2442 - sparse: 0.9385 - perfect: 0.1122 - val_loss: 0.2680 - val_sparse: 0.9352 - val_perfect: 0.0963\n",
            "Epoch 38/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2428 - sparse: 0.9388 - perfect: 0.1132 - val_loss: 0.2671 - val_sparse: 0.9354 - val_perfect: 0.0975\n",
            "Epoch 39/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.2416 - sparse: 0.9391 - perfect: 0.1162 - val_loss: 0.2661 - val_sparse: 0.9355 - val_perfect: 0.0985\n",
            "Epoch 40/50\n",
            "71716/71716 [==============================] - 33s 462us/step - loss: 0.2403 - sparse: 0.9393 - perfect: 0.1187 - val_loss: 0.2662 - val_sparse: 0.9350 - val_perfect: 0.0964\n",
            "Epoch 41/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2390 - sparse: 0.9396 - perfect: 0.1207 - val_loss: 0.2643 - val_sparse: 0.9358 - val_perfect: 0.1001\n",
            "Epoch 42/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2380 - sparse: 0.9397 - perfect: 0.1209 - val_loss: 0.2633 - val_sparse: 0.9360 - val_perfect: 0.1020\n",
            "Epoch 43/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.2368 - sparse: 0.9400 - perfect: 0.1223 - val_loss: 0.2624 - val_sparse: 0.9364 - val_perfect: 0.1034\n",
            "Epoch 44/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.2359 - sparse: 0.9402 - perfect: 0.1250 - val_loss: 0.2623 - val_sparse: 0.9360 - val_perfect: 0.1019\n",
            "Epoch 45/50\n",
            "71716/71716 [==============================] - 34s 469us/step - loss: 0.2347 - sparse: 0.9405 - perfect: 0.1265 - val_loss: 0.2606 - val_sparse: 0.9365 - val_perfect: 0.1057\n",
            "Epoch 46/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.2337 - sparse: 0.9406 - perfect: 0.1279 - val_loss: 0.2604 - val_sparse: 0.9368 - val_perfect: 0.1064\n",
            "Epoch 47/50\n",
            "71716/71716 [==============================] - 33s 464us/step - loss: 0.2327 - sparse: 0.9408 - perfect: 0.1294 - val_loss: 0.2597 - val_sparse: 0.9366 - val_perfect: 0.1068\n",
            "Epoch 48/50\n",
            "71716/71716 [==============================] - 33s 466us/step - loss: 0.2318 - sparse: 0.9410 - perfect: 0.1312 - val_loss: 0.2586 - val_sparse: 0.9371 - val_perfect: 0.1091\n",
            "Epoch 49/50\n",
            "71716/71716 [==============================] - 33s 465us/step - loss: 0.2309 - sparse: 0.9412 - perfect: 0.1325 - val_loss: 0.2584 - val_sparse: 0.9369 - val_perfect: 0.1093\n",
            "Epoch 50/50\n",
            "71716/71716 [==============================] - 33s 463us/step - loss: 0.2301 - sparse: 0.9413 - perfect: 0.1332 - val_loss: 0.2578 - val_sparse: 0.9371 - val_perfect: 0.1100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "084d7103-82fc-47e8-ba41-e6d95af5cfe6"
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['val_sparse'])\n",
        "  plt.plot(history.history['val_perfect'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('sparse_categorical_accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "b299a3bb-e3aa-40bb-ff18-a73af50b34c2"
      },
      "source": [
        "short = x_test[0:1000]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights('./model.h5')  \n",
        "  eval = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on val holdout: ' ,model.metrics_names, eval)\n",
        "  print('history: ', history)\n",
        "  predicts = model.predict(short, batch_size=32)\n",
        "  print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  512/23906 [..............................] - ETA: 28s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f721c1a97b8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23906/23906 [==============================] - 7s 303us/step\n",
            "model.evaluate on val holdout:  ['loss', 'sparse', 'perfect'] [0.2577958069545379, 0.9370506702881223, 0.1100142223709529]\n",
            "history:  <keras.callbacks.History object at 0x7f721c2e8cc0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f721c0d2550>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape: (1000, 30, 2000)\n",
            "30\n",
            "2000\n",
            "[[5.9983528e-01 1.2863971e-01 2.6613809e-03 ... 4.5892266e-06\n",
            "  5.6674867e-07 3.7716106e-07]\n",
            " [3.3188197e-01 8.3305672e-02 7.5759506e-04 ... 7.8412068e-06\n",
            "  2.7797621e-06 5.1509435e-08]\n",
            " [6.8288404e-01 1.9245148e-02 1.7440571e-03 ... 1.7150329e-05\n",
            "  7.4165632e-06 1.6640320e-07]\n",
            " ...\n",
            " [9.9998176e-01 3.6948803e-12 5.2141160e-12 ... 1.2295507e-07\n",
            "  3.9898391e-09 2.9301377e-11]\n",
            " [9.9998164e-01 3.6674830e-12 5.0927084e-12 ... 1.2439719e-07\n",
            "  3.9990731e-09 2.9754289e-11]\n",
            " [9.9998152e-01 3.6420806e-12 4.9842240e-12 ... 1.2573001e-07\n",
            "  4.0077621e-09 3.0173655e-11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFy2tfyIYOtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17401
        },
        "outputId": "2c188c6c-d130-44fd-abc3-2c45afe3a063"
      },
      "source": [
        "def maxindx(pred):\n",
        "    maxi=-1\n",
        "    maxv=-1.0\n",
        "    for x in range(len(pred)):\n",
        "        if pred[x] > maxv:\n",
        "            maxv = pred[x]\n",
        "            maxi = x\n",
        "    return (maxi, maxv)\n",
        "\n",
        "def match(data, prediction):\n",
        "    good = 0\n",
        "    total = 0\n",
        "    for i in range(len(data)):\n",
        "        if data[i] == np.argmax(prediction[i]):\n",
        "            good += 1\n",
        "        total += 1\n",
        "    #print('{}, {}'.format(good, total))\n",
        "    if (total == 0):\n",
        "        return 0\n",
        "    return good / total\n",
        "\n",
        "parallel = 0.0\n",
        "serial = 0.0\n",
        "total = 0\n",
        "for n in range(len(short)):\n",
        "    print(len(short[n]))\n",
        "    check = match(short[n], predicts[n])\n",
        "    parallel += check\n",
        "    if check > 0.9999:\n",
        "        serial += 1\n",
        "    total += 1\n",
        "\n",
        "print('Parallel, serial: ', parallel / total, serial / total)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21\n",
            "17\n",
            "19\n",
            "19\n",
            "18\n",
            "19\n",
            "20\n",
            "22\n",
            "14\n",
            "19\n",
            "17\n",
            "18\n",
            "19\n",
            "26\n",
            "15\n",
            "18\n",
            "17\n",
            "21\n",
            "14\n",
            "20\n",
            "20\n",
            "15\n",
            "19\n",
            "17\n",
            "20\n",
            "21\n",
            "17\n",
            "17\n",
            "21\n",
            "22\n",
            "17\n",
            "17\n",
            "26\n",
            "18\n",
            "16\n",
            "25\n",
            "16\n",
            "19\n",
            "19\n",
            "19\n",
            "16\n",
            "18\n",
            "20\n",
            "21\n",
            "21\n",
            "17\n",
            "16\n",
            "20\n",
            "19\n",
            "19\n",
            "15\n",
            "18\n",
            "11\n",
            "16\n",
            "18\n",
            "17\n",
            "13\n",
            "19\n",
            "20\n",
            "19\n",
            "20\n",
            "20\n",
            "21\n",
            "12\n",
            "18\n",
            "19\n",
            "23\n",
            "19\n",
            "21\n",
            "21\n",
            "26\n",
            "18\n",
            "17\n",
            "21\n",
            "25\n",
            "21\n",
            "20\n",
            "17\n",
            "23\n",
            "19\n",
            "21\n",
            "20\n",
            "17\n",
            "24\n",
            "18\n",
            "15\n",
            "23\n",
            "19\n",
            "21\n",
            "19\n",
            "23\n",
            "17\n",
            "18\n",
            "19\n",
            "16\n",
            "20\n",
            "17\n",
            "18\n",
            "22\n",
            "17\n",
            "19\n",
            "17\n",
            "16\n",
            "17\n",
            "25\n",
            "16\n",
            "15\n",
            "26\n",
            "18\n",
            "13\n",
            "12\n",
            "18\n",
            "19\n",
            "22\n",
            "18\n",
            "13\n",
            "20\n",
            "15\n",
            "17\n",
            "23\n",
            "17\n",
            "20\n",
            "17\n",
            "20\n",
            "16\n",
            "20\n",
            "16\n",
            "22\n",
            "20\n",
            "22\n",
            "20\n",
            "27\n",
            "22\n",
            "17\n",
            "21\n",
            "20\n",
            "18\n",
            "22\n",
            "14\n",
            "21\n",
            "14\n",
            "20\n",
            "22\n",
            "19\n",
            "17\n",
            "18\n",
            "18\n",
            "18\n",
            "19\n",
            "15\n",
            "20\n",
            "22\n",
            "19\n",
            "20\n",
            "19\n",
            "19\n",
            "15\n",
            "16\n",
            "18\n",
            "20\n",
            "18\n",
            "18\n",
            "23\n",
            "17\n",
            "14\n",
            "18\n",
            "16\n",
            "19\n",
            "19\n",
            "21\n",
            "20\n",
            "19\n",
            "21\n",
            "15\n",
            "19\n",
            "14\n",
            "19\n",
            "19\n",
            "17\n",
            "18\n",
            "17\n",
            "16\n",
            "20\n",
            "23\n",
            "24\n",
            "22\n",
            "18\n",
            "20\n",
            "18\n",
            "15\n",
            "15\n",
            "18\n",
            "22\n",
            "17\n",
            "20\n",
            "19\n",
            "18\n",
            "21\n",
            "19\n",
            "21\n",
            "8\n",
            "17\n",
            "21\n",
            "18\n",
            "18\n",
            "16\n",
            "19\n",
            "18\n",
            "18\n",
            "20\n",
            "27\n",
            "15\n",
            "27\n",
            "20\n",
            "20\n",
            "19\n",
            "18\n",
            "18\n",
            "16\n",
            "18\n",
            "18\n",
            "21\n",
            "12\n",
            "20\n",
            "16\n",
            "16\n",
            "21\n",
            "23\n",
            "16\n",
            "15\n",
            "22\n",
            "15\n",
            "18\n",
            "19\n",
            "15\n",
            "19\n",
            "21\n",
            "15\n",
            "18\n",
            "22\n",
            "18\n",
            "22\n",
            "15\n",
            "20\n",
            "16\n",
            "18\n",
            "19\n",
            "22\n",
            "15\n",
            "17\n",
            "16\n",
            "16\n",
            "20\n",
            "23\n",
            "26\n",
            "24\n",
            "20\n",
            "23\n",
            "19\n",
            "18\n",
            "16\n",
            "22\n",
            "21\n",
            "19\n",
            "21\n",
            "18\n",
            "14\n",
            "16\n",
            "18\n",
            "16\n",
            "15\n",
            "21\n",
            "20\n",
            "23\n",
            "20\n",
            "18\n",
            "20\n",
            "16\n",
            "21\n",
            "18\n",
            "19\n",
            "23\n",
            "15\n",
            "17\n",
            "22\n",
            "23\n",
            "16\n",
            "18\n",
            "20\n",
            "21\n",
            "21\n",
            "23\n",
            "22\n",
            "19\n",
            "22\n",
            "12\n",
            "18\n",
            "21\n",
            "14\n",
            "20\n",
            "17\n",
            "23\n",
            "17\n",
            "17\n",
            "24\n",
            "19\n",
            "24\n",
            "21\n",
            "17\n",
            "27\n",
            "13\n",
            "21\n",
            "14\n",
            "18\n",
            "22\n",
            "19\n",
            "26\n",
            "16\n",
            "18\n",
            "18\n",
            "26\n",
            "17\n",
            "18\n",
            "18\n",
            "23\n",
            "17\n",
            "15\n",
            "25\n",
            "23\n",
            "19\n",
            "22\n",
            "17\n",
            "19\n",
            "23\n",
            "16\n",
            "17\n",
            "13\n",
            "18\n",
            "20\n",
            "15\n",
            "18\n",
            "21\n",
            "16\n",
            "16\n",
            "15\n",
            "22\n",
            "20\n",
            "24\n",
            "17\n",
            "18\n",
            "22\n",
            "18\n",
            "14\n",
            "24\n",
            "18\n",
            "20\n",
            "23\n",
            "19\n",
            "19\n",
            "21\n",
            "21\n",
            "18\n",
            "16\n",
            "21\n",
            "18\n",
            "18\n",
            "17\n",
            "21\n",
            "18\n",
            "20\n",
            "14\n",
            "15\n",
            "21\n",
            "21\n",
            "19\n",
            "20\n",
            "18\n",
            "21\n",
            "21\n",
            "11\n",
            "21\n",
            "19\n",
            "18\n",
            "21\n",
            "18\n",
            "20\n",
            "20\n",
            "19\n",
            "17\n",
            "16\n",
            "20\n",
            "19\n",
            "22\n",
            "17\n",
            "18\n",
            "19\n",
            "17\n",
            "17\n",
            "16\n",
            "24\n",
            "22\n",
            "18\n",
            "20\n",
            "18\n",
            "19\n",
            "15\n",
            "19\n",
            "17\n",
            "16\n",
            "22\n",
            "20\n",
            "23\n",
            "18\n",
            "19\n",
            "23\n",
            "22\n",
            "17\n",
            "23\n",
            "17\n",
            "24\n",
            "21\n",
            "19\n",
            "18\n",
            "17\n",
            "10\n",
            "19\n",
            "17\n",
            "22\n",
            "20\n",
            "18\n",
            "19\n",
            "21\n",
            "19\n",
            "18\n",
            "20\n",
            "21\n",
            "14\n",
            "22\n",
            "16\n",
            "19\n",
            "17\n",
            "19\n",
            "18\n",
            "23\n",
            "22\n",
            "17\n",
            "15\n",
            "15\n",
            "17\n",
            "19\n",
            "21\n",
            "20\n",
            "20\n",
            "15\n",
            "20\n",
            "20\n",
            "21\n",
            "21\n",
            "14\n",
            "17\n",
            "17\n",
            "16\n",
            "13\n",
            "17\n",
            "21\n",
            "22\n",
            "16\n",
            "23\n",
            "18\n",
            "21\n",
            "23\n",
            "17\n",
            "19\n",
            "17\n",
            "19\n",
            "17\n",
            "16\n",
            "18\n",
            "19\n",
            "19\n",
            "15\n",
            "18\n",
            "18\n",
            "13\n",
            "24\n",
            "18\n",
            "21\n",
            "16\n",
            "16\n",
            "20\n",
            "20\n",
            "18\n",
            "16\n",
            "20\n",
            "20\n",
            "17\n",
            "19\n",
            "19\n",
            "20\n",
            "15\n",
            "22\n",
            "19\n",
            "22\n",
            "23\n",
            "18\n",
            "22\n",
            "22\n",
            "19\n",
            "21\n",
            "21\n",
            "19\n",
            "22\n",
            "22\n",
            "20\n",
            "20\n",
            "20\n",
            "14\n",
            "18\n",
            "18\n",
            "22\n",
            "21\n",
            "16\n",
            "17\n",
            "18\n",
            "17\n",
            "26\n",
            "17\n",
            "21\n",
            "16\n",
            "21\n",
            "22\n",
            "17\n",
            "19\n",
            "22\n",
            "23\n",
            "19\n",
            "19\n",
            "16\n",
            "20\n",
            "18\n",
            "15\n",
            "17\n",
            "21\n",
            "22\n",
            "22\n",
            "14\n",
            "17\n",
            "19\n",
            "21\n",
            "15\n",
            "21\n",
            "16\n",
            "18\n",
            "21\n",
            "22\n",
            "26\n",
            "17\n",
            "15\n",
            "21\n",
            "16\n",
            "21\n",
            "15\n",
            "15\n",
            "19\n",
            "18\n",
            "20\n",
            "20\n",
            "15\n",
            "21\n",
            "19\n",
            "17\n",
            "22\n",
            "15\n",
            "21\n",
            "20\n",
            "16\n",
            "22\n",
            "16\n",
            "20\n",
            "17\n",
            "22\n",
            "20\n",
            "23\n",
            "25\n",
            "22\n",
            "14\n",
            "21\n",
            "18\n",
            "20\n",
            "21\n",
            "18\n",
            "16\n",
            "16\n",
            "18\n",
            "22\n",
            "16\n",
            "17\n",
            "13\n",
            "19\n",
            "16\n",
            "24\n",
            "17\n",
            "17\n",
            "16\n",
            "20\n",
            "18\n",
            "18\n",
            "22\n",
            "19\n",
            "20\n",
            "23\n",
            "16\n",
            "20\n",
            "22\n",
            "18\n",
            "21\n",
            "21\n",
            "19\n",
            "19\n",
            "22\n",
            "18\n",
            "18\n",
            "15\n",
            "19\n",
            "13\n",
            "21\n",
            "18\n",
            "17\n",
            "12\n",
            "16\n",
            "19\n",
            "19\n",
            "22\n",
            "21\n",
            "19\n",
            "12\n",
            "18\n",
            "16\n",
            "19\n",
            "16\n",
            "15\n",
            "18\n",
            "19\n",
            "17\n",
            "27\n",
            "22\n",
            "18\n",
            "18\n",
            "15\n",
            "19\n",
            "18\n",
            "21\n",
            "21\n",
            "19\n",
            "17\n",
            "21\n",
            "18\n",
            "15\n",
            "16\n",
            "20\n",
            "21\n",
            "19\n",
            "17\n",
            "15\n",
            "19\n",
            "18\n",
            "20\n",
            "26\n",
            "18\n",
            "15\n",
            "19\n",
            "17\n",
            "19\n",
            "21\n",
            "18\n",
            "17\n",
            "19\n",
            "19\n",
            "15\n",
            "17\n",
            "22\n",
            "21\n",
            "19\n",
            "20\n",
            "17\n",
            "21\n",
            "21\n",
            "18\n",
            "18\n",
            "21\n",
            "17\n",
            "17\n",
            "17\n",
            "20\n",
            "14\n",
            "23\n",
            "22\n",
            "18\n",
            "21\n",
            "19\n",
            "27\n",
            "14\n",
            "17\n",
            "16\n",
            "18\n",
            "12\n",
            "18\n",
            "16\n",
            "16\n",
            "20\n",
            "17\n",
            "16\n",
            "19\n",
            "25\n",
            "22\n",
            "17\n",
            "15\n",
            "16\n",
            "15\n",
            "22\n",
            "25\n",
            "22\n",
            "21\n",
            "17\n",
            "14\n",
            "27\n",
            "19\n",
            "19\n",
            "19\n",
            "17\n",
            "15\n",
            "15\n",
            "17\n",
            "15\n",
            "21\n",
            "21\n",
            "14\n",
            "21\n",
            "27\n",
            "15\n",
            "20\n",
            "12\n",
            "22\n",
            "18\n",
            "19\n",
            "21\n",
            "17\n",
            "17\n",
            "18\n",
            "18\n",
            "22\n",
            "14\n",
            "19\n",
            "18\n",
            "18\n",
            "16\n",
            "19\n",
            "19\n",
            "18\n",
            "15\n",
            "18\n",
            "21\n",
            "22\n",
            "17\n",
            "21\n",
            "15\n",
            "19\n",
            "21\n",
            "18\n",
            "22\n",
            "21\n",
            "14\n",
            "21\n",
            "17\n",
            "18\n",
            "15\n",
            "23\n",
            "17\n",
            "22\n",
            "17\n",
            "19\n",
            "25\n",
            "20\n",
            "27\n",
            "20\n",
            "15\n",
            "19\n",
            "18\n",
            "22\n",
            "15\n",
            "17\n",
            "16\n",
            "18\n",
            "21\n",
            "23\n",
            "21\n",
            "19\n",
            "16\n",
            "22\n",
            "27\n",
            "22\n",
            "20\n",
            "23\n",
            "22\n",
            "23\n",
            "22\n",
            "17\n",
            "22\n",
            "22\n",
            "15\n",
            "18\n",
            "23\n",
            "22\n",
            "20\n",
            "20\n",
            "19\n",
            "18\n",
            "22\n",
            "14\n",
            "20\n",
            "19\n",
            "19\n",
            "18\n",
            "17\n",
            "20\n",
            "14\n",
            "22\n",
            "14\n",
            "16\n",
            "15\n",
            "18\n",
            "24\n",
            "16\n",
            "22\n",
            "18\n",
            "19\n",
            "22\n",
            "22\n",
            "22\n",
            "18\n",
            "15\n",
            "16\n",
            "19\n",
            "17\n",
            "18\n",
            "16\n",
            "18\n",
            "19\n",
            "22\n",
            "25\n",
            "17\n",
            "14\n",
            "17\n",
            "15\n",
            "20\n",
            "22\n",
            "20\n",
            "20\n",
            "23\n",
            "20\n",
            "19\n",
            "16\n",
            "17\n",
            "20\n",
            "18\n",
            "18\n",
            "19\n",
            "25\n",
            "17\n",
            "20\n",
            "15\n",
            "21\n",
            "20\n",
            "18\n",
            "19\n",
            "16\n",
            "15\n",
            "15\n",
            "16\n",
            "17\n",
            "17\n",
            "25\n",
            "21\n",
            "17\n",
            "20\n",
            "20\n",
            "20\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "19\n",
            "15\n",
            "20\n",
            "14\n",
            "17\n",
            "19\n",
            "18\n",
            "18\n",
            "24\n",
            "17\n",
            "19\n",
            "22\n",
            "17\n",
            "14\n",
            "22\n",
            "20\n",
            "22\n",
            "19\n",
            "17\n",
            "17\n",
            "21\n",
            "16\n",
            "16\n",
            "18\n",
            "20\n",
            "21\n",
            "13\n",
            "20\n",
            "16\n",
            "23\n",
            "18\n",
            "16\n",
            "20\n",
            "19\n",
            "19\n",
            "20\n",
            "21\n",
            "17\n",
            "18\n",
            "18\n",
            "20\n",
            "16\n",
            "16\n",
            "20\n",
            "18\n",
            "14\n",
            "16\n",
            "21\n",
            "19\n",
            "19\n",
            "19\n",
            "20\n",
            "16\n",
            "19\n",
            "18\n",
            "17\n",
            "18\n",
            "16\n",
            "15\n",
            "19\n",
            "21\n",
            "16\n",
            "16\n",
            "20\n",
            "19\n",
            "18\n",
            "20\n",
            "15\n",
            "20\n",
            "19\n",
            "16\n",
            "19\n",
            "25\n",
            "18\n",
            "22\n",
            "17\n",
            "19\n",
            "15\n",
            "17\n",
            "17\n",
            "15\n",
            "16\n",
            "20\n",
            "19\n",
            "22\n",
            "18\n",
            "20\n",
            "15\n",
            "17\n",
            "18\n",
            "15\n",
            "21\n",
            "15\n",
            "19\n",
            "17\n",
            "18\n",
            "19\n",
            "14\n",
            "20\n",
            "20\n",
            "23\n",
            "20\n",
            "19\n",
            "13\n",
            "21\n",
            "18\n",
            "18\n",
            "18\n",
            "19\n",
            "Parallel, serial:  0.0 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWVxMyKd3P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}