{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "f56634b0-7ebb-4233-b711-d92148dbefae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "e17ed880-4b83-4cb8-97d6-109e3a5a6976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "50cac56c-c49f-4779-bf8b-cfceb63b1eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "from cmu.wordmap import Wordmap\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 20000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 16000\n",
        "# longest output sentence\n",
        "max_len = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=False\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_5_\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_5_.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "013bec4a-6593-48e9-ef41-0473ad71c7d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!date\n",
        "word2sylls = trim_homynyms(word2sylls)\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul 18 00:14:18 UTC 2019\n",
            "2443 0\n",
            "# features:  15098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "e00637a9-0ca8-4a72-bd64-f949b45e66d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "big_text = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "textwordset = set()\n",
        "haikuwordset = set()\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        _use_input = True\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            continue\n",
        "        textwords = list(text.text_to_word_sequence(_text))\n",
        "        if len(textwords) > max_words:\n",
        "            use_input = False\n",
        "        _lastidx = -1\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            if word in word2sylls:\n",
        "                haikuwordset.add(word)\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "                _thisidx = decoder.word2idx[word]\n",
        "                #print('word {}, idx {}'.format(word, decoder.word2idx[word]))\n",
        "                if _lastidx != -1:\n",
        "                    wordmap.add(_lastidx, _thisidx)\n",
        "                lastidx = _thisidx\n",
        "        for textword in textwords:\n",
        "            textwordset.add(textword)\n",
        "        if len(_sylls) != 5:\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        if _use_input:\n",
        "            big_text.append(_text)\n",
        "            big_haiku.append(_haiku)\n",
        "            big_data.append(_data)\n",
        "        if duplicate_haiku:\n",
        "            big_text.append(_haiku)\n",
        "            big_haiku.append(_haiku)\n",
        "            big_data.append(_data)\n",
        "        last_haiku = _haiku\n",
        "        if len(big_text) == max_data:\n",
        "            break\n",
        "\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "big_data = np.expand_dims(big_data, -1)\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "office equipment -> office equipment : [[ 211]\n",
            " [2732]\n",
            " [4968]\n",
            " [6573]\n",
            " [7383]]\n",
            "office equipment -> office equipment : [[ 211]\n",
            " [2732]\n",
            " [4968]\n",
            " [6573]\n",
            " [7383]]\n",
            "Full length clauses:  20000\n",
            "Wordmap total entries:  0\n",
            "Wordmap length:  210354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV9PEaKFxil5",
        "colab_type": "code",
        "outputId": "d83f9849-02f9-4a5f-ca31-7b42e3a162e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Wordmap[15]: ', wordmap.mat[150000])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wordmap[15]:  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "289bb5f2-061e-4351-9b92-8669c076d660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(big_text, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print(x_test[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to his motorcycle [[ 4724]\n",
            " [ 7685]\n",
            " [12823]\n",
            " [ 9899]\n",
            " [ 5519]]\n",
            "x_train shape: (14976,)\n",
            "x_test shape: (4992,)\n",
            "y_train shape: (14976, 5, 1)\n",
            "y_test shape: (4992, 5, 1)\n",
            "[4724]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "c6df780a-a622-45a7-b035-d28660744f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "def sparse_loss(y_true, y_pred):\n",
        "    return scc(y_true, y_pred)\n",
        "\n",
        "def perfect_loss(y_true, y_pred):\n",
        "    return scct(y_true, y_pred, scale=1.0)\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.4\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_medium, activation='relu')(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_small, activation='relu')(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.RepeatVector(max_len)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', sparse_loss, metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=300,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=0)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0718 00:14:28.522444 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0718 00:14:28.523676 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0718 00:14:29.428697 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0718 00:14:29.437493 140038015625088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0718 00:14:29.453542 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0718 00:14:30.793993 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0718 00:14:30.815659 140038015625088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras_stuff/loss.py:58: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_1 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 512)            2449408   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_2 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5, 16000)          10928000  \n",
            "=================================================================\n",
            "Total params: 14,602,452\n",
            "Trainable params: 14,602,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0718 00:14:33.618044 140038015625088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 14976 samples, validate on 4992 samples\n",
            "Epoch 1/300\n",
            " - 25s - loss: 5.1207 - sparse: 0.1657 - perfect: 0.0238 - val_loss: 3.8593 - val_sparse: 0.3123 - val_perfect: 0.1102\n",
            "Epoch 2/300\n",
            " - 21s - loss: 3.5972 - sparse: 0.3400 - perfect: 0.1018 - val_loss: 3.0265 - val_sparse: 0.4347 - val_perfect: 0.1989\n",
            "Epoch 3/300\n",
            " - 21s - loss: 2.9991 - sparse: 0.4168 - perfect: 0.1500 - val_loss: 2.5890 - val_sparse: 0.4948 - val_perfect: 0.2412\n",
            "Epoch 4/300\n",
            " - 21s - loss: 2.6370 - sparse: 0.4650 - perfect: 0.1794 - val_loss: 2.3141 - val_sparse: 0.5303 - val_perfect: 0.2602\n",
            "Epoch 5/300\n",
            " - 21s - loss: 2.3743 - sparse: 0.5003 - perfect: 0.2017 - val_loss: 2.1052 - val_sparse: 0.5659 - val_perfect: 0.2999\n",
            "Epoch 6/300\n",
            " - 21s - loss: 2.1744 - sparse: 0.5300 - perfect: 0.2198 - val_loss: 1.9464 - val_sparse: 0.5932 - val_perfect: 0.3203\n",
            "Epoch 7/300\n",
            " - 21s - loss: 2.0126 - sparse: 0.5536 - perfect: 0.2354 - val_loss: 1.8211 - val_sparse: 0.6102 - val_perfect: 0.3315\n",
            "Epoch 8/300\n",
            " - 21s - loss: 1.8856 - sparse: 0.5737 - perfect: 0.2479 - val_loss: 1.7162 - val_sparse: 0.6297 - val_perfect: 0.3596\n",
            "Epoch 9/300\n",
            " - 21s - loss: 1.7743 - sparse: 0.5907 - perfect: 0.2610 - val_loss: 1.6451 - val_sparse: 0.6482 - val_perfect: 0.3712\n",
            "Epoch 10/300\n",
            " - 21s - loss: 1.6772 - sparse: 0.6092 - perfect: 0.2682 - val_loss: 1.5723 - val_sparse: 0.6567 - val_perfect: 0.3760\n",
            "Epoch 11/300\n",
            " - 21s - loss: 1.5885 - sparse: 0.6246 - perfect: 0.2832 - val_loss: 1.5055 - val_sparse: 0.6762 - val_perfect: 0.4036\n",
            "Epoch 12/300\n",
            " - 21s - loss: 1.5226 - sparse: 0.6378 - perfect: 0.2936 - val_loss: 1.4558 - val_sparse: 0.6843 - val_perfect: 0.4109\n",
            "Epoch 13/300\n",
            " - 21s - loss: 1.4541 - sparse: 0.6481 - perfect: 0.2968 - val_loss: 1.4008 - val_sparse: 0.6956 - val_perfect: 0.4307\n",
            "Epoch 14/300\n",
            " - 21s - loss: 1.3895 - sparse: 0.6617 - perfect: 0.3120 - val_loss: 1.3672 - val_sparse: 0.7026 - val_perfect: 0.4297\n",
            "Epoch 15/300\n",
            " - 21s - loss: 1.3455 - sparse: 0.6698 - perfect: 0.3158 - val_loss: 1.3399 - val_sparse: 0.7101 - val_perfect: 0.4421\n",
            "Epoch 16/300\n",
            " - 21s - loss: 1.2933 - sparse: 0.6796 - perfect: 0.3216 - val_loss: 1.3067 - val_sparse: 0.7183 - val_perfect: 0.4597\n",
            "Epoch 17/300\n",
            " - 21s - loss: 1.2487 - sparse: 0.6897 - perfect: 0.3330 - val_loss: 1.2728 - val_sparse: 0.7266 - val_perfect: 0.4657\n",
            "Epoch 18/300\n",
            " - 21s - loss: 1.2136 - sparse: 0.6957 - perfect: 0.3346 - val_loss: 1.2543 - val_sparse: 0.7307 - val_perfect: 0.4720\n",
            "Epoch 19/300\n",
            " - 21s - loss: 1.1739 - sparse: 0.7036 - perfect: 0.3420 - val_loss: 1.2365 - val_sparse: 0.7351 - val_perfect: 0.4864\n",
            "Epoch 20/300\n",
            " - 21s - loss: 1.1522 - sparse: 0.7088 - perfect: 0.3476 - val_loss: 1.2229 - val_sparse: 0.7421 - val_perfect: 0.4924\n",
            "Epoch 21/300\n",
            " - 21s - loss: 1.1151 - sparse: 0.7157 - perfect: 0.3539 - val_loss: 1.1962 - val_sparse: 0.7465 - val_perfect: 0.5046\n",
            "Epoch 22/300\n",
            " - 21s - loss: 1.0804 - sparse: 0.7238 - perfect: 0.3590 - val_loss: 1.1874 - val_sparse: 0.7475 - val_perfect: 0.5070\n",
            "Epoch 23/300\n",
            " - 21s - loss: 1.0524 - sparse: 0.7290 - perfect: 0.3670 - val_loss: 1.1700 - val_sparse: 0.7530 - val_perfect: 0.5162\n",
            "Epoch 24/300\n",
            " - 21s - loss: 1.0289 - sparse: 0.7329 - perfect: 0.3676 - val_loss: 1.1622 - val_sparse: 0.7570 - val_perfect: 0.5200\n",
            "Epoch 25/300\n",
            " - 21s - loss: 1.0206 - sparse: 0.7359 - perfect: 0.3749 - val_loss: 1.1528 - val_sparse: 0.7592 - val_perfect: 0.5298\n",
            "Epoch 26/300\n",
            " - 21s - loss: 0.9878 - sparse: 0.7422 - perfect: 0.3811 - val_loss: 1.1305 - val_sparse: 0.7663 - val_perfect: 0.5359\n",
            "Epoch 27/300\n",
            " - 21s - loss: 0.9661 - sparse: 0.7479 - perfect: 0.3836 - val_loss: 1.1279 - val_sparse: 0.7665 - val_perfect: 0.5405\n",
            "Epoch 28/300\n",
            " - 21s - loss: 0.9479 - sparse: 0.7513 - perfect: 0.3890 - val_loss: 1.1214 - val_sparse: 0.7691 - val_perfect: 0.5417\n",
            "Epoch 29/300\n",
            " - 21s - loss: 0.9394 - sparse: 0.7511 - perfect: 0.3849 - val_loss: 1.1161 - val_sparse: 0.7720 - val_perfect: 0.5473\n",
            "Epoch 30/300\n",
            " - 21s - loss: 0.9265 - sparse: 0.7573 - perfect: 0.3986 - val_loss: 1.1084 - val_sparse: 0.7710 - val_perfect: 0.5507\n",
            "Epoch 31/300\n",
            " - 21s - loss: 0.9070 - sparse: 0.7604 - perfect: 0.4008 - val_loss: 1.0989 - val_sparse: 0.7766 - val_perfect: 0.5613\n",
            "Epoch 32/300\n",
            " - 21s - loss: 0.8884 - sparse: 0.7662 - perfect: 0.4089 - val_loss: 1.1062 - val_sparse: 0.7734 - val_perfect: 0.5561\n",
            "Epoch 33/300\n",
            " - 21s - loss: 0.8758 - sparse: 0.7663 - perfect: 0.4114 - val_loss: 1.0952 - val_sparse: 0.7790 - val_perfect: 0.5675\n",
            "Epoch 34/300\n",
            " - 21s - loss: 0.8623 - sparse: 0.7731 - perfect: 0.4133 - val_loss: 1.0969 - val_sparse: 0.7810 - val_perfect: 0.5719\n",
            "Epoch 35/300\n",
            " - 21s - loss: 0.8418 - sparse: 0.7760 - perfect: 0.4177 - val_loss: 1.0814 - val_sparse: 0.7835 - val_perfect: 0.5825\n",
            "Epoch 36/300\n",
            " - 21s - loss: 0.8415 - sparse: 0.7741 - perfect: 0.4145 - val_loss: 1.0853 - val_sparse: 0.7846 - val_perfect: 0.5797\n",
            "Epoch 37/300\n",
            " - 21s - loss: 0.8280 - sparse: 0.7809 - perfect: 0.4230 - val_loss: 1.0948 - val_sparse: 0.7819 - val_perfect: 0.5719\n",
            "Epoch 38/300\n",
            " - 21s - loss: 0.8143 - sparse: 0.7817 - perfect: 0.4264 - val_loss: 1.0684 - val_sparse: 0.7895 - val_perfect: 0.5841\n",
            "Epoch 39/300\n",
            " - 21s - loss: 0.8042 - sparse: 0.7840 - perfect: 0.4288 - val_loss: 1.0716 - val_sparse: 0.7880 - val_perfect: 0.5833\n",
            "Epoch 40/300\n",
            " - 21s - loss: 0.7955 - sparse: 0.7854 - perfect: 0.4306 - val_loss: 1.0788 - val_sparse: 0.7876 - val_perfect: 0.5889\n",
            "Epoch 41/300\n",
            " - 21s - loss: 0.7879 - sparse: 0.7883 - perfect: 0.4327 - val_loss: 1.0636 - val_sparse: 0.7901 - val_perfect: 0.5847\n",
            "Epoch 42/300\n",
            " - 21s - loss: 0.7603 - sparse: 0.7924 - perfect: 0.4397 - val_loss: 1.0658 - val_sparse: 0.7910 - val_perfect: 0.5903\n",
            "Epoch 43/300\n",
            " - 21s - loss: 0.7673 - sparse: 0.7914 - perfect: 0.4367 - val_loss: 1.0655 - val_sparse: 0.7921 - val_perfect: 0.5986\n",
            "Epoch 44/300\n",
            " - 21s - loss: 0.7529 - sparse: 0.7959 - perfect: 0.4450 - val_loss: 1.0603 - val_sparse: 0.7939 - val_perfect: 0.5976\n",
            "Epoch 45/300\n",
            " - 21s - loss: 0.7530 - sparse: 0.7961 - perfect: 0.4446 - val_loss: 1.0577 - val_sparse: 0.7959 - val_perfect: 0.6038\n",
            "Epoch 46/300\n",
            " - 21s - loss: 0.7403 - sparse: 0.7992 - perfect: 0.4474 - val_loss: 1.0536 - val_sparse: 0.7990 - val_perfect: 0.6136\n",
            "Epoch 47/300\n",
            " - 21s - loss: 0.7365 - sparse: 0.8008 - perfect: 0.4519 - val_loss: 1.0566 - val_sparse: 0.7979 - val_perfect: 0.6108\n",
            "Epoch 48/300\n",
            " - 20s - loss: 0.7269 - sparse: 0.8016 - perfect: 0.4510 - val_loss: 1.0645 - val_sparse: 0.7989 - val_perfect: 0.6128\n",
            "Epoch 49/300\n",
            " - 21s - loss: 0.7307 - sparse: 0.8000 - perfect: 0.4485 - val_loss: 1.0546 - val_sparse: 0.7979 - val_perfect: 0.6174\n",
            "Epoch 50/300\n",
            " - 21s - loss: 0.7159 - sparse: 0.8064 - perfect: 0.4561 - val_loss: 1.0542 - val_sparse: 0.8009 - val_perfect: 0.6244\n",
            "Epoch 51/300\n",
            " - 21s - loss: 0.7019 - sparse: 0.8075 - perfect: 0.4613 - val_loss: 1.0622 - val_sparse: 0.7992 - val_perfect: 0.6190\n",
            "Epoch 52/300\n",
            " - 21s - loss: 0.7026 - sparse: 0.8080 - perfect: 0.4620 - val_loss: 1.0465 - val_sparse: 0.8032 - val_perfect: 0.6210\n",
            "Epoch 53/300\n",
            " - 21s - loss: 0.6957 - sparse: 0.8095 - perfect: 0.4620 - val_loss: 1.0601 - val_sparse: 0.7986 - val_perfect: 0.6110\n",
            "Epoch 54/300\n",
            " - 21s - loss: 0.6907 - sparse: 0.8098 - perfect: 0.4641 - val_loss: 1.0524 - val_sparse: 0.8051 - val_perfect: 0.6320\n",
            "Epoch 55/300\n",
            " - 21s - loss: 0.6842 - sparse: 0.8129 - perfect: 0.4707 - val_loss: 1.0587 - val_sparse: 0.8024 - val_perfect: 0.6264\n",
            "Epoch 56/300\n",
            " - 20s - loss: 0.6782 - sparse: 0.8137 - perfect: 0.4719 - val_loss: 1.0564 - val_sparse: 0.8048 - val_perfect: 0.6308\n",
            "Epoch 57/300\n",
            " - 21s - loss: 0.6693 - sparse: 0.8168 - perfect: 0.4720 - val_loss: 1.0632 - val_sparse: 0.8054 - val_perfect: 0.6368\n",
            "Epoch 58/300\n",
            " - 21s - loss: 0.6689 - sparse: 0.8158 - perfect: 0.4742 - val_loss: 1.0577 - val_sparse: 0.8057 - val_perfect: 0.6346\n",
            "Epoch 59/300\n",
            " - 20s - loss: 0.6673 - sparse: 0.8168 - perfect: 0.4756 - val_loss: 1.0644 - val_sparse: 0.8036 - val_perfect: 0.6320\n",
            "Epoch 60/300\n",
            " - 21s - loss: 0.6584 - sparse: 0.8182 - perfect: 0.4737 - val_loss: 1.0638 - val_sparse: 0.8039 - val_perfect: 0.6344\n",
            "Epoch 61/300\n",
            " - 20s - loss: 0.6515 - sparse: 0.8196 - perfect: 0.4787 - val_loss: 1.0619 - val_sparse: 0.8080 - val_perfect: 0.6396\n",
            "Epoch 62/300\n",
            " - 21s - loss: 0.6581 - sparse: 0.8190 - perfect: 0.4852 - val_loss: 1.0489 - val_sparse: 0.8096 - val_perfect: 0.6430\n",
            "Epoch 63/300\n",
            " - 21s - loss: 0.6487 - sparse: 0.8210 - perfect: 0.4820 - val_loss: 1.0439 - val_sparse: 0.8113 - val_perfect: 0.6436\n",
            "Epoch 64/300\n",
            " - 21s - loss: 0.6404 - sparse: 0.8231 - perfect: 0.4836 - val_loss: 1.0560 - val_sparse: 0.8091 - val_perfect: 0.6420\n",
            "Epoch 65/300\n",
            " - 20s - loss: 0.6461 - sparse: 0.8224 - perfect: 0.4861 - val_loss: 1.0532 - val_sparse: 0.8093 - val_perfect: 0.6446\n",
            "Epoch 66/300\n",
            " - 21s - loss: 0.6368 - sparse: 0.8235 - perfect: 0.4898 - val_loss: 1.0551 - val_sparse: 0.8098 - val_perfect: 0.6450\n",
            "Epoch 67/300\n",
            " - 21s - loss: 0.6295 - sparse: 0.8248 - perfect: 0.4898 - val_loss: 1.0637 - val_sparse: 0.8076 - val_perfect: 0.6354\n",
            "Epoch 68/300\n",
            " - 21s - loss: 0.6260 - sparse: 0.8271 - perfect: 0.4886 - val_loss: 1.0499 - val_sparse: 0.8126 - val_perfect: 0.6448\n",
            "Epoch 69/300\n",
            " - 20s - loss: 0.6148 - sparse: 0.8284 - perfect: 0.4899 - val_loss: 1.0558 - val_sparse: 0.8115 - val_perfect: 0.6524\n",
            "Epoch 70/300\n",
            " - 21s - loss: 0.6116 - sparse: 0.8314 - perfect: 0.4981 - val_loss: 1.0667 - val_sparse: 0.8106 - val_perfect: 0.6448\n",
            "Epoch 71/300\n",
            " - 20s - loss: 0.6148 - sparse: 0.8292 - perfect: 0.4949 - val_loss: 1.0622 - val_sparse: 0.8121 - val_perfect: 0.6526\n",
            "Epoch 72/300\n",
            " - 21s - loss: 0.6082 - sparse: 0.8308 - perfect: 0.4985 - val_loss: 1.0732 - val_sparse: 0.8092 - val_perfect: 0.6456\n",
            "Epoch 73/300\n",
            " - 20s - loss: 0.6064 - sparse: 0.8302 - perfect: 0.4899 - val_loss: 1.0605 - val_sparse: 0.8135 - val_perfect: 0.6560\n",
            "Epoch 74/300\n",
            " - 21s - loss: 0.6034 - sparse: 0.8312 - perfect: 0.4995 - val_loss: 1.0741 - val_sparse: 0.8128 - val_perfect: 0.6540\n",
            "Epoch 75/300\n",
            " - 20s - loss: 0.6019 - sparse: 0.8330 - perfect: 0.5028 - val_loss: 1.0634 - val_sparse: 0.8121 - val_perfect: 0.6516\n",
            "Epoch 76/300\n",
            " - 21s - loss: 0.6028 - sparse: 0.8331 - perfect: 0.5045 - val_loss: 1.0710 - val_sparse: 0.8134 - val_perfect: 0.6575\n",
            "Epoch 77/300\n",
            " - 21s - loss: 0.5974 - sparse: 0.8342 - perfect: 0.5030 - val_loss: 1.0616 - val_sparse: 0.8143 - val_perfect: 0.6607\n",
            "Epoch 78/300\n",
            " - 21s - loss: 0.5948 - sparse: 0.8337 - perfect: 0.5034 - val_loss: 1.0577 - val_sparse: 0.8142 - val_perfect: 0.6556\n",
            "Epoch 79/300\n",
            " - 21s - loss: 0.5775 - sparse: 0.8390 - perfect: 0.5143 - val_loss: 1.0570 - val_sparse: 0.8148 - val_perfect: 0.6554\n",
            "Epoch 80/300\n",
            " - 21s - loss: 0.5857 - sparse: 0.8381 - perfect: 0.5114 - val_loss: 1.0694 - val_sparse: 0.8135 - val_perfect: 0.6530\n",
            "Epoch 81/300\n",
            " - 20s - loss: 0.5874 - sparse: 0.8366 - perfect: 0.5094 - val_loss: 1.0672 - val_sparse: 0.8142 - val_perfect: 0.6613\n",
            "Epoch 82/300\n",
            " - 21s - loss: 0.5792 - sparse: 0.8377 - perfect: 0.5055 - val_loss: 1.0677 - val_sparse: 0.8123 - val_perfect: 0.6530\n",
            "Epoch 83/300\n",
            " - 21s - loss: 0.5820 - sparse: 0.8365 - perfect: 0.5103 - val_loss: 1.0755 - val_sparse: 0.8159 - val_perfect: 0.6641\n",
            "Epoch 84/300\n",
            " - 21s - loss: 0.5817 - sparse: 0.8380 - perfect: 0.5076 - val_loss: 1.0787 - val_sparse: 0.8124 - val_perfect: 0.6560\n",
            "Epoch 85/300\n",
            " - 20s - loss: 0.5721 - sparse: 0.8405 - perfect: 0.5143 - val_loss: 1.0716 - val_sparse: 0.8137 - val_perfect: 0.6613\n",
            "Epoch 86/300\n",
            " - 20s - loss: 0.5709 - sparse: 0.8394 - perfect: 0.5117 - val_loss: 1.0647 - val_sparse: 0.8147 - val_perfect: 0.6633\n",
            "Epoch 87/300\n",
            " - 20s - loss: 0.5720 - sparse: 0.8401 - perfect: 0.5097 - val_loss: 1.0656 - val_sparse: 0.8190 - val_perfect: 0.6695\n",
            "Epoch 88/300\n",
            " - 21s - loss: 0.5683 - sparse: 0.8424 - perfect: 0.5200 - val_loss: 1.0758 - val_sparse: 0.8151 - val_perfect: 0.6639\n",
            "Epoch 89/300\n",
            " - 20s - loss: 0.5636 - sparse: 0.8409 - perfect: 0.5145 - val_loss: 1.0709 - val_sparse: 0.8138 - val_perfect: 0.6629\n",
            "Epoch 90/300\n",
            " - 20s - loss: 0.5591 - sparse: 0.8436 - perfect: 0.5188 - val_loss: 1.0722 - val_sparse: 0.8163 - val_perfect: 0.6673\n",
            "Epoch 91/300\n",
            " - 20s - loss: 0.5588 - sparse: 0.8437 - perfect: 0.5183 - val_loss: 1.0684 - val_sparse: 0.8165 - val_perfect: 0.6661\n",
            "Epoch 92/300\n",
            " - 20s - loss: 0.5547 - sparse: 0.8414 - perfect: 0.5158 - val_loss: 1.0734 - val_sparse: 0.8180 - val_perfect: 0.6691\n",
            "Epoch 93/300\n",
            " - 21s - loss: 0.5469 - sparse: 0.8462 - perfect: 0.5241 - val_loss: 1.0740 - val_sparse: 0.8167 - val_perfect: 0.6665\n",
            "Epoch 94/300\n",
            " - 21s - loss: 0.5591 - sparse: 0.8438 - perfect: 0.5146 - val_loss: 1.0812 - val_sparse: 0.8198 - val_perfect: 0.6679\n",
            "Epoch 95/300\n",
            " - 21s - loss: 0.5540 - sparse: 0.8450 - perfect: 0.5208 - val_loss: 1.0760 - val_sparse: 0.8185 - val_perfect: 0.6685\n",
            "Epoch 96/300\n",
            " - 21s - loss: 0.5419 - sparse: 0.8489 - perfect: 0.5282 - val_loss: 1.0653 - val_sparse: 0.8190 - val_perfect: 0.6681\n",
            "Epoch 97/300\n",
            " - 21s - loss: 0.5485 - sparse: 0.8457 - perfect: 0.5250 - val_loss: 1.0742 - val_sparse: 0.8197 - val_perfect: 0.6737\n",
            "Epoch 98/300\n",
            " - 21s - loss: 0.5498 - sparse: 0.8450 - perfect: 0.5217 - val_loss: 1.0591 - val_sparse: 0.8209 - val_perfect: 0.6723\n",
            "Epoch 99/300\n",
            " - 21s - loss: 0.5452 - sparse: 0.8475 - perfect: 0.5254 - val_loss: 1.0855 - val_sparse: 0.8165 - val_perfect: 0.6671\n",
            "Epoch 100/300\n",
            " - 21s - loss: 0.5360 - sparse: 0.8486 - perfect: 0.5280 - val_loss: 1.0849 - val_sparse: 0.8180 - val_perfect: 0.6657\n",
            "Epoch 101/300\n",
            " - 21s - loss: 0.5423 - sparse: 0.8470 - perfect: 0.5254 - val_loss: 1.0884 - val_sparse: 0.8195 - val_perfect: 0.6693\n",
            "Epoch 102/300\n",
            " - 21s - loss: 0.5398 - sparse: 0.8500 - perfect: 0.5318 - val_loss: 1.0766 - val_sparse: 0.8197 - val_perfect: 0.6701\n",
            "Epoch 103/300\n",
            " - 21s - loss: 0.5354 - sparse: 0.8499 - perfect: 0.5299 - val_loss: 1.0885 - val_sparse: 0.8159 - val_perfect: 0.6679\n",
            "Epoch 104/300\n",
            " - 21s - loss: 0.5369 - sparse: 0.8493 - perfect: 0.5302 - val_loss: 1.0901 - val_sparse: 0.8188 - val_perfect: 0.6739\n",
            "Epoch 105/300\n",
            " - 21s - loss: 0.5318 - sparse: 0.8507 - perfect: 0.5309 - val_loss: 1.0730 - val_sparse: 0.8226 - val_perfect: 0.6745\n",
            "Epoch 106/300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "x_short = big_text[0:500]\n",
        "y_short = big_data[0:500]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "    #K.manual_variable_initialization(True)  \n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # need this?\n",
        "    session.run(tf.tables_initializer())\n",
        "    #model = load_model(model_file)  \n",
        "    model.load_weights(model_file)  \n",
        "    eval = model.evaluate(x_train, y_train)\n",
        "    print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "    predicts = model.predict(x_short, batch_size=32)\n",
        "    print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmYm4s4AJ6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y_short)):\n",
        "    rep = report([y_short[i]], [predicts[i]])\n",
        "    if rep['perfect5'] == 1.0:\n",
        "        f = find_top_k_match(y_short[i], predicts[i], 5)\n",
        "        print('[{}]: {} -> {}'.format(i, x_short[i], [decoder.idx2syll[j] for j in f]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBZfxMLgpRA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k=5\n",
        "\n",
        "fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "def decodem(predict, top_k):\n",
        "    global fs\n",
        "    (top_vals, top_paths) = get_top_k(np.array([predict]), top_k=top_k)\n",
        "    #print('top_paths: ' + str(top_paths))\n",
        "    #print(top_paths.shape)\n",
        "    fs.mainloop(top_paths[0])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print('scorepaths[{}]: {}'.format(j, fs.scorepaths[j]))\n",
        "        #print('predict.shape: ', predict.shape)\n",
        "        #print('top_paths.shape: ', top_paths.shape)\n",
        "        #print('top_paths[{}]: {}'.format(j, top_paths))\n",
        "        #print('top_paths[{}][]: {}'.format(j, top_paths[0][np.arange(max_len), fs.scorepaths[j]]))\n",
        "        morepaths[j] = top_paths[0][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print('morepaths: ' + str(morepaths))\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    sentences = {}\n",
        "    if len(encoded) > 0:\n",
        "        #print(encoded)\n",
        "        decoded = []\n",
        "        for e1 in encoded:\n",
        "            if len(e1) > 0 and len(e1[0]) > 0:\n",
        "                dec = decoder.decode_sentences([e1])\n",
        "                decoded.append(dec)\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    for d4 in d3:\n",
        "                        go = True\n",
        "                        _lastidx = -1\n",
        "                        for w in d4:\n",
        "                            if not w in haikuwordset:\n",
        "                                go = False\n",
        "                            _idx = decoder.word2idx[w]\n",
        "                            if _lastidx > 0:\n",
        "                                if _lastidx == _idx or not wordmap.get(_lastidx, _idx):\n",
        "                                    go = False\n",
        "                                    #print('Fail: {},{} {},{}'.format(_lastidx, _idx, _lastword, w))\n",
        "                            _lastidx = _idx\n",
        "                            _lastword = w\n",
        "                        if go:\n",
        "                            key = ' '.join(d4)\n",
        "                            sentences[key] = d4\n",
        "                    #print('d3: ', d3)\n",
        "                    #key = ' '.join(d3)\n",
        "                    #sentences[key] = d3\n",
        "    return sentences\n",
        "\n",
        "# return N possible sentences with the fewest words\n",
        "def short_sentences(sentences):\n",
        "    out = {}\n",
        "    for i in range(1, max_len + 1):\n",
        "        for (k, v) in sentences.items():\n",
        "            if len(v) == i:\n",
        "                out[k] = v\n",
        "        if len(out) > 0:\n",
        "            return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   \n",
        "bigbatch = batch_size * 32\n",
        "big_eval = big_text[0:bigbatch]\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(big_eval[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          sentences = decodem(predicts[j], 5)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences):\n",
        "                    print('{} -> {}'.format(x_test[i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "#print('top preds: ', mini_preds[0])\n",
        "#print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    print('{} -> {}'.format(x_short[i], [decoder.idx2syll[x] for x in fs.scorepaths[0]]))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    print(mini_preds[i].shape)\n",
        "    print(morepaths.shape)\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print(fs.scorepaths[j])\n",
        "        #z = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "        #print(z)\n",
        "        morepaths[j] = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print(morepaths[0])\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    #d = []\n",
        "    # for x in encoded:\n",
        "    #   for y in x:\n",
        "    #        if len(y) > 0:\n",
        "    #           d.append(y[0])  \n",
        "    #d = np.array(d)\n",
        "    d = encoded\n",
        "    if len(d) > 0:\n",
        "        print('encoded sentences: ', d)\n",
        "        print(x_short[i], ':')\n",
        "        #print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(encoded)\n",
        "        #print('len(decoded): ', len(decoded))\n",
        "        ##print('len(decoded[0]): ', len(decoded[0]))\n",
        "        #print('len(decoded[0][0]): ', len(decoded[0][0]))\n",
        "        #print('len(decoded[0][0][0]): ', len(decoded[0][0][0]))\n",
        "        #print('decoded[0][0][0]: ', decoded[0][0][0])\n",
        "        sentences = {}\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    print('d3: ', d3)\n",
        "                    break\n",
        "                    key = ' '.join(d3)\n",
        "                    sentences[key] = d3\n",
        "                    break\n",
        "                     \n",
        "        print('[{}]  -> {}', i,list(sentences.keys())[0:10])\n",
        "    break\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_list = [sparse, perfect, sparse5, perfect5]\n",
        "metric_names = ['sparse', 'perfect', 'sparse5', 'perfect5']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6aZfdAnkCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}