{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+USE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "997f7b31-530f-4111-804c-241c6bd71485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pip install tensorflow-hub\n",
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n",
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.1)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "149cfd84-ae71-448b-df0a-c6ec89f444f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "4a39677c-5f45-49d6-899f-0d3b81be8e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "from cmu.wordmap import Wordmap\n",
        "from cmu.readhaiku import Reader\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 200000\n",
        "# cut texts after this number of words\n",
        "# number of output syllables in short haiku\n",
        "max_features = 16000\n",
        "# longest output sentence\n",
        "max_len = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=False\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_USE_5\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_USE_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "981095e4-1e13-47eb-848b-e08c1b6872fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!date\n",
        "word2sylls = trim_homynyms(word2sylls)\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "big_haiku_file = \"haiku_5.txt\"\n",
        "\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n",
        "reader = Reader(word2sylls, decoder, wordmap)\n",
        "(big_text, big_haiku, big_data) = reader.readfile(big_haiku_file)\n",
        "haikuwordset = reader.haikuwordset\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 28 01:49:45 UTC 2019\n",
            "2443 0\n",
            "# features:  15098\n",
            "office equipment -> office equipment : [ 211 2732 4968 6573 7383]\n",
            "office equipment -> office equipment : [[ 211]\n",
            " [2732]\n",
            " [4968]\n",
            " [6573]\n",
            " [7383]]\n",
            "Full length clauses:  40658\n",
            "Wordmap total entries:  47775\n",
            "Wordmap length:  210354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbXnnIliX2td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
        "embed = hub.Module(module_url)\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "a5ced0de-9080-4409-d57f-1676eea08cdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(big_text, big_data)\n",
        "x_train = x_train[0:(len(x_train) // batch_size) * batch_size]\n",
        "y_train = y_train[0:(len(y_train) // batch_size) * batch_size]\n",
        "x_test = x_test[0:(len(x_test) // batch_size) * batch_size]\n",
        "y_test = y_test[0:(len(y_test) // batch_size) * batch_size]\n",
        "\n",
        "print(x_test[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "some other people [[9827]\n",
            " [ 156]\n",
            " [2447]\n",
            " [8724]\n",
            " [8482]]\n",
            "x_train shape: (30464,)\n",
            "x_test shape: (10144,)\n",
            "y_train shape: (30464, 5, 1)\n",
            "y_test shape: (10144, 5, 1)\n",
            "[9827]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "e1526b32-569e-4034-a4f4-07694b183994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "def sparse_loss(y_true, y_pred):\n",
        "    return scc(y_true, y_pred)\n",
        "\n",
        "def perfect_loss(y_true, y_pred):\n",
        "    return scct(y_true, y_pred, scale=1.0)\n",
        "\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.4\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_medium, activation='relu')(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_small, activation='relu')(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.RepeatVector(max_len)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', sparse_loss, metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=True\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=3,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=0)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n",
        "    model.save_weights(model_file)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 01:50:16.527220 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0728 01:50:16.528907 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 01:50:17.318040 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0728 01:50:17.327586 140401553680256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0728 01:50:17.344130 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0728 01:50:19.798700 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 01:50:19.822025 140401553680256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras_stuff/loss.py:58: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_1 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 5, 512)            2449408   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_2 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5, 16000)          10928000  \n",
            "=================================================================\n",
            "Total params: 14,602,452\n",
            "Trainable params: 14,602,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "outputId": "1b400a04-6140-4e81-8044-2484b848aea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "outputId": "d4c4bf59-b2a6-4d16-f5df-a97da8b6df06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "\n",
        "\n",
        "x_short = big_text[0:500]\n",
        "y_short = big_data[0:500]\n",
        "predicts = None\n",
        "\n",
        "with tf.Session() as session:\n",
        "    #K.manual_variable_initialization(True)  \n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    # need this?\n",
        "    session.run(tf.tables_initializer())\n",
        "    #model = load_model(model_file)  \n",
        "    model.load_weights(model_file)  \n",
        "    eval = model.evaluate(x_train, y_train)\n",
        "    print('model.evaluate on short: ' ,model.metrics_names, eval)\n",
        "    predicts = model.predict(x_short, batch_size=32)\n",
        "    print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30464/30464 [==============================] - 25s 827us/step\n",
            "model.evaluate on short:  ['loss', 'sparse', 'perfect'] [0.4242363538546096, 0.9026391802966094, 0.7501969537815126]\n",
            "shape: (500, 5, 16000)\n",
            "5\n",
            "16000\n",
            "[[2.2796884e-10 2.2763902e-10 2.2919523e-10 ... 2.2466705e-10\n",
            "  2.2474850e-10 2.2584686e-10]\n",
            " [1.2497636e-13 1.2494919e-13 1.2988002e-13 ... 1.2551481e-13\n",
            "  1.2927501e-13 1.2390165e-13]\n",
            " [5.9984590e-17 6.1312758e-17 6.0831438e-17 ... 5.8041556e-17\n",
            "  5.9609811e-17 5.9504389e-17]\n",
            " [3.4098510e-19 3.3017403e-19 3.4410284e-19 ... 3.3824377e-19\n",
            "  3.2904991e-19 3.2384470e-19]\n",
            " [3.3415643e-16 3.3163698e-16 3.2803723e-16 ... 3.4137027e-16\n",
            "  3.2937893e-16 3.2102922e-16]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWmYm4s4AJ6l",
        "colab_type": "code",
        "outputId": "6be6d596-fccd-4275-d8eb-93d423505d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for i in range(len(y_short)):\n",
        "    rep = report([y_short[i]], [predicts[i]])\n",
        "    if rep['perfect5'] == 1.0:\n",
        "        f = find_top_k_match(y_short[i], predicts[i], 5)\n",
        "        print('[{}]: {} -> {}'.format(i, x_short[i], [decoder.idx2syll[j] for j in f]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]: office equipment -> ['AO', 'F AH S', 'IH', 'K W IH P', 'M AH N T']\n",
            "[1]: office equipment -> ['AO', 'F AH S', 'IH', 'K W IH P', 'M AH N T']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBZfxMLgpRA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k=5\n",
        "\n",
        "fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "def decodem(predict, top_k):\n",
        "    global fs\n",
        "    (top_vals, top_paths) = get_top_k(np.array([predict]), top_k=top_k)\n",
        "    #print('top_paths: ' + str(top_paths))\n",
        "    #print(top_paths.shape)\n",
        "    fs.mainloop(top_paths[0])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print('scorepaths[{}]: {}'.format(j, fs.scorepaths[j]))\n",
        "        #print('predict.shape: ', predict.shape)\n",
        "        #print('top_paths.shape: ', top_paths.shape)\n",
        "        #print('top_paths[{}]: {}'.format(j, top_paths))\n",
        "        #print('top_paths[{}][]: {}'.format(j, top_paths[0][np.arange(max_len), fs.scorepaths[j]]))\n",
        "        morepaths[j] = top_paths[0][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print('morepaths: ' + str(morepaths))\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    sentences = {}\n",
        "    if len(encoded) > 0:\n",
        "        #print(encoded)\n",
        "        decoded = []\n",
        "        for e1 in encoded:\n",
        "            if len(e1) > 0 and len(e1[0]) > 0:\n",
        "                dec = decoder.decode_sentences([e1])\n",
        "                decoded.append(dec)\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    for d4 in d3:\n",
        "                        go = True\n",
        "                        _lastidx = -1\n",
        "                        for w in d4:\n",
        "                            if not w in haikuwordset:\n",
        "                                go = False\n",
        "                            _idx = decoder.word2idx[w]\n",
        "                            if _lastidx > 0:\n",
        "                                if _lastidx == _idx or not wordmap.get(_lastidx, _idx):\n",
        "                                    go = False\n",
        "                                    #print('Fail: {},{} {},{}'.format(_lastidx, _idx, _lastword, w))\n",
        "                            _lastidx = _idx\n",
        "                            _lastword = w\n",
        "                        if go:\n",
        "                            key = ' '.join(d4)\n",
        "                            sentences[key] = d4\n",
        "                    #print('d3: ', d3)\n",
        "                    #key = ' '.join(d3)\n",
        "                    #sentences[key] = d3\n",
        "    return sentences\n",
        "\n",
        "# return N possible sentences with the fewest words\n",
        "def short_sentences(sentences):\n",
        "    out = {}\n",
        "    for i in range(1, max_len + 1):\n",
        "        for (k, v) in sentences.items():\n",
        "            if len(v) == i:\n",
        "                out[k] = v\n",
        "        if len(out) > 0:\n",
        "            return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "outputId": "8de880f0-bcee-4bf2-8eb7-f2b3b26ff2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "   \n",
        "bigbatch = batch_size * 32\n",
        "big_eval = big_text[0:bigbatch]\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(big_eval[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          sentences = decodem(predicts[j], 5)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences):\n",
        "                    print('{} -> {}'.format(x_test[i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7fb1503e8cf8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4661e49dbba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m        \u001b[0;31m#if np.min(f) > 0 and j == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0;31m#    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m        \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecodem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshort_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c4aa2ddcece8>\u001b[0m in \u001b[0;36mdecodem\u001b[0;34m(predict, top_k)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmorepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#print('morepaths: ' + str(morepaths))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_sentences\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_partial_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_partial_sentences\u001b[0;34m(self, predict, partial, step)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_partial_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'passing {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_partial_sentences\u001b[0;34m(self, predict, partial, step)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_partial_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'passing {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_partial_sentences\u001b[0;34m(self, predict, partial, step)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_sylls\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_sylls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'k: {}, pred: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "#print('top preds: ', mini_preds[0])\n",
        "#print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    print('{} -> {}'.format(x_short[i], [decoder.idx2syll[x] for x in fs.scorepaths[0]]))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    print(mini_preds[i].shape)\n",
        "    print(morepaths.shape)\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print(fs.scorepaths[j])\n",
        "        #z = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "        #print(z)\n",
        "        morepaths[j] = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print(morepaths[0])\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    #d = []\n",
        "    # for x in encoded:\n",
        "    #   for y in x:\n",
        "    #        if len(y) > 0:\n",
        "    #           d.append(y[0])  \n",
        "    #d = np.array(d)\n",
        "    d = encoded\n",
        "    if len(d) > 0:\n",
        "        print('encoded sentences: ', d)\n",
        "        print(x_short[i], ':')\n",
        "        #print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(encoded)\n",
        "        #print('len(decoded): ', len(decoded))\n",
        "        ##print('len(decoded[0]): ', len(decoded[0]))\n",
        "        #print('len(decoded[0][0]): ', len(decoded[0][0]))\n",
        "        #print('len(decoded[0][0][0]): ', len(decoded[0][0][0]))\n",
        "        #print('decoded[0][0][0]: ', decoded[0][0][0])\n",
        "        sentences = {}\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    print('d3: ', d3)\n",
        "                    break\n",
        "                    key = ' '.join(d3)\n",
        "                    sentences[key] = d3\n",
        "                    break\n",
        "                     \n",
        "        print('[{}]  -> {}', i,list(sentences.keys())[0:10])\n",
        "    break\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(big_haiku, big_data)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_list = [sparse, perfect, sparse5, perfect5]\n",
        "metric_names = ['sparse', 'perfect', 'sparse5', 'perfect5']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6aZfdAnkCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}