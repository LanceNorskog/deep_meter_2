{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZHG 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrnHfTpkale",
        "colab_type": "text"
      },
      "source": [
        "Use Cyber-ZHG's various Keras tools for NLP: Attention, Multi-head, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "76a4bfc9-0aa6-4683-9612-44adfff121a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.3\n",
        "!pip install keras-multi-head keras-pos-embd\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: keras==2.2.3 in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (3.13)\n",
            "Requirement already satisfied: keras-multi-head in /usr/local/lib/python3.6/dist-packages (0.20.0)\n",
            "Requirement already satisfied: keras-pos-embd in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.16.4)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (0.41.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.2.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "eb02086c-b01d-4bdb-f8a2-79bce12c4e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "e5567ea4-4408-40af-80de-e8fea8448e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from cmu.full import FullSearch\n",
        "from cmu.topk import get_top_k, decodem, short_sentences\n",
        "from cmu.wordmap import Wordmap\n",
        "from cmu.readhaiku import Reader\n",
        "\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_pos_embd import PositionEmbedding\n",
        "\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "#from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "import keras_stuff.metrics as my_metrics\n",
        "\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# number of words for hashing trick\n",
        "hash_mole = 20000\n",
        "# number of output syllables in short haiku\n",
        "max_features = 17000\n",
        "# longest output sentence\n",
        "num_sylls = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=True\n",
        "# use long as input\n",
        "use_big_text=True\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5\"\n",
        "model_file=model_base + \".h5\"\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "215d54d1-e742-49dc-9ebd-c63df758cb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!date\n",
        "print(word2sylls['door'])\n",
        "#word2sylls = trim_homynyms(word2sylls)\n",
        "print(word2sylls['door'])\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "big_haiku_file = \"haiku_5.txt\"\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n",
        "reader = Reader(word2sylls, decoder, wordmap)\n",
        "(big_text, big_haiku, big_data) = reader.readfile(big_haiku_file, max_words=max_words, \n",
        "    deduplicate_haiku=deduplicate_haiku, duplicate_haiku=duplicate_haiku, max_data=max_data)\n",
        "if use_big_text:\n",
        "    input_text = big_text\n",
        "else:\n",
        "    input_text = big_haiku\n",
        "big_hash = reader.gethash(input_text, max_words=max_words, hash_mole=hash_mole)\n",
        "haikuwordset = reader.haikuwordset\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 31 01:53:36 UTC 2019\n",
            "['D AO R']\n",
            "['D AO R']\n",
            "2443 0\n",
            "# features:  15098\n",
            "a white sink and door -> a white sink and door : [[  156]\n",
            " [14238]\n",
            " [10115]\n",
            " [  125]\n",
            " [ 1844]]\n",
            "Full length clauses:  100001\n",
            "Wordmap total entries:  12523\n",
            "Wordmap length:  229463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "bb219299-475b-4681-ace3-d9e727cc17a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Split multiple datasets across same index\n",
        "(train_i, test_i, _, _) = train_test_split(np.arange(len(big_data)), np.arange(len(big_data)))\n",
        "\n",
        "train_len=(len(train_i)//batch_size) * batch_size\n",
        "test_len=(len(test_i)//batch_size) * batch_size\n",
        "x_train = big_hash[train_i][:train_len]\n",
        "y_train = big_data[train_i][:train_len]\n",
        "x_test = big_hash[test_i][-test_len:]\n",
        "y_test = big_data[test_i][-test_len:]\n",
        "\n",
        "print(input_text[train_i[0]], x_train[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    #return layers.LSTM(size, return_sequences=return_sequences)\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on ledge of bathtub [  815. 19703. 16971.   474.     0.     0.     0.     0.     0.     0.] [[ 1873]\n",
            " [  123]\n",
            " [ 2513]\n",
            " [12923]\n",
            " [ 9622]]\n",
            "x_train shape: (74976, 10)\n",
            "x_test shape: (24992, 10)\n",
            "y_train shape: (74976, 5, 1)\n",
            "y_test shape: (24992, 5, 1)\n",
            "[1873]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "532f41f0-cf0a-4774-b42c-48b4ae7e74fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "embed_size=512\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.4\n",
        "\n",
        "metric_list = [my_metrics.sparse, my_metrics.perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "hash_input = layers.Input(shape=(max_words,), dtype='int32')\n",
        "x = layers.Embedding(hash_mole, embed_size, input_length=max_words)(hash_input)\n",
        "if False:\n",
        "    # did not train\n",
        "    # needs positional embedding?\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = MultiHeadAttention(4)(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Bidirectional(get_lstm(units//2, return_sequences=True))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.TimeDistributed(MultiHeadAttention(4))(x)\n",
        "    #x = layers.Flatten()(x)\n",
        "    #x = layers.Dropout(dropout)(x)\n",
        "    #x = layers.Dense(embed_size)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Bidirectional(get_lstm(units//2, return_sequences=False))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.RepeatVector(num_sylls)(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = PositionEmbedding(\n",
        "        input_dim=embed_size,\n",
        "        output_dim=embed_size,\n",
        "        mode=PositionEmbedding.MODE_ADD\n",
        "    )(x)\n",
        "    #x = layers.Dropout(dropout)(x)\n",
        "    #x = MultiHeadAttention(4)(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = get_lstm(units, return_sequences=True)(x)\n",
        "if False:\n",
        "    # this was somewhat effective\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = MultiHeadAttention(4)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=[hash_input], outputs=[output_layer])\n",
        "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=200,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 01:53:42.120000 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0731 01:53:42.136506 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0731 01:53:42.140397 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0731 01:53:42.158310 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0731 01:53:42.172358 139843943868288 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0731 01:53:44.192460 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0731 01:53:44.220478 139843943868288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 512)           10240000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "position_embedding_1 (Positi (None, 5, 512)            262144    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 17000)          8721000   \n",
            "=================================================================\n",
            "Total params: 22,901,352\n",
            "Trainable params: 22,901,352\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 01:53:45.009331 139843943868288 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 74976 samples, validate on 24992 samples\n",
            "Epoch 1/200\n",
            " - 148s - loss: 4.4161 - sparse: 0.2407 - perfect: 0.0485 - val_loss: 3.2296 - val_sparse: 0.3990 - val_perfect: 0.1303\n",
            "\n",
            "Epoch 00001: val_perfect improved from -inf to 0.13032, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 2/200\n",
            " - 146s - loss: 3.0182 - sparse: 0.4315 - perfect: 0.1387 - val_loss: 2.3782 - val_sparse: 0.5419 - val_perfect: 0.2262\n",
            "\n",
            "Epoch 00002: val_perfect improved from 0.13032 to 0.22623, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 3/200\n",
            " - 146s - loss: 2.4241 - sparse: 0.5251 - perfect: 0.1960 - val_loss: 1.8657 - val_sparse: 0.6286 - val_perfect: 0.3164\n",
            "\n",
            "Epoch 00003: val_perfect improved from 0.22623 to 0.31638, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 4/200\n",
            " - 146s - loss: 2.0301 - sparse: 0.5870 - perfect: 0.2472 - val_loss: 1.5112 - val_sparse: 0.6893 - val_perfect: 0.3862\n",
            "\n",
            "Epoch 00004: val_perfect improved from 0.31638 to 0.38616, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 5/200\n",
            " - 146s - loss: 1.7520 - sparse: 0.6306 - perfect: 0.2881 - val_loss: 1.2804 - val_sparse: 0.7324 - val_perfect: 0.4384\n",
            "\n",
            "Epoch 00005: val_perfect improved from 0.38616 to 0.43842, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 6/200\n",
            " - 146s - loss: 1.5402 - sparse: 0.6651 - perfect: 0.3236 - val_loss: 1.0930 - val_sparse: 0.7681 - val_perfect: 0.4845\n",
            "\n",
            "Epoch 00006: val_perfect improved from 0.43842 to 0.48448, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 7/200\n",
            " - 146s - loss: 1.3804 - sparse: 0.6922 - perfect: 0.3514 - val_loss: 0.9492 - val_sparse: 0.7957 - val_perfect: 0.5285\n",
            "\n",
            "Epoch 00007: val_perfect improved from 0.48448 to 0.52853, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 8/200\n",
            " - 146s - loss: 1.2494 - sparse: 0.7157 - perfect: 0.3808 - val_loss: 0.8458 - val_sparse: 0.8186 - val_perfect: 0.5675\n",
            "\n",
            "Epoch 00008: val_perfect improved from 0.52853 to 0.56754, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 9/200\n",
            " - 145s - loss: 1.1420 - sparse: 0.7348 - perfect: 0.4016 - val_loss: 0.7688 - val_sparse: 0.8372 - val_perfect: 0.6054\n",
            "\n",
            "Epoch 00009: val_perfect improved from 0.56754 to 0.60539, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 10/200\n",
            " - 146s - loss: 1.0533 - sparse: 0.7527 - perfect: 0.4258 - val_loss: 0.6884 - val_sparse: 0.8548 - val_perfect: 0.6419\n",
            "\n",
            "Epoch 00010: val_perfect improved from 0.60539 to 0.64189, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 11/200\n",
            " - 146s - loss: 0.9784 - sparse: 0.7667 - perfect: 0.4452 - val_loss: 0.6374 - val_sparse: 0.8681 - val_perfect: 0.6702\n",
            "\n",
            "Epoch 00011: val_perfect improved from 0.64189 to 0.67021, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 12/200\n",
            " - 145s - loss: 0.9127 - sparse: 0.7801 - perfect: 0.4637 - val_loss: 0.5910 - val_sparse: 0.8775 - val_perfect: 0.6901\n",
            "\n",
            "Epoch 00012: val_perfect improved from 0.67021 to 0.69010, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 13/200\n",
            " - 145s - loss: 0.8555 - sparse: 0.7914 - perfect: 0.4798 - val_loss: 0.5604 - val_sparse: 0.8850 - val_perfect: 0.7117\n",
            "\n",
            "Epoch 00013: val_perfect improved from 0.69010 to 0.71167, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 14/200\n",
            " - 146s - loss: 0.8066 - sparse: 0.8025 - perfect: 0.4983 - val_loss: 0.5301 - val_sparse: 0.8936 - val_perfect: 0.7349\n",
            "\n",
            "Epoch 00014: val_perfect improved from 0.71167 to 0.73492, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 15/200\n",
            " - 146s - loss: 0.7680 - sparse: 0.8101 - perfect: 0.5110 - val_loss: 0.5069 - val_sparse: 0.8993 - val_perfect: 0.7496\n",
            "\n",
            "Epoch 00015: val_perfect improved from 0.73492 to 0.74960, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 16/200\n",
            " - 146s - loss: 0.7260 - sparse: 0.8195 - perfect: 0.5266 - val_loss: 0.4816 - val_sparse: 0.9057 - val_perfect: 0.7659\n",
            "\n",
            "Epoch 00016: val_perfect improved from 0.74960 to 0.76593, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 17/200\n",
            " - 145s - loss: 0.6947 - sparse: 0.8263 - perfect: 0.5363 - val_loss: 0.4669 - val_sparse: 0.9107 - val_perfect: 0.7793\n",
            "\n",
            "Epoch 00017: val_perfect improved from 0.76593 to 0.77933, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 18/200\n",
            " - 146s - loss: 0.6630 - sparse: 0.8331 - perfect: 0.5467 - val_loss: 0.4447 - val_sparse: 0.9162 - val_perfect: 0.7980\n",
            "\n",
            "Epoch 00018: val_perfect improved from 0.77933 to 0.79802, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 19/200\n",
            " - 146s - loss: 0.6352 - sparse: 0.8383 - perfect: 0.5568 - val_loss: 0.4436 - val_sparse: 0.9181 - val_perfect: 0.8041\n",
            "\n",
            "Epoch 00019: val_perfect improved from 0.79802 to 0.80410, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_zhg_mha_5.h5\n",
            "Epoch 20/200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy (dropout={})'.format(dropout))\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTpMmewvKjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  print('x_test.shape ', x_test.shape)\n",
        "  print('y_text.shape ', y_test.shape)\n",
        "  eval_small = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on test data: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "top_k=2\n",
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          fs = FullSearch(num_sylls * 5, num_sylls, top_k)\n",
        "          (top_vals, top_paths) = get_top_k(predicts[j], top_k=top_k)\n",
        "          fs.mainloop(top_paths)\n",
        "          sentences = decodem(fs.scorepaths, top_paths, decoder, haikuwordset, wordmap)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences, num_sylls):\n",
        "                    print('{} -> {}'.format(input_text[test_i][i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSA8FtUUyju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}