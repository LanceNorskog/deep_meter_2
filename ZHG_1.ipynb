{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZHG 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrnHfTpkale",
        "colab_type": "text"
      },
      "source": [
        "Use Cyber-ZHG's various Keras tools for NLP: Attention, Multi-head, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "635bb35e-650a-44ca-831b-4f3f9edcb509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.3\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: keras==2.2.3 in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.16.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.3.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "e2f7b614-7808-4bea-9dc4-6fe1717f7826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "070b35d9-cc00-4cff-cc8b-d1ca31673d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from cmu.full import FullSearch\n",
        "from cmu.topk import get_top_k, decodem, short_sentences\n",
        "from cmu.wordmap import Wordmap\n",
        "from cmu.readhaiku import Reader\n",
        "\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "#from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "import keras_stuff.metrics as my_metrics\n",
        "\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# number of words for hashing trick\n",
        "hash_mole = 20000\n",
        "# number of output syllables in short haiku\n",
        "max_features = 17000\n",
        "# longest output sentence\n",
        "num_sylls = 5\n",
        "# longest input sentence\n",
        "max_words = 12\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=True\n",
        "# use long as input\n",
        "use_big_text=True\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_5\"\n",
        "model_file=model_base + \".h5\"\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "e37b5bf5-8e59-45f3-9ea5-8a8ba00735ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!date\n",
        "print(word2sylls['door'])\n",
        "#word2sylls = trim_homynyms(word2sylls)\n",
        "print(word2sylls['door'])\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "big_haiku_file = \"haiku_5.txt\"\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n",
        "reader = Reader(word2sylls, decoder, wordmap)\n",
        "(big_text, big_haiku, big_data) = reader.readfile(big_haiku_file, max_words=max_words, \n",
        "    deduplicate_haiku=deduplicate_haiku, duplicate_haiku=duplicate_haiku, max_data=max_data)\n",
        "if use_big_text:\n",
        "    input_text = big_text\n",
        "else:\n",
        "    input_text = big_haiku\n",
        "big_hash = reader.gethash(input_text, max_words=max_words, hash_mole=hash_mole)\n",
        "haikuwordset = reader.haikuwordset\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 29 02:10:16 UTC 2019\n",
            "['D AO R']\n",
            "['D AO R']\n",
            "2443 0\n",
            "# features:  15098\n",
            "a white sink and door -> a white sink and door : [[  156]\n",
            " [14238]\n",
            " [10115]\n",
            " [  125]\n",
            " [ 1844]]\n",
            "Full length clauses:  100001\n",
            "Wordmap total entries:  12388\n",
            "Wordmap length:  229463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "2366e396-4d67-4294-aef9-e3bd8e455a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# Split multiple datasets across same index\n",
        "(train_i, test_i, _, _) = train_test_split(np.arange(len(big_data)), np.arange(len(big_data)))\n",
        "\n",
        "train_len=(len(train_i)//batch_size) * batch_size\n",
        "test_len=(len(test_i)//batch_size) * batch_size\n",
        "x_train = big_hash[train_i][:train_len]\n",
        "y_train = big_data[train_i][:train_len]\n",
        "x_test = big_hash[test_i][-test_len:]\n",
        "y_test = big_data[test_i][-test_len:]\n",
        "\n",
        "print(input_text[train_i[0]], x_train[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    #return layers.LSTM(size, return_sequences=return_sequences)\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "her hands on her hips [18127. 13619.   815. 18127.  3193.     0.     0.     0.     0.     0.\n",
            "     0.     0.] [[2425]\n",
            " [ 394]\n",
            " [ 208]\n",
            " [4724]\n",
            " [ 575]]\n",
            "x_train shape: (74976, 12)\n",
            "x_test shape: (24992, 12)\n",
            "y_train shape: (74976, 5, 1)\n",
            "y_test shape: (24992, 5, 1)\n",
            "[2425]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "a6f73bdf-b689-4910-c751-1277806b2a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        }
      },
      "source": [
        "\n",
        "\n",
        "embed_size=512\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.5\n",
        "\n",
        "metric_list = [my_metrics.sparse, my_metrics.perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "hash_input = layers.Input(shape=(max_words,), dtype='int32')\n",
        "x = layers.Embedding(hash_mole, embed_size, input_length=max_words)(hash_input)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "x = layers.Bidirectional(get_lstm(units//2, return_sequences=False))(x)\n",
        "x = layers.RepeatVector(num_sylls)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "#output_layer = layers.TimeDistributed(layers.Dense(max_features, activation='softmax'))(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[hash_input], outputs=[output_layer])\n",
        "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=True\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0729 02:10:22.017467 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0729 02:10:22.032557 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0729 02:10:22.035873 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0729 02:10:22.048638 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0729 02:10:22.057490 140112299325312 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0729 02:10:23.562586 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0729 02:10:23.751166 140112299325312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 12)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 12, 512)           10240000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 17000)          8721000   \n",
            "=================================================================\n",
            "Total params: 22,639,208\n",
            "Trainable params: 22,639,208\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95067fb2-af51-459a-fb7f-e6554e81b606"
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy (dropout={})'.format(dropout))\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTpMmewvKjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c617bb44-1ab6-4ca4-8604-02d501a5f42d"
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  print('x_test.shape ', x_test.shape)\n",
        "  print('y_text.shape ', y_test.shape)\n",
        "  eval_small = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on test data: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_test.shape  (24992, 12)\n",
            "y_text.shape  (24992, 5, 1)\n",
            "24992/24992 [==============================] - 7s 286us/step\n",
            "model.evaluate on test data:  ['loss', 'sparse', 'perfect'] [0.5310620646661436, 0.8898607555027961, 0.7236715749039693]\n",
            "history:  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4738cd4e-f9ac-419b-efb6-525501bd3945"
      },
      "source": [
        "top_k=2\n",
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(x_train[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          fs = FullSearch(num_sylls * 5, num_sylls, top_k)\n",
        "          (top_vals, top_paths) = get_top_k(predicts[j], top_k=top_k)\n",
        "          fs.mainloop(top_paths)\n",
        "          sentences = decodem(fs.scorepaths, top_paths, decoder, haikuwordset, wordmap)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences, num_sylls):\n",
        "                    print('{} -> {}'.format(input_text[train_i][i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "her hands on her hips -> his hands on her hips\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a safari truck -> a safari truck\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "sitting on a motorcycle, wearing a jacket -> a motorcycle\n",
            "sitting on a motorcycle, wearing a jacket -> a motor cycle\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "tending to goats on top of a van -> a top of a van\n",
            "tending to goats on top of a van -> on top of a van\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "top of a block wall -> top of a stone wall\n",
            "top of a block wall -> front of a stone wall\n",
            "top of a block wall -> top of a block wall\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "above an intersection -> a intersection\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a motorcycle -> his motorcycle\n",
            "a motorcycle -> a motorcycle\n",
            "a motorcycle -> his motor scooter\n",
            "a motorcycle -> a motor scooter\n",
            "a motorcycle -> his motor cycle\n",
            "a motorcycle -> a motor cycle\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a huge docked sail boat -> a huge docked sail boat\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "pictures of themselves -> pictures of themselves\n",
            "pictures of themselves -> picture of themselves\n",
            "pictures of themselves -> pictures of them self\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "near a window sil -> near a window sil\n",
            "near a window sil -> near a window sill\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "on a landing strip -> down a landing strip\n",
            "on a landing strip -> on a landing strip\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "this persons bathroom -> this lavish bathroom\n",
            "this persons bathroom -> this persons bathroom\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a motorcycle -> his motorcycle\n",
            "a motorcycle -> a motorcycle\n",
            "a motorcycle -> his motor scooter\n",
            "a motorcycle -> a motor scooter\n",
            "a motorcycle -> his motor cycle\n",
            "a motorcycle -> a motor cycle\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "riding down the street -> standing on the street\n",
            "riding down the street -> riding down the street\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "in a tiny space -> in a tiny space\n",
            "in a tiny space -> on a tiny space\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "sitting beside each other in a garage -> beside each other\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a motorcycle -> his motorcycle\n",
            "a motorcycle -> a motorcycle\n",
            "a motorcycle -> his motor scooter\n",
            "a motorcycle -> a motor scooter\n",
            "a motorcycle -> his motor cycle\n",
            "a motorcycle -> a motor cycle\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "over the edge of the tub -> the side of the wall\n",
            "over the edge of the tub -> the side of the tub\n",
            "over the edge of the tub -> the side of a wall\n",
            "over the edge of the tub -> the side of a tub\n",
            "over the edge of the tub -> the edge of the wall\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "is parked on the drive -> is parked at the road\n",
            "is parked on the drive -> is parked on the road\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "of cars and people -> of cars and people\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "on a wooden bench -> at a wooden park\n",
            "on a wooden bench -> on a wooden park\n",
            "on a wooden bench -> at a wooden bench\n",
            "on a wooden bench -> on a wooden bench\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "sitting in a chair -> sitting in a chair\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a parking garage -> a parking meter\n",
            "a parking garage -> a parking garage\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "parked on a sidewalk -> parked in the sidewalk\n",
            "parked on a sidewalk -> sits in a sidewalk\n",
            "parked on a sidewalk -> parked in a sidewalk\n",
            "parked on a sidewalk -> sits on the sidewalk\n",
            "parked on a sidewalk -> parked on the sidewalk\n",
            "parked on a sidewalk -> sits on a sidewalk\n",
            "parked on a sidewalk -> parked on a sidewalk\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "be an empty bathroom -> an empty restroom\n",
            "be an empty bathroom -> an empty bathtub\n",
            "be an empty bathroom -> an empty bathroom\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "in the dinning room -> in the dinning room\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "across a kitchen table -> a kitchen table\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "grassy area -> grassy area\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "using a cell phone -> using a cellphone\n",
            "using a cell phone -> using her cell phone\n",
            "using a cell phone -> using a cell phone\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a brown and black dog -> white brown and white cat\n",
            "a brown and black dog -> a white and white cat\n",
            "a brown and black dog -> white brown and white dog\n",
            "a brown and black dog -> a white and white dog\n",
            "a brown and black dog -> white brown and black cat\n",
            "a brown and black dog -> a brown and white cat\n",
            "a brown and black dog -> a white and black cat\n",
            "a brown and black dog -> white brown and black dog\n",
            "a brown and black dog -> a brown and white dog\n",
            "a brown and black dog -> a white and black dog\n",
            "a brown and black dog -> a brown and black cat\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "over a mountain -> over a mountain\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "a motorcycle sitting next to a dump truch on the road -> a motorcycle\n",
            "a motorcycle sitting next to a dump truch on the road -> a motor cycle\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n",
            "on the toilet seat -> on the toilet seat\n",
            "(5, 17000)\n",
            "predict.shape:  (5, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cbb09458ee17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0;34m(\u001b[0m\u001b[0mtop_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecodem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorepaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhaikuwordset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshort_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sylls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/topk.py\u001b[0m in \u001b[0;36mdecodem\u001b[0;34m(scorepaths, top_paths, decoder, wordset, wordmap)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmorepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorepaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m#print('morepaths: ' + str(morepaths))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmorepaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_sentences\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_partial_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_partial_sentences\u001b[0;34m(self, predict, partial, step)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_partial_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                     \u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'passing {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cmu/mappers.py\u001b[0m in \u001b[0;36mget_partial_sentences\u001b[0;34m(self, predict, partial, step)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_sylls\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_sylls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword_ind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mnoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'k: {}, pred: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}