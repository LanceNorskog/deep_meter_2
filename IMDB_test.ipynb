{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "06bb35d2-b0ad-4c19-8508-5b50c1a556c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2277
        }
      },
      "source": [
        "!pip install numpy==1.16.1\n",
        "!pip install keras==2.1.2\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, CuDNNLSTM, Bidirectional\n",
        "from keras.datasets import imdb\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words\n",
        "# (among top max_features most common words)\n",
        "maxlen = 100\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_val = x_test[0:len(x_test)//2]\n",
        "y_val = x_test[0:len(y_test)//2]\n",
        "x_test = x_test[len(x_test)//2 : ]\n",
        "y_test = x_test[len(y_test)//2 : ]\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print(len(x_val), 'validation sequences')\n",
        "\n",
        "def compress(x, y):\n",
        "    xout = []\n",
        "    yout = []\n",
        "    for i in range(len(x)):\n",
        "        if x[i][0] != 0:\n",
        "            xout.append(x[i])\n",
        "            yout.append(y[i])\n",
        "    xa = np.array(xout)\n",
        "    print(xa.shape)\n",
        "    return np.array(xout), np.array(yout)\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "#y_train = np.array(y_train)\n",
        "#y_test = np.array(y_test)\n",
        "y_train = np.expand_dims(np.array(x_train), -1)\n",
        "y_test = np.expand_dims(np.array(x_test), -1)\n",
        "y_val = np.expand_dims(np.array(x_val), -1)\n",
        "(x_train, y_train) = compress(x_train, y_train)\n",
        "(x_test, y_test) = compress(x_test, y_test)\n",
        "(x_val, y_val) = compress(x_val, y_val)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('x_val shape:', x_val.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print('y_val shape:', y_val.shape)\n",
        "\n",
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    # If the shape of y_true is (num_samples, 1), flatten to (num_samples,)\n",
        "    return K.cast(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),\n",
        "                  K.floatx())\n",
        "\n",
        "def sparse_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    # If the shape of y_true is (num_samples, 1), flatten to (num_samples,)\n",
        "    return K.cast(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),\n",
        "                  K.floatx())\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sequence(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def sparse_k(y_true, y_pred):\n",
        "    return sparse_top_k_categorical_accuracy(y_true, y_pred)\n",
        "def sequence_k(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_top_k_per_sequence(y_true, y_pred)\n",
        "\n",
        "def test_top_k_categorical_accuracy():\n",
        "    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n",
        "    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n",
        "    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
        "                                                               k=3))\n",
        "    assert np.mean(success_result) == 1\n",
        "    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
        "                                                               k=2))\n",
        "    assert np.mean(partial_result) == 0.5\n",
        "    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n",
        "                                                               k=1))\n",
        "    assert np.mean(failure_result) == 0\n",
        "\n",
        "\n",
        "units=16\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, units*2, input_length=maxlen))\n",
        "model.add(Bidirectional(get_lstm(units, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(max_features, activation='softmax'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=[sparse, sequence])\n",
        "model.summary()\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=30,\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n",
            "Requirement already satisfied: keras==2.1.2 in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.16.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.2) (3.13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "12500 test sequences\n",
            "12500 validation sequences\n",
            "Pad sequences (samples x time)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0619 21:27:01.368257 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0619 21:27:01.383125 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0619 21:27:01.387421 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(22227, 100)\n",
            "(11012, 100)\n",
            "(11025, 100)\n",
            "x_train shape: (22227, 100)\n",
            "x_test shape: (11012, 100)\n",
            "x_val shape: (11025, 100)\n",
            "y_train shape: (22227, 100, 1)\n",
            "y_test shape: (11012, 100, 1)\n",
            "y_val shape: (11025, 100, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0619 21:27:02.226286 140490570119040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0619 21:27:02.422053 140490570119040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3019: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0619 21:27:02.458566 140490570119040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0619 21:27:02.465406 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0619 21:27:02.485378 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2915: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0619 21:27:02.511855 140490570119040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1230: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0619 21:27:02.580248 140490570119040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 32)           640000    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 32)           6400      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 32)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100, 20000)        660000    \n",
            "=================================================================\n",
            "Total params: 1,306,400\n",
            "Trainable params: 1,306,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0619 21:27:02.965644 140490570119040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0619 21:27:02.979163 140490570119040 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22227 samples, validate on 11012 samples\n",
            "Epoch 1/30\n",
            " - 105s - loss: 6.7257 - sparse: 0.0514 - sequence: 0.0000e+00 - val_loss: 6.0533 - val_sparse: 0.0552 - val_sequence: 0.0000e+00\n",
            "Epoch 2/30\n",
            " - 102s - loss: 5.9447 - sparse: 0.0548 - sequence: 0.0000e+00 - val_loss: 5.6575 - val_sparse: 0.0552 - val_sequence: 0.0000e+00\n",
            "Epoch 3/30\n",
            " - 102s - loss: 5.5278 - sparse: 0.0649 - sequence: 0.0000e+00 - val_loss: 5.1200 - val_sparse: 0.1167 - val_sequence: 0.0000e+00\n",
            "Epoch 4/30\n",
            " - 102s - loss: 5.0858 - sparse: 0.0947 - sequence: 0.0000e+00 - val_loss: 4.6250 - val_sparse: 0.1886 - val_sequence: 0.0000e+00\n",
            "Epoch 5/30\n",
            " - 102s - loss: 4.6219 - sparse: 0.1353 - sequence: 0.0000e+00 - val_loss: 4.0007 - val_sparse: 0.2747 - val_sequence: 0.0000e+00\n",
            "Epoch 6/30\n",
            " - 102s - loss: 4.2079 - sparse: 0.1780 - sequence: 0.0000e+00 - val_loss: 3.5236 - val_sparse: 0.3588 - val_sequence: 0.0000e+00\n",
            "Epoch 7/30\n",
            " - 102s - loss: 3.7759 - sparse: 0.2510 - sequence: 0.0000e+00 - val_loss: 2.8598 - val_sparse: 0.5558 - val_sequence: 0.0000e+00\n",
            "Epoch 8/30\n",
            " - 102s - loss: 3.2915 - sparse: 0.3582 - sequence: 0.0000e+00 - val_loss: 2.3753 - val_sparse: 0.6730 - val_sequence: 0.0000e+00\n",
            "Epoch 9/30\n",
            " - 102s - loss: 2.9639 - sparse: 0.4197 - sequence: 0.0000e+00 - val_loss: 2.0283 - val_sparse: 0.7325 - val_sequence: 0.0000e+00\n",
            "Epoch 10/30\n",
            " - 102s - loss: 2.7113 - sparse: 0.4624 - sequence: 0.0000e+00 - val_loss: 1.7597 - val_sparse: 0.7843 - val_sequence: 0.0000e+00\n",
            "Epoch 11/30\n",
            " - 102s - loss: 2.5229 - sparse: 0.4907 - sequence: 0.0000e+00 - val_loss: 1.5485 - val_sparse: 0.8142 - val_sequence: 0.0000e+00\n",
            "Epoch 12/30\n",
            " - 102s - loss: 2.3466 - sparse: 0.5226 - sequence: 0.0000e+00 - val_loss: 1.3712 - val_sparse: 0.8409 - val_sequence: 0.0000e+00\n",
            "Epoch 13/30\n",
            " - 102s - loss: 2.2138 - sparse: 0.5433 - sequence: 0.0000e+00 - val_loss: 1.2292 - val_sparse: 0.8573 - val_sequence: 0.0000e+00\n",
            "Epoch 14/30\n",
            " - 102s - loss: 2.0976 - sparse: 0.5635 - sequence: 0.0000e+00 - val_loss: 1.1135 - val_sparse: 0.8721 - val_sequence: 0.0000e+00\n",
            "Epoch 15/30\n",
            " - 102s - loss: 1.9931 - sparse: 0.5828 - sequence: 0.0000e+00 - val_loss: 1.0158 - val_sparse: 0.8854 - val_sequence: 9.0810e-05\n",
            "Epoch 16/30\n",
            " - 102s - loss: 1.9078 - sparse: 0.5959 - sequence: 0.0000e+00 - val_loss: 0.9312 - val_sparse: 0.8973 - val_sequence: 5.4486e-04\n",
            "Epoch 17/30\n",
            " - 102s - loss: 1.8343 - sparse: 0.6079 - sequence: 0.0000e+00 - val_loss: 0.8596 - val_sparse: 0.9059 - val_sequence: 0.0013\n",
            "Epoch 18/30\n",
            " - 102s - loss: 1.7653 - sparse: 0.6204 - sequence: 0.0000e+00 - val_loss: 0.7961 - val_sparse: 0.9153 - val_sequence: 0.0022\n",
            "Epoch 19/30\n",
            " - 102s - loss: 1.7032 - sparse: 0.6323 - sequence: 0.0000e+00 - val_loss: 0.7413 - val_sparse: 0.9228 - val_sequence: 0.0040\n",
            "Epoch 20/30\n",
            " - 102s - loss: 1.6464 - sparse: 0.6424 - sequence: 0.0000e+00 - val_loss: 0.6931 - val_sparse: 0.9258 - val_sequence: 0.0047\n",
            "Epoch 21/30\n",
            " - 102s - loss: 1.5960 - sparse: 0.6512 - sequence: 0.0000e+00 - val_loss: 0.6515 - val_sparse: 0.9320 - val_sequence: 0.0077\n",
            "Epoch 22/30\n",
            " - 102s - loss: 1.5515 - sparse: 0.6594 - sequence: 0.0000e+00 - val_loss: 0.6153 - val_sparse: 0.9352 - val_sequence: 0.0092\n",
            "Epoch 23/30\n",
            " - 102s - loss: 1.5121 - sparse: 0.6668 - sequence: 0.0000e+00 - val_loss: 0.5836 - val_sparse: 0.9388 - val_sequence: 0.0122\n",
            "Epoch 24/30\n",
            " - 102s - loss: 1.4734 - sparse: 0.6745 - sequence: 0.0000e+00 - val_loss: 0.5547 - val_sparse: 0.9422 - val_sequence: 0.0140\n",
            "Epoch 25/30\n",
            " - 102s - loss: 1.4380 - sparse: 0.6817 - sequence: 0.0000e+00 - val_loss: 0.5276 - val_sparse: 0.9452 - val_sequence: 0.0174\n",
            "Epoch 26/30\n",
            " - 102s - loss: 1.4042 - sparse: 0.6883 - sequence: 0.0000e+00 - val_loss: 0.5030 - val_sparse: 0.9476 - val_sequence: 0.0204\n",
            "Epoch 27/30\n",
            " - 102s - loss: 1.3740 - sparse: 0.6941 - sequence: 0.0000e+00 - val_loss: 0.4809 - val_sparse: 0.9515 - val_sequence: 0.0274\n",
            "Epoch 28/30\n",
            " - 102s - loss: 1.3444 - sparse: 0.7002 - sequence: 0.0000e+00 - val_loss: 0.4603 - val_sparse: 0.9539 - val_sequence: 0.0308\n",
            "Epoch 29/30\n",
            " - 102s - loss: 1.3198 - sparse: 0.7047 - sequence: 0.0000e+00 - val_loss: 0.4402 - val_sparse: 0.9573 - val_sequence: 0.0414\n",
            "Epoch 30/30\n",
            " - 102s - loss: 1.2960 - sparse: 0.7095 - sequence: 0.0000e+00 - val_loss: 0.4231 - val_sparse: 0.9593 - val_sequence: 0.0453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a5f2a766-d38e-4524-883e-3548d3f8e9c7"
      },
      "source": [
        "\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['sparse'])\n",
        "  plt.plot(history.history['val_sparse'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('sparse_categorical_accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ZR0kc7X-tm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "c05a6de3-cffc-4081-bc32-b399d507401f"
      },
      "source": [
        "short = x_val[0:100]\n",
        "\n",
        "if True:\n",
        "#with tf.Session() as session:\n",
        "#  K.set_session(session)\n",
        "#  session.run(tf.global_variables_initializer())\n",
        "#  session.run(tf.tables_initializer())\n",
        "  #model.load_weights('./model.h5')  \n",
        "  eval = model.evaluate(x_val, y_val)\n",
        "  print('model.evaluate on val holdout: ' ,model.metrics_names, eval)\n",
        "  print('history: ', history)\n",
        "  predicts = model.predict(short, batch_size=32)\n",
        "  print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11025/11025 [==============================] - 20s 2ms/step\n",
            "model.evaluate on val holdout:  ['loss', 'sparse', 'sequence'] [0.4216869519762441, 0.9597560077513697, 0.04417233560124492]\n",
            "history:  <keras.callbacks.History object at 0x7fc625786a58>\n",
            "shape: (100, 100, 20000)\n",
            "100\n",
            "20000\n",
            "[[1.0213755e-18 2.1669586e-18 2.4100757e-13 ... 7.5307121e-21\n",
            "  1.7676047e-22 1.2392732e-25]\n",
            " [8.1977246e-12 3.7876737e-09 1.3391892e-06 ... 1.0526842e-09\n",
            "  4.5809236e-14 1.0058988e-09]\n",
            " [1.1759797e-10 1.3913035e-08 8.1781621e-09 ... 4.9446470e-08\n",
            "  3.4127712e-08 1.0619011e-09]\n",
            " ...\n",
            " [9.2855955e-16 2.0520503e-17 2.8446085e-13 ... 5.0007432e-17\n",
            "  3.7405610e-15 1.1881861e-21]\n",
            " [7.8344126e-21 1.9702042e-18 7.7574420e-15 ... 3.4856338e-22\n",
            "  5.8134182e-25 1.3493316e-24]\n",
            " [2.5707738e-09 4.2684383e-07 2.1300134e-03 ... 1.3003063e-07\n",
            "  1.7719579e-09 5.0279020e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFy2tfyIYOtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bc459c5a-8ccd-4e8c-81c4-a6b3356bddbe"
      },
      "source": [
        "def maxindx(pred):\n",
        "    maxi=-1\n",
        "    maxv=-1.0\n",
        "    for x in range(len(pred)):\n",
        "        if pred[x] > maxv:\n",
        "            maxv = pred[x]\n",
        "            maxi = x\n",
        "    return (maxi, maxv)\n",
        "\n",
        "def match(data, prediction):\n",
        "    good = 0\n",
        "    total = 0\n",
        "    for i in range(len(data)):\n",
        "       # if data[i] < 0.000001:\n",
        "       #     break\n",
        "        if data[i] == np.argmax(prediction[i]):\n",
        "            good += 1\n",
        "        total += 1\n",
        "    #print('{}, {}'.format(good, total))\n",
        "    if (total == 0):\n",
        "        return 0\n",
        "    return good / total\n",
        "\n",
        "good = 0.0\n",
        "total = 0\n",
        "for n in range(len(short)):\n",
        "    #print(short[n][0:5])\n",
        "    if short[n][0] != 0 or True:\n",
        "        check = match(short[n], predicts[n])\n",
        "        if check > 0.9999:\n",
        "            good += 1\n",
        "    total += 1\n",
        "\n",
        "print('Score: ', good / total)\n",
        "print('values: ', maxindx(predicts[0][0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score:  0.02\n",
            "values:  (6, 0.9999993)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNWVxMyKd3P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}