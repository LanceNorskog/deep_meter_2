{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZHG text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrnHfTpkale",
        "colab_type": "text"
      },
      "source": [
        "Use Cyber-ZHG's various Keras tools for NLP: Attention, Multi-head, etc. \n",
        "Use external model manager to allow freezing weights of basic model.\n",
        "Text output, not syllables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "bd05f749-b7a7-4d47-b415-2927dfb1f20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "#!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.3\n",
        "!pip install keras-multi-head keras-pos-embd\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.2.3 in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.16.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.1.0)\n",
            "Requirement already satisfied: keras-multi-head in /usr/local/lib/python3.6/dist-packages (0.20.0)\n",
            "Requirement already satisfied: keras-pos-embd in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.16.4)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (0.41.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.2.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.3.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "de1bc01d-f4dc-475d-db5a-c9e3ff124287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q -U git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "6b930163-1813-4652-837a-8111d92095ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from cmu.full import FullSearch\n",
        "from cmu.topk import get_top_k, decodem, short_sentences\n",
        "from cmu.wordmap import Wordmap\n",
        "from cmu.readhaiku2 import Reader\n",
        "\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_pos_embd import PositionEmbedding\n",
        "\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "#from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "import keras_stuff.metrics as my_metrics\n",
        "from keras_stuff.modelmgr import ModelManager\n",
        "\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 10000\n",
        "# number of words for hashing trick\n",
        "hash_mole = 20000\n",
        "# number of output words in haiku\n",
        "max_features = 140000\n",
        "# longest output sentence\n",
        "num_sylls = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=True\n",
        "# use long as input\n",
        "use_big_text=True\n",
        "\n",
        "model_base_file=\"/content/gdrive/My Drive/Colab Notebooks/haiku_text_base_5.h5\"\n",
        "model_file=\"/content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\"\n",
        "print(model_file)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "05e740de-ab48-4686-f811-d03a3abe41cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!date\n",
        "print(word2sylls['door'])\n",
        "#word2sylls = trim_homynyms(word2sylls)\n",
        "print(word2sylls['door'])\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "big_haiku_file = \"haiku_5.txt\"\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n",
        "reader = Reader(word2sylls, decoder)\n",
        "(big_text, big_haiku) = reader.readfile(big_haiku_file, max_words=max_words, \n",
        "    deduplicate_haiku=deduplicate_haiku, duplicate_haiku=duplicate_haiku, max_data=max_data)\n",
        "if use_big_text:\n",
        "    input_text = big_text\n",
        "else:\n",
        "    input_text = big_haiku\n",
        "big_hash = reader.gethash(input_text, max_words=max_words, hash_mole=hash_mole)\n",
        "big_indexes = reader.getindexes(big_haiku, max_words=5)\n",
        "print('{} -> {} : {}k, {}'.format(big_text[0], big_haiku[0], big_hash[0], big_indexes[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', len(decoder.wordlist))\n",
        "assert max_features > len(decoder.wordlist)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug  4 02:53:27 UTC 2019\n",
            "['D AO R']\n",
            "['D AO R']\n",
            "['<START>', '<STOP>', '<PAD>', '!exclamation-point', '\"close-quote', '\"double-quote', '\"end-of-quote', '\"end-quote', '\"in-quotes', '\"quote']\n",
            "2443 0\n",
            "# features:  15098\n",
            "a white sink and door -> a white sink and door : [  309. 16042.   392.  7161.  3485.     0.     0.     0.     0.     0.]k, [    70 125741 106762   3649  32458]\n",
            "Full length clauses:  10000\n",
            "Wordmap total entries:  129476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "1044f859-867f-400c-9cc1-e2870b42667d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Split multiple datasets across same index\n",
        "(train_i, test_i, _, _) = train_test_split(np.arange(len(big_text)), np.arange(len(big_text)))\n",
        "\n",
        "train_len=(len(train_i)//batch_size) * batch_size\n",
        "test_len=(len(test_i)//batch_size) * batch_size\n",
        "x_train = big_hash[train_i][:train_len]\n",
        "y_train = big_indexes[train_i][:train_len]\n",
        "y_train = np.reshape(y_train, (-1, y_train.shape[1], 1))\n",
        "x_test = big_hash[test_i][-test_len:]\n",
        "y_test = big_indexes[test_i][-test_len:]\n",
        "y_test = np.reshape(y_test, (-1, y_test.shape[1], 1))\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(input_text[train_i[0]], x_train[0], str(x_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    #return layers.LSTM(size, return_sequences=return_sequences)\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7488, 10)\n",
            "(7488, 5, 1)\n",
            "toiletry items [16237. 16995.     0.     0.     0.     0.     0.     0.     0.     0.] [11541.   309.  5179.  6343.     0.     0.     0.     0.     0.     0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "ecd783f5-9c44-4270-b597-e855fdd3d8e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelmgr = ModelManager()\n",
        "\n",
        "embed_size=512\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.5\n",
        "\n",
        "model_params = {\n",
        "    'max_words':max_words , 'hash_mole': hash_mole, \n",
        "    'embed_size': embed_size, 'units': units, \n",
        "    'num_sylls': num_sylls, 'max_features': max_features\n",
        "    }\n",
        "\n",
        "metric_list = [my_metrics.sparse, my_metrics.perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = modelmgr.get_model(model_params)\n",
        "\n",
        "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_base_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    if os.path.exists(model_base_file):\n",
        "        print(\"Reloading weights\")\n",
        "        #model.load_weights(model_base_file, by_name=True)\n",
        "        #modelmgr.unfreeze(model, 'dense_1')\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=20,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 02:53:31.730429 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0804 02:53:31.752432 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0804 02:53:31.756469 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0804 02:53:31.773869 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0804 02:53:31.785213 140256397109120 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0804 02:53:33.915547 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0804 02:53:33.944914 140256397109120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 512)           10240000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 140000)         71820000  \n",
            "=================================================================\n",
            "Total params: 85,738,208\n",
            "Trainable params: 85,738,208\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0804 02:53:34.654598 140256397109120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 7488 samples, validate on 2496 samples\n",
            "Epoch 1/20\n",
            " - 61s - loss: 5.1078 - sparse: 0.3564 - perfect: 0.0000e+00 - val_loss: 4.6194 - val_sparse: 0.3989 - val_perfect: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_perfect improved from -inf to 0.00000, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n",
            "Epoch 2/20\n",
            " - 58s - loss: 4.4658 - sparse: 0.4023 - perfect: 0.0000e+00 - val_loss: 4.4915 - val_sparse: 0.4079 - val_perfect: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_perfect did not improve from 0.00000\n",
            "Epoch 3/20\n",
            " - 58s - loss: 4.3071 - sparse: 0.4126 - perfect: 0.0000e+00 - val_loss: 4.2909 - val_sparse: 0.4121 - val_perfect: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_perfect did not improve from 0.00000\n",
            "Epoch 4/20\n",
            " - 58s - loss: 4.0795 - sparse: 0.4220 - perfect: 0.0134 - val_loss: 4.0216 - val_sparse: 0.4273 - val_perfect: 0.0469\n",
            "\n",
            "Epoch 00004: val_perfect improved from 0.00000 to 0.04688, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n",
            "Epoch 5/20\n",
            " - 58s - loss: 3.7959 - sparse: 0.4355 - perfect: 0.0462 - val_loss: 3.7680 - val_sparse: 0.4358 - val_perfect: 0.0557\n",
            "\n",
            "Epoch 00005: val_perfect improved from 0.04688 to 0.05569, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n",
            "Epoch 6/20\n",
            " - 58s - loss: 3.4520 - sparse: 0.4597 - perfect: 0.0506 - val_loss: 3.1626 - val_sparse: 0.4726 - val_perfect: 0.0637\n",
            "\n",
            "Epoch 00006: val_perfect improved from 0.05569 to 0.06370, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n",
            "Epoch 7/20\n",
            " - 58s - loss: 2.8455 - sparse: 0.4856 - perfect: 0.0564 - val_loss: 2.7261 - val_sparse: 0.4999 - val_perfect: 0.0737\n",
            "\n",
            "Epoch 00007: val_perfect improved from 0.06370 to 0.07372, saving model to /content/gdrive/My Drive/Colab Notebooks/haiku_text_5.h5\n",
            "Epoch 8/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-980d5e779d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m             ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n\u001b[1;32m     45\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m           validation_data=[x_test, y_test])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy (dropout={})'.format(dropout))\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTpMmewvKjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  print('x_test.shape ', x_test.shape)\n",
        "  print('y_text.shape ', y_test.shape)\n",
        "  eval_small = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on test data: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "top_k=2\n",
        "\n",
        "try_indx=x_test\n",
        "try_text=input_text[test_i]\n",
        "#try_indx=x_train\n",
        "#try_text=input_text[train_i]\n",
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(try_indx[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          fs = FullSearch(num_sylls * 5, num_sylls, top_k)\n",
        "          (top_vals, top_paths) = get_top_k(predicts[j], top_k=top_k)\n",
        "          fs.mainloop(top_paths)\n",
        "          sentences = decodem(fs.scorepaths, top_paths, decoder, haikuwordset, wordmap)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences, num_sylls):\n",
        "                    print('{} -> {}'.format(try_text[i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSA8FtUUyju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}