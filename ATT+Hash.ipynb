{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATT+Hash.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrnHfTpkale",
        "colab_type": "text"
      },
      "source": [
        "Our attention block plus our text encoder: hashing trick on word stream for input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "6d87bfc3-1901-44ed-9d6a-9c2f862c416c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.4\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "32a86f9e-b3de-40ce-fb6d-db6e1dfbb827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "dcb8e971-6d92-4750-81a1-e2af8cf09b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from search.full import FullSearch\n",
        "from cmu.topk import topk as get_top_k\n",
        "from cmu.wordmap import Wordmap\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# number of words for hashing trick\n",
        "hash_mole = 20000\n",
        "# number of output syllables in short haiku\n",
        "max_features = 16000\n",
        "# longest output sentence\n",
        "max_len = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=True\n",
        "# use long as input\n",
        "use_big_text=False\n",
        "\n",
        "model_base=\"/content/gdrive/My Drive/Colab Notebooks/haiku_hash_5\"\n",
        "model_file=model_base + \".h5\".format(int(time.time()))\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_hash_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "07e0b29e-e417-4210-c957-67f4da0dc412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!date\n",
        "print(word2sylls['door'])\n",
        "#word2sylls = trim_homynyms(word2sylls)\n",
        "print(word2sylls['door'])\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "num_sylls = len(idx2syll)\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul 20 04:58:30 UTC 2019\n",
            "['D AO R']\n",
            "['D AO R']\n",
            "2443 0\n",
            "# features:  15098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPpSGpck_JAv",
        "colab_type": "code",
        "outputId": "62388564-9f94-403c-b4b1-34a540bc0b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        }
      },
      "source": [
        "big_text = []\n",
        "big_hash = []\n",
        "big_haiku = []\n",
        "big_data = []\n",
        "big_data_file = \"haiku_5.txt\"\n",
        "haikuwordset = set()\n",
        "haiku_duped = set()\n",
        "with open(big_data_file) as f:\n",
        "    last_haiku = ''\n",
        "    for line in f.readlines():\n",
        "        _parts = line.strip().split('\\t')\n",
        "        _text = _parts[0]\n",
        "        _haiku = _parts[1]\n",
        "        _sylls = []\n",
        "        _use_input = True\n",
        "        if deduplicate_haiku and _haiku == last_haiku:\n",
        "            #print('fail 1: ', line.strip())\n",
        "            continue\n",
        "        _hashed = list(text.hashing_trick(_text, hash_mole))\n",
        "        if len(_hashed) > max_words:\n",
        "            #print('fail 4: ', line.strip())\n",
        "            use_input = False\n",
        "        _lastidx = -1\n",
        "        _wordseq = []\n",
        "        for word in text.text_to_word_sequence(_haiku):\n",
        "            _word = None\n",
        "            if not word in word2sylls and word[-2:] == \"'s\":\n",
        "                if word[:-2] + 's' in word2sylls:\n",
        "                    _word = word[:-2] + 's'\n",
        "                elif word[:-2] + 'es' in word2sylls:\n",
        "                    _word = word[:-2] + 'es'\n",
        "                if _word:\n",
        "                    #print('merged {} => {}'.format(word, _word))\n",
        "                    word = _word\n",
        "            if word in word2sylls:\n",
        "                haikuwordset.add(word)\n",
        "                for syll in word2sylls[word]:\n",
        "                    _sylls.append(syll)\n",
        "                _thisidx = decoder.word2idx[word]\n",
        "                #print('word {}, idx {}'.format(word, decoder.word2idx[word]))\n",
        "                if _lastidx != -1:\n",
        "                    wordmap.add(_lastidx, _thisidx)\n",
        "                _lastidx = _thisidx\n",
        "            elif len(word) > 1:\n",
        "                print('fail 5: ', word)\n",
        "        if len(_sylls) != 5:\n",
        "            #print('fail 2: ', _sylls)\n",
        "            continue\n",
        "        _data = np.zeros((5), dtype='int32')\n",
        "        for j in range(5):\n",
        "             _data[j] = syll2idx[_sylls[j]]\n",
        "        if use_big_text and _use_input:\n",
        "            _text_hash = np.zeros((max_words), dtype='int32')\n",
        "            for j in range(len(_hashed)):\n",
        "                _text_hash[j] = _hashed[j]\n",
        "            big_text.append(_text)\n",
        "            big_hash.append(_text_hash)\n",
        "            big_haiku.append(_haiku)\n",
        "            big_data.append(_data)\n",
        "        if duplicate_haiku and not _haiku in haiku_duped:\n",
        "            haiku_duped.add(_haiku)\n",
        "            _haiku_hash = np.zeros((max_words), dtype='int32')\n",
        "            j = 0\n",
        "            for h in text.hashing_trick(_haiku, hash_mole):\n",
        "                _haiku_hash[j] = h\n",
        "                j += 1\n",
        "            big_text.append(_haiku)\n",
        "            big_hash.append(_haiku_hash)\n",
        "            big_haiku.append(_haiku)\n",
        "            big_data.append(_data)\n",
        "        last_haiku = _haiku\n",
        "        if len(big_text) == max_data:\n",
        "            print('fail 3: ', line)\n",
        "            break\n",
        "\n",
        "big_text = np.array(big_text)\n",
        "big_hash = np.array(big_hash)\n",
        "big_haiku = np.array(big_haiku)\n",
        "big_data = np.array(big_data)\n",
        "big_data = np.expand_dims(big_data, -1)\n",
        "print('{}/{} -> {} : {}'.format(big_text[0], big_hash[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())\n",
        "print('Haiku duped: ', len(haiku_duped))\n",
        "haiku_duped = None"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fail 5:  caveman's\n",
            "fail 5:  caveman's\n",
            "fail 5:  caveman's\n",
            "fail 5:  smitty's\n",
            "fail 5:  smitty's\n",
            "fail 5:  smitty's\n",
            "fail 5:  frisbee's\n",
            "fail 5:  frisbee's\n",
            "fail 5:  frisbee's\n",
            "fail 5:  pony's\n",
            "fail 5:  pony's\n",
            "fail 5:  pony's\n",
            "fail 5:  mouse's\n",
            "fail 5:  mouse's\n",
            "fail 5:  mouse's\n",
            "fail 5:  tv's\n",
            "fail 5:  tv's\n",
            "fail 5:  tv's\n",
            "fail 5:  shelf's\n",
            "fail 5:  shelf's\n",
            "fail 5:  shelf's\n",
            "fail 5:  tv's\n",
            "fail 5:  tv's\n",
            "fail 5:  tv's\n",
            "fail 5:  shelf's\n",
            "fail 5:  shelf's\n",
            "fail 5:  shelf's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  tomasino's\n",
            "fail 5:  emu's\n",
            "fail 5:  emu's\n",
            "a white sink and door/[13098 19387 18514  1381 13206     0     0     0     0     0] -> a white sink and door : [[  156]\n",
            " [14238]\n",
            " [10115]\n",
            " [  125]\n",
            " [ 1844]]\n",
            "Full length clauses:  94563\n",
            "Wordmap total entries:  76410\n",
            "Wordmap length:  229463\n",
            "Haiku duped:  94563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "f8704e3f-54af-4fe7-bb28-29b1c88f7628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "# Split multiple datasets\n",
        "(x_train_i, x_test_i, y_train_i, y_test_i) = train_test_split(np.arange(len(big_data)), np.arange(len(big_data)))\n",
        "\n",
        "train_len=(len(x_train_i)//batch_size) * batch_size\n",
        "test_len=(len(x_test_i)//batch_size) * batch_size\n",
        "x_train = big_hash[x_train_i][:train_len]\n",
        "y_train = big_data[x_train_i][:train_len]\n",
        "x_test = big_hash[x_test_i][-test_len:]\n",
        "y_test = big_data[x_test_i][-test_len:]\n",
        "\n",
        "print(big_text[x_train_i[0]], x_test[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to all of her friends [7677 5327 5666    0    0    0    0    0    0    0] [[13020]\n",
            " [13726]\n",
            " [ 5000]\n",
            " [ 1873]\n",
            " [10829]]\n",
            "x_train shape: (70912, 10)\n",
            "x_test shape: (23616, 10)\n",
            "y_train shape: (70912, 5, 1)\n",
            "y_test shape: (23616, 5, 1)\n",
            "[13020]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YyrrjKwTDhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/philipperemy/keras-snail-attention/blob/master/attention.py\n",
        "# Do these Dense layers need activation tanh?\n",
        "# https://www.d2l.ai/chapter_attention-mechanism/attention.html\n",
        "# k, q have attention, v does not?\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "    def __init__(self, dims, k_size, v_size, seq_len=None, **kwargs):\n",
        "        self.k_size = k_size\n",
        "        self.seq_len = seq_len\n",
        "        self.v_size = v_size\n",
        "        self.dims = dims\n",
        "        self.sqrt_k = math.sqrt(k_size)\n",
        "        self.keys_fc = None\n",
        "        self.queries_fc = None\n",
        "        self.values_fc = None\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # https://stackoverflow.com/questions/54194724/how-to-use-keras-layers-in-custom-keras-layer\n",
        "        self.keys_fc = layers.Dense(self.k_size)\n",
        "        self.keys_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.keys_fc.trainable_weights)\n",
        "\n",
        "        self.queries_fc = layers.Dense(self.k_size)\n",
        "        self.queries_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.queries_fc.trainable_weights)\n",
        "\n",
        "        self.values_fc = layers.Dense(self.v_size)\n",
        "        self.values_fc.build((None, self.dims))\n",
        "        self._trainable_weights.extend(self.values_fc.trainable_weights)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        # check that the implementation matches exactly py torch.\n",
        "        keys = self.keys_fc(inputs)\n",
        "        queries = self.queries_fc(inputs)\n",
        "        values = self.values_fc(inputs)\n",
        "        logits = K.batch_dot(queries, K.permute_dimensions(keys, (0, 2, 1)))\n",
        "        mask = K.ones_like(logits) * np.triu((-np.inf) * np.ones(logits.shape.as_list()[1:]), k=1)\n",
        "        logits = mask + logits\n",
        "        probs = layers.Softmax(axis=-1)(logits / self.sqrt_k)\n",
        "        read = K.batch_dot(probs, values)\n",
        "        output = K.concatenate([inputs, read], axis=-1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[-1] += self.v_size\n",
        "        return tuple(output_shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "aca89d9e-f288-46ad-b079-49b869119eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def sparse_categorical_accuracy(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "def sparse_categorical_accuracy_per_sequence(y_true, y_pred):\n",
        "    # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "    if K.ndim(y_true) == K.ndim(y_pred):\n",
        "        y_true = K.squeeze(y_true, -1)\n",
        "    # convert dense predictions to labels\n",
        "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "    y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "    return K.min(K.cast(K.equal(y_true, y_pred_labels), K.floatx()), axis=-1)\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    return K.reshape(top_k, original_shape[:-1])\n",
        "\n",
        "def sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5):\n",
        "    original_shape = K.shape(y_true)\n",
        "    y_true = K.reshape(y_true, (-1, K.shape(y_true)[-1]))\n",
        "    y_pred = K.reshape(y_pred, (-1, K.shape(y_pred)[-1]))\n",
        "    top_k = K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k)\n",
        "    perfect = K.min(K.cast(top_k, 'int32'), axis=-1)\n",
        "    return perfect #K.expand_dims(perfect, axis=-1)\n",
        "\n",
        "def sparse(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy(y_true, y_pred)\n",
        "def sparse1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
        "def perfect(y_true, y_pred):\n",
        "    return sparse_categorical_accuracy_per_sequence(y_true, y_pred)\n",
        "def perfect1(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=1)\n",
        "def sparse5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
        "def perfect5(y_true, y_pred):\n",
        "    return sparse_temporal_top_k_categorical_accuracy_per_sequence(y_true, y_pred, k=5)\n",
        "def fscore(y_true, y_pred):\n",
        "    recall = K.mean(sparse_categorical_accuracy(y_true, y_pred))\n",
        "    precision = K.mean(sparse_categorical_accuracy_per_sequence(y_true, y_pred))\n",
        "    return 2 * ((recall * precision)/(recall + precision))\n",
        "\n",
        "def sparse_loss(y_true, y_pred):\n",
        "    return scc(y_true, y_pred)\n",
        "\n",
        "def perfect_loss(y_true, y_pred):\n",
        "    return scct(y_true, y_pred, scale=1.0)\n",
        "\n",
        "embed_size=512\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.5\n",
        "\n",
        "metric_list = [sparse, perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "hash_input = layers.Input(shape=(max_words,), dtype='int32')\n",
        "x = layers.Embedding(hash_mole, embed_size, input_length=max_words)(hash_input)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "#x = layers.Dense(units, activation='relu')(x)\n",
        "#x = layers.RepeatVector(max_words)(x)\n",
        "x = layers.Bidirectional(get_lstm(units//2, return_sequences=False))(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_medium, activation='relu')(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(unit_small, activation='relu')(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.RepeatVector(max_len)(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "x = layers.RepeatVector(max_len)(x)\n",
        "if False:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "x = get_lstm(units, return_sequences=True)(x)\n",
        "if True:\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = AttentionBlock(embed_size, k_size=units_k, v_size=units_v)(x)\n",
        "x = layers.Dropout(dropout)(x)\n",
        "output_layer = layers.Dense(max_features, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=[hash_input], outputs=output_layer)\n",
        "model.compile('adam', sparse_loss, metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    model_file=model_base + \".h5\".format(int(time.time()))\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=300,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=0)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 04:58:57.788864 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 04:58:57.805070 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 04:58:57.809039 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0720 04:58:57.822956 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0720 04:58:57.831277 139657408362368 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0720 04:58:59.784136 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0720 04:58:59.809601 139657408362368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras_stuff/loss.py:58: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 512)           10240000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_1 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3410)              0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 5, 3410)           0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 3410)           0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5, 512)            8036352   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "attention_block_2 (Attention (None, 5, 682)            612522    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 5, 682)            0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5, 16000)          10928000  \n",
            "=================================================================\n",
            "Total params: 32,006,356\n",
            "Trainable params: 32,006,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0720 04:59:00.771582 139657408362368 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 70912 samples, validate on 23616 samples\n",
            "Epoch 1/300\n",
            " - 106s - loss: 4.5393 - sparse: 0.2777 - perfect: 0.0014 - val_loss: 2.9253 - val_sparse: 0.4906 - val_perfect: 0.0354\n",
            "Epoch 2/300\n",
            " - 103s - loss: 2.9939 - sparse: 0.4549 - perfect: 0.0231 - val_loss: 1.9518 - val_sparse: 0.6528 - val_perfect: 0.1751\n",
            "Epoch 3/300\n",
            " - 103s - loss: 2.3563 - sparse: 0.5457 - perfect: 0.0677 - val_loss: 1.4566 - val_sparse: 0.7395 - val_perfect: 0.3189\n",
            "Epoch 4/300\n",
            " - 103s - loss: 1.9902 - sparse: 0.6028 - perfect: 0.1152 - val_loss: 1.2088 - val_sparse: 0.7888 - val_perfect: 0.4212\n",
            "Epoch 5/300\n",
            " - 104s - loss: 1.7402 - sparse: 0.6449 - perfect: 0.1598 - val_loss: 1.0203 - val_sparse: 0.8226 - val_perfect: 0.4962\n",
            "Epoch 6/300\n",
            " - 103s - loss: 1.5591 - sparse: 0.6756 - perfect: 0.1986 - val_loss: 0.9006 - val_sparse: 0.8444 - val_perfect: 0.5532\n",
            "Epoch 7/300\n",
            " - 103s - loss: 1.4250 - sparse: 0.6996 - perfect: 0.2299 - val_loss: 0.8300 - val_sparse: 0.8595 - val_perfect: 0.5918\n",
            "Epoch 8/300\n",
            " - 103s - loss: 1.3225 - sparse: 0.7190 - perfect: 0.2610 - val_loss: 0.7611 - val_sparse: 0.8725 - val_perfect: 0.6260\n",
            "Epoch 9/300\n",
            " - 103s - loss: 1.2399 - sparse: 0.7341 - perfect: 0.2837 - val_loss: 0.7158 - val_sparse: 0.8796 - val_perfect: 0.6444\n",
            "Epoch 10/300\n",
            " - 104s - loss: 1.1716 - sparse: 0.7473 - perfect: 0.3068 - val_loss: 0.6788 - val_sparse: 0.8882 - val_perfect: 0.6697\n",
            "Epoch 11/300\n",
            " - 103s - loss: 1.1141 - sparse: 0.7568 - perfect: 0.3243 - val_loss: 0.6384 - val_sparse: 0.8944 - val_perfect: 0.6878\n",
            "Epoch 12/300\n",
            " - 103s - loss: 1.0668 - sparse: 0.7672 - perfect: 0.3439 - val_loss: 0.6159 - val_sparse: 0.8989 - val_perfect: 0.6990\n",
            "Epoch 13/300\n",
            " - 103s - loss: 1.0224 - sparse: 0.7759 - perfect: 0.3609 - val_loss: 0.5879 - val_sparse: 0.9039 - val_perfect: 0.7176\n",
            "Epoch 14/300\n",
            " - 103s - loss: 0.9831 - sparse: 0.7829 - perfect: 0.3751 - val_loss: 0.5720 - val_sparse: 0.9070 - val_perfect: 0.7253\n",
            "Epoch 15/300\n",
            " - 103s - loss: 0.9490 - sparse: 0.7899 - perfect: 0.3864 - val_loss: 0.5558 - val_sparse: 0.9099 - val_perfect: 0.7315\n",
            "Epoch 16/300\n",
            " - 103s - loss: 0.9180 - sparse: 0.7969 - perfect: 0.3997 - val_loss: 0.5378 - val_sparse: 0.9132 - val_perfect: 0.7414\n",
            "Epoch 17/300\n",
            " - 103s - loss: 0.8924 - sparse: 0.8004 - perfect: 0.4088 - val_loss: 0.5216 - val_sparse: 0.9170 - val_perfect: 0.7507\n",
            "Epoch 18/300\n",
            " - 103s - loss: 0.8676 - sparse: 0.8062 - perfect: 0.4187 - val_loss: 0.5131 - val_sparse: 0.9185 - val_perfect: 0.7577\n",
            "Epoch 19/300\n",
            " - 103s - loss: 0.8464 - sparse: 0.8106 - perfect: 0.4304 - val_loss: 0.5062 - val_sparse: 0.9194 - val_perfect: 0.7613\n",
            "Epoch 20/300\n",
            " - 102s - loss: 0.8241 - sparse: 0.8151 - perfect: 0.4398 - val_loss: 0.4966 - val_sparse: 0.9219 - val_perfect: 0.7689\n",
            "Epoch 21/300\n",
            " - 102s - loss: 0.8054 - sparse: 0.8182 - perfect: 0.4456 - val_loss: 0.4829 - val_sparse: 0.9238 - val_perfect: 0.7750\n",
            "Epoch 22/300\n",
            " - 102s - loss: 0.7867 - sparse: 0.8222 - perfect: 0.4555 - val_loss: 0.4745 - val_sparse: 0.9255 - val_perfect: 0.7791\n",
            "Epoch 23/300\n",
            " - 103s - loss: 0.7775 - sparse: 0.8247 - perfect: 0.4601 - val_loss: 0.4684 - val_sparse: 0.9273 - val_perfect: 0.7840\n",
            "Epoch 24/300\n",
            " - 103s - loss: 0.7518 - sparse: 0.8296 - perfect: 0.4711 - val_loss: 0.4662 - val_sparse: 0.9278 - val_perfect: 0.7886\n",
            "Epoch 25/300\n",
            " - 103s - loss: 0.7409 - sparse: 0.8320 - perfect: 0.4766 - val_loss: 0.4557 - val_sparse: 0.9290 - val_perfect: 0.7910\n",
            "Epoch 26/300\n",
            " - 103s - loss: 0.7293 - sparse: 0.8336 - perfect: 0.4800 - val_loss: 0.4547 - val_sparse: 0.9295 - val_perfect: 0.7926\n",
            "Epoch 27/300\n",
            " - 103s - loss: 0.7156 - sparse: 0.8367 - perfect: 0.4861 - val_loss: 0.4469 - val_sparse: 0.9318 - val_perfect: 0.8012\n",
            "Epoch 28/300\n",
            " - 103s - loss: 0.7050 - sparse: 0.8386 - perfect: 0.4902 - val_loss: 0.4470 - val_sparse: 0.9316 - val_perfect: 0.7998\n",
            "Epoch 29/300\n",
            " - 103s - loss: 0.6947 - sparse: 0.8411 - perfect: 0.4984 - val_loss: 0.4396 - val_sparse: 0.9329 - val_perfect: 0.8025\n",
            "Epoch 30/300\n",
            " - 103s - loss: 0.6836 - sparse: 0.8433 - perfect: 0.5021 - val_loss: 0.4349 - val_sparse: 0.9340 - val_perfect: 0.8054\n",
            "Epoch 31/300\n",
            " - 103s - loss: 0.6728 - sparse: 0.8457 - perfect: 0.5062 - val_loss: 0.4306 - val_sparse: 0.9349 - val_perfect: 0.8103\n",
            "Epoch 32/300\n",
            " - 103s - loss: 0.6650 - sparse: 0.8471 - perfect: 0.5113 - val_loss: 0.4264 - val_sparse: 0.9357 - val_perfect: 0.8130\n",
            "Epoch 33/300\n",
            " - 103s - loss: 0.6558 - sparse: 0.8489 - perfect: 0.5159 - val_loss: 0.4252 - val_sparse: 0.9356 - val_perfect: 0.8122\n",
            "Epoch 34/300\n",
            " - 103s - loss: 0.6481 - sparse: 0.8510 - perfect: 0.5199 - val_loss: 0.4283 - val_sparse: 0.9356 - val_perfect: 0.8124\n",
            "Epoch 35/300\n",
            " - 102s - loss: 0.6428 - sparse: 0.8519 - perfect: 0.5220 - val_loss: 0.4202 - val_sparse: 0.9360 - val_perfect: 0.8132\n",
            "Epoch 36/300\n",
            " - 103s - loss: 0.6349 - sparse: 0.8542 - perfect: 0.5282 - val_loss: 0.4207 - val_sparse: 0.9361 - val_perfect: 0.8155\n",
            "Epoch 37/300\n",
            " - 103s - loss: 0.6274 - sparse: 0.8553 - perfect: 0.5315 - val_loss: 0.4192 - val_sparse: 0.9376 - val_perfect: 0.8200\n",
            "Epoch 38/300\n",
            " - 102s - loss: 0.6174 - sparse: 0.8578 - perfect: 0.5385 - val_loss: 0.4152 - val_sparse: 0.9376 - val_perfect: 0.8198\n",
            "Epoch 39/300\n",
            " - 102s - loss: 0.6108 - sparse: 0.8588 - perfect: 0.5386 - val_loss: 0.4165 - val_sparse: 0.9373 - val_perfect: 0.8194\n",
            "Epoch 40/300\n",
            " - 102s - loss: 0.6096 - sparse: 0.8592 - perfect: 0.5415 - val_loss: 0.4125 - val_sparse: 0.9380 - val_perfect: 0.8228\n",
            "Epoch 41/300\n",
            " - 102s - loss: 0.6032 - sparse: 0.8608 - perfect: 0.5464 - val_loss: 0.4112 - val_sparse: 0.9384 - val_perfect: 0.8233\n",
            "Epoch 42/300\n",
            " - 102s - loss: 0.6216 - sparse: 0.8601 - perfect: 0.5466 - val_loss: 0.4101 - val_sparse: 0.9389 - val_perfect: 0.8245\n",
            "Epoch 43/300\n",
            " - 102s - loss: 0.5948 - sparse: 0.8621 - perfect: 0.5480 - val_loss: 0.4078 - val_sparse: 0.9395 - val_perfect: 0.8267\n",
            "Epoch 44/300\n",
            " - 103s - loss: 0.5776 - sparse: 0.8656 - perfect: 0.5558 - val_loss: 0.4054 - val_sparse: 0.9398 - val_perfect: 0.8280\n",
            "Epoch 45/300\n",
            " - 103s - loss: 0.5775 - sparse: 0.8661 - perfect: 0.5580 - val_loss: 0.4033 - val_sparse: 0.9403 - val_perfect: 0.8288\n",
            "Epoch 46/300\n",
            " - 102s - loss: 0.5738 - sparse: 0.8673 - perfect: 0.5597 - val_loss: 0.4015 - val_sparse: 0.9401 - val_perfect: 0.8288\n",
            "Epoch 47/300\n",
            " - 103s - loss: 0.5684 - sparse: 0.8679 - perfect: 0.5622 - val_loss: 0.4027 - val_sparse: 0.9399 - val_perfect: 0.8280\n",
            "Epoch 48/300\n",
            " - 103s - loss: 0.5700 - sparse: 0.8675 - perfect: 0.5639 - val_loss: 0.4007 - val_sparse: 0.9406 - val_perfect: 0.8297\n",
            "Epoch 49/300\n",
            " - 103s - loss: 0.5603 - sparse: 0.8703 - perfect: 0.5686 - val_loss: 0.4010 - val_sparse: 0.9405 - val_perfect: 0.8311\n",
            "Epoch 50/300\n",
            " - 103s - loss: 0.5585 - sparse: 0.8706 - perfect: 0.5707 - val_loss: 0.3994 - val_sparse: 0.9410 - val_perfect: 0.8311\n",
            "Epoch 51/300\n",
            " - 103s - loss: 0.5554 - sparse: 0.8709 - perfect: 0.5710 - val_loss: 0.3963 - val_sparse: 0.9418 - val_perfect: 0.8339\n",
            "Epoch 52/300\n",
            " - 103s - loss: 0.5480 - sparse: 0.8721 - perfect: 0.5734 - val_loss: 0.3956 - val_sparse: 0.9418 - val_perfect: 0.8344\n",
            "Epoch 53/300\n",
            " - 102s - loss: 0.5457 - sparse: 0.8725 - perfect: 0.5746 - val_loss: 0.3984 - val_sparse: 0.9416 - val_perfect: 0.8341\n",
            "Epoch 54/300\n",
            " - 102s - loss: 0.5412 - sparse: 0.8742 - perfect: 0.5789 - val_loss: 0.3941 - val_sparse: 0.9426 - val_perfect: 0.8352\n",
            "Epoch 55/300\n",
            " - 103s - loss: 0.5383 - sparse: 0.8750 - perfect: 0.5804 - val_loss: 0.3945 - val_sparse: 0.9422 - val_perfect: 0.8360\n",
            "Epoch 56/300\n",
            " - 102s - loss: 0.5377 - sparse: 0.8744 - perfect: 0.5783 - val_loss: 0.3921 - val_sparse: 0.9428 - val_perfect: 0.8386\n",
            "Epoch 57/300\n",
            " - 102s - loss: 0.5301 - sparse: 0.8767 - perfect: 0.5840 - val_loss: 0.3924 - val_sparse: 0.9429 - val_perfect: 0.8386\n",
            "Epoch 58/300\n",
            " - 103s - loss: 0.5297 - sparse: 0.8762 - perfect: 0.5849 - val_loss: 0.3926 - val_sparse: 0.9427 - val_perfect: 0.8388\n",
            "Epoch 59/300\n",
            " - 102s - loss: 0.5228 - sparse: 0.8784 - perfect: 0.5886 - val_loss: 0.3905 - val_sparse: 0.9428 - val_perfect: 0.8394\n",
            "Epoch 60/300\n",
            " - 102s - loss: 0.5200 - sparse: 0.8786 - perfect: 0.5900 - val_loss: 0.3894 - val_sparse: 0.9429 - val_perfect: 0.8393\n",
            "Epoch 61/300\n",
            " - 102s - loss: 0.5213 - sparse: 0.8787 - perfect: 0.5920 - val_loss: 0.3901 - val_sparse: 0.9433 - val_perfect: 0.8407\n",
            "Epoch 62/300\n",
            " - 103s - loss: 0.5133 - sparse: 0.8797 - perfect: 0.5940 - val_loss: 0.3890 - val_sparse: 0.9436 - val_perfect: 0.8409\n",
            "Epoch 63/300\n",
            " - 103s - loss: 0.5143 - sparse: 0.8803 - perfect: 0.5941 - val_loss: 0.3911 - val_sparse: 0.9429 - val_perfect: 0.8411\n",
            "Epoch 64/300\n",
            " - 102s - loss: 0.5130 - sparse: 0.8804 - perfect: 0.5949 - val_loss: 0.3893 - val_sparse: 0.9435 - val_perfect: 0.8415\n",
            "Epoch 65/300\n",
            " - 103s - loss: 0.5078 - sparse: 0.8810 - perfect: 0.5952 - val_loss: 0.3910 - val_sparse: 0.9431 - val_perfect: 0.8393\n",
            "Epoch 66/300\n",
            " - 103s - loss: 0.5038 - sparse: 0.8822 - perfect: 0.5995 - val_loss: 0.3892 - val_sparse: 0.9436 - val_perfect: 0.8434\n",
            "Epoch 67/300\n",
            " - 103s - loss: 0.5028 - sparse: 0.8828 - perfect: 0.6002 - val_loss: 0.3847 - val_sparse: 0.9444 - val_perfect: 0.8425\n",
            "Epoch 68/300\n",
            " - 103s - loss: 0.4982 - sparse: 0.8833 - perfect: 0.6043 - val_loss: 0.3906 - val_sparse: 0.9435 - val_perfect: 0.8438\n",
            "Epoch 69/300\n",
            " - 103s - loss: 0.4985 - sparse: 0.8833 - perfect: 0.6039 - val_loss: 0.3857 - val_sparse: 0.9440 - val_perfect: 0.8424\n",
            "Epoch 70/300\n",
            " - 103s - loss: 0.4964 - sparse: 0.8840 - perfect: 0.6034 - val_loss: 0.3865 - val_sparse: 0.9441 - val_perfect: 0.8431\n",
            "Epoch 71/300\n",
            " - 102s - loss: 0.4967 - sparse: 0.8841 - perfect: 0.6030 - val_loss: 0.3877 - val_sparse: 0.9445 - val_perfect: 0.8447\n",
            "Epoch 72/300\n",
            " - 103s - loss: 0.4927 - sparse: 0.8851 - perfect: 0.6087 - val_loss: 0.3856 - val_sparse: 0.9442 - val_perfect: 0.8437\n",
            "Epoch 73/300\n",
            " - 103s - loss: 0.4886 - sparse: 0.8857 - perfect: 0.6096 - val_loss: 0.3832 - val_sparse: 0.9445 - val_perfect: 0.8440\n",
            "Epoch 74/300\n",
            " - 102s - loss: 0.4885 - sparse: 0.8852 - perfect: 0.6087 - val_loss: 0.3845 - val_sparse: 0.9442 - val_perfect: 0.8439\n",
            "Epoch 75/300\n",
            " - 103s - loss: 0.4855 - sparse: 0.8860 - perfect: 0.6109 - val_loss: 0.3838 - val_sparse: 0.9452 - val_perfect: 0.8473\n",
            "Epoch 76/300\n",
            " - 103s - loss: 0.4839 - sparse: 0.8870 - perfect: 0.6132 - val_loss: 0.3856 - val_sparse: 0.9444 - val_perfect: 0.8465\n",
            "Epoch 77/300\n",
            " - 102s - loss: 0.4806 - sparse: 0.8870 - perfect: 0.6132 - val_loss: 0.3831 - val_sparse: 0.9450 - val_perfect: 0.8478\n",
            "Epoch 78/300\n",
            " - 102s - loss: 2.0018 - sparse: 0.7988 - perfect: 0.5438 - val_loss: 12.3754 - val_sparse: 0.2036 - val_perfect: 0.0000e+00\n",
            "Epoch 79/300\n",
            " - 102s - loss: 6.1564 - sparse: 0.5200 - perfect: 0.2213 - val_loss: 0.4251 - val_sparse: 0.9372 - val_perfect: 0.8194\n",
            "Epoch 80/300\n",
            " - 102s - loss: 0.6079 - sparse: 0.8602 - perfect: 0.5410 - val_loss: 0.3987 - val_sparse: 0.9427 - val_perfect: 0.8386\n",
            "Epoch 81/300\n",
            " - 102s - loss: 0.5280 - sparse: 0.8779 - perfect: 0.5881 - val_loss: 0.3917 - val_sparse: 0.9438 - val_perfect: 0.8429\n",
            "Epoch 82/300\n",
            " - 103s - loss: 0.4962 - sparse: 0.8843 - perfect: 0.6062 - val_loss: 0.3886 - val_sparse: 0.9443 - val_perfect: 0.8435\n",
            "Epoch 83/300\n",
            " - 102s - loss: 0.4875 - sparse: 0.8865 - perfect: 0.6106 - val_loss: 0.3872 - val_sparse: 0.9444 - val_perfect: 0.8452\n",
            "Epoch 84/300\n",
            " - 102s - loss: 0.4862 - sparse: 0.8865 - perfect: 0.6118 - val_loss: 0.3838 - val_sparse: 0.9452 - val_perfect: 0.8467\n",
            "Epoch 85/300\n",
            " - 103s - loss: 0.4789 - sparse: 0.8883 - perfect: 0.6166 - val_loss: 0.3871 - val_sparse: 0.9441 - val_perfect: 0.8453\n",
            "Epoch 86/300\n",
            " - 103s - loss: 0.4856 - sparse: 0.8869 - perfect: 0.6132 - val_loss: 0.3848 - val_sparse: 0.9448 - val_perfect: 0.8455\n",
            "Epoch 87/300\n",
            " - 102s - loss: 0.4701 - sparse: 0.8898 - perfect: 0.6204 - val_loss: 0.3796 - val_sparse: 0.9455 - val_perfect: 0.8473\n",
            "Epoch 00087: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "dee5ee1d-4fa2-4bb1-c989-0fc7801fcfb4"
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy (dropout={})'.format(dropout))\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEWCAYAAACdaNcBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTpMmewvKjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "fc15d5d7-f4e9-4541-c4ce-fc69f964cdfc"
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on test data: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  288/23616 [..............................] - ETA: 27s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f03ccecbba8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1473, in __del__\n",
            "    self._session._session, self._handle)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: (None, None, 'Session has been closed.')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23616/23616 [==============================] - 7s 309us/step\n",
            "model.evaluate on test data:  ['loss', 'sparse', 'perfect'] [0.38310739803342475, 0.944977980803668, 0.8477726964769647]\n",
            "history:  <keras.callbacks.History object at 0x7f043089c978>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBZfxMLgpRA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k=5\n",
        "\n",
        "fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "def decodem(predict, top_k):\n",
        "    global fs\n",
        "    (top_vals, top_paths) = get_top_k(np.array([predict]), top_k=top_k)\n",
        "    #print('top_paths: ' + str(top_paths))\n",
        "    #print(top_paths.shape)\n",
        "    fs.mainloop(top_paths[0])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print('scorepaths[{}]: {}'.format(j, fs.scorepaths[j]))\n",
        "        #print('predict.shape: ', predict.shape)\n",
        "        #print('top_paths.shape: ', top_paths.shape)\n",
        "        #print('top_paths[{}]: {}'.format(j, top_paths))\n",
        "        #print('top_paths[{}][]: {}'.format(j, top_paths[0][np.arange(max_len), fs.scorepaths[j]]))\n",
        "        morepaths[j] = top_paths[0][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print('morepaths: ' + str(morepaths))\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    sentences = {}\n",
        "    if len(encoded) > 0:\n",
        "        #print(encoded)\n",
        "        decoded = []\n",
        "        for e1 in encoded:\n",
        "            if len(e1) > 0 and len(e1[0]) > 0:\n",
        "                dec = decoder.decode_sentences([e1])\n",
        "                decoded.append(dec)\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    for d4 in d3:\n",
        "                        go = True\n",
        "                        _lastidx = -1\n",
        "                        for w in d4:\n",
        "                            if not w in haikuwordset:\n",
        "                                go = False\n",
        "                            _idx = decoder.word2idx[w]\n",
        "                            if _lastidx > 0:\n",
        "                                if _lastidx == _idx or not wordmap.get(_lastidx, _idx):\n",
        "                                    go = False\n",
        "                                    #print('Fail: {},{} {},{}'.format(_lastidx, _idx, _lastword, w))\n",
        "                            _lastidx = _idx\n",
        "                            _lastword = w\n",
        "                        if go:\n",
        "                            key = ' '.join(d4)\n",
        "                            sentences[key] = d4\n",
        "                    #print('d3: ', d3)\n",
        "                    #key = ' '.join(d3)\n",
        "                    #sentences[key] = d3\n",
        "    return sentences\n",
        "\n",
        "# return N possible sentences with the fewest words\n",
        "def short_sentences(sentences):\n",
        "    out = {}\n",
        "    for i in range(1, max_len + 1):\n",
        "        for (k, v) in sentences.items():\n",
        "            if len(v) == i:\n",
        "                out[k] = v\n",
        "        if len(out) > 4:\n",
        "            return out\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "3d18e9c7-9e6b-48bb-d898-630a9529ecff"
      },
      "source": [
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(x_train[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          sentences = decodem(predicts[j], 5)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences):\n",
        "                    print('{} -> {}'.format(big_text[x_train_i][i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "to all of her friends -> with some in the straw\n",
            "to all of her friends -> with plants in the straw\n",
            "to all of her friends -> to some in the straw\n",
            "to all of her friends -> to rose in the straw\n",
            "to all of her friends -> to plants in the straw\n",
            "to all of her friends -> two plants in the straw\n",
            "to all of her friends -> with some in the pan\n",
            "the green city bus -> sil tall city stop\n",
            "the green city bus -> sill tall city stop\n",
            "the green city bus -> sil old city stop\n",
            "the green city bus -> sill old city stop\n",
            "the large, old steam train -> the small red tile work\n",
            "the large, old steam train -> a small red tile work\n",
            "the large, old steam train -> an small red tile work\n",
            "the large, old steam train -> the old red tile work\n",
            "the large, old steam train -> on small red tile work\n",
            "the large, old steam train -> the small red steam train\n",
            "a blonde haired woman -> with blond young woman\n",
            "a blonde haired woman -> with blond young women\n",
            "on to of a field -> into two long sink\n",
            "on to of a field -> into two the snow\n",
            "on to of a field -> into two the sink\n",
            "on to of a field -> onto two long sink\n",
            "on to of a field -> into two the lake\n",
            "on to of a field -> onto two the snow\n",
            "on to of a field -> onto two the sink\n",
            "next to a bush bench -> stands with two long truck\n",
            "next to a bush bench -> stands to two long truck\n",
            "next to a bush bench -> stands in to wood stand\n",
            "sits on a brick wall -> snow in large tall wall\n",
            "sits on a brick wall -> sits in large tall wall\n",
            "sits on a brick wall -> top of large tall wall\n",
            "sits on a brick wall -> top and large tall wall\n",
            "sits on a brick wall -> top on large tall wall\n",
            "sits on a brick wall -> snow in the tall wall\n",
            "sits on a brick wall -> snow of large tall wall\n",
            "sits on a brick wall -> snow and large tall wall\n",
            "sits on a brick wall -> sits in the tall wall\n",
            "sits on a brick wall -> snow on large tall wall\n",
            "sits on a brick wall -> sits of large tall wall\n",
            "sits on a brick wall -> sits and large tall wall\n",
            "sits on a brick wall -> sits on large tall wall\n",
            "sits on a brick wall -> high in large tall wall\n",
            "sits on a brick wall -> snow in a tall wall\n",
            "sits on a brick wall -> snow in an tall wall\n",
            "sits on a brick wall -> snow in the side wall\n",
            "sits on a brick wall -> sits in a tall wall\n",
            "sits on a brick wall -> sits in an tall wall\n",
            "sits on a brick wall -> top of the tall wall\n",
            "sits on a brick wall -> top and the tall wall\n",
            "sits on a brick wall -> top on the tall wall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0FUopUDJUY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "(mini_vals, mini_preds) = get_top_k(np.array(predicts), top_k=top_k)\n",
        "#print('top preds: ', mini_preds[0])\n",
        "#print(mini_preds.shape)\n",
        "total = 0\n",
        "_go = []\n",
        "for x in mini_preds[0]:\n",
        "            _go.append(decoder.idx2syll[x[0]])\n",
        "print('{} -> {}'.format(x_short[0], str(_go)))\n",
        "for i in range(len(mini_preds)):\n",
        "    fs = FullSearch(top_k * top_k * top_k, 5, top_k)\n",
        "    fs.mainloop(mini_preds[i])\n",
        "    #print('score[0]: {}'.format(fs.scorevals[0]))\n",
        "    #print('paths[0]: {}'.format(fs.scorepaths[0]))\n",
        "    #print('score[-1]: {}'.format(fs.scorevals[-1]))\n",
        "    #print('paths[-1]: {}'.format(fs.scorepaths[-1]))\n",
        "    #print('min {}, max {}'.format(np.min(fs.scorevals), np.max(fs.scorevals)))\n",
        "    print('{} -> {}'.format(x_short[i], [decoder.idx2syll[x] for x in fs.scorepaths[0]]))\n",
        "    morepaths = np.zeros(fs.scorepaths.shape, dtype='int32')\n",
        "    print(mini_preds[i].shape)\n",
        "    print(morepaths.shape)\n",
        "    for j in range(fs.scorepaths.shape[0]):\n",
        "        #print(fs.scorepaths[j])\n",
        "        #z = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "        #print(z)\n",
        "        morepaths[j] = mini_preds[i][np.arange(max_len), fs.scorepaths[j]]\n",
        "    #print(morepaths[0])\n",
        "    encoded = decoder.get_sentences(morepaths)\n",
        "    if i == 0:\n",
        "        print('encoded[0]: ', encoded[0])\n",
        "    #d = []\n",
        "    # for x in encoded:\n",
        "    #   for y in x:\n",
        "    #        if len(y) > 0:\n",
        "    #           d.append(y[0])  \n",
        "    #d = np.array(d)\n",
        "    d = encoded\n",
        "    if len(d) > 0:\n",
        "        print('encoded sentences: ', d)\n",
        "        print(x_short[i], ':')\n",
        "        #print(mini_preds[0])\n",
        "        total += 1\n",
        "        decoded = decoder.decode_sentences(encoded)\n",
        "        #print('len(decoded): ', len(decoded))\n",
        "        ##print('len(decoded[0]): ', len(decoded[0]))\n",
        "        #print('len(decoded[0][0]): ', len(decoded[0][0]))\n",
        "        #print('len(decoded[0][0][0]): ', len(decoded[0][0][0]))\n",
        "        #print('decoded[0][0][0]: ', decoded[0][0][0])\n",
        "        sentences = {}\n",
        "        for d1 in decoded:\n",
        "            for d2 in d1:\n",
        "                for d3 in d2:\n",
        "                    print('d3: ', d3)\n",
        "                    break\n",
        "                    key = ' '.join(d3)\n",
        "                    sentences[key] = d3\n",
        "                    break\n",
        "                     \n",
        "        print('[{}]  -> {}', i,list(sentences.keys())[0:10])\n",
        "    break\n",
        "print('Total decoded: {}'.format(total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unppo_tZk4fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  eval_small = model.evaluate(test_x, test_y)\n",
        "  print('model.evaluate on haiku clauses: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n",
        "  eval_big = model.evaluate(big_text, big_data)\n",
        "  print('model.evaluate on long clauses: ' ,model.metrics_names, eval_big)\n",
        "  print('history: ', history)\n",
        "  biglen = len(big_text)\n",
        "  #for i in range(0, len(big_text), batch_size):\n",
        "  #  predicts = model.predict(big_text[i:i + batch_size], batch_size=batch_size)\n",
        "  #  print('shape: {}'.format(predicts.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYiGf2LOtTbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metric_list = [sparse, perfect, sparse5, perfect5]\n",
        "metric_names = ['sparse', 'perfect', 'sparse5', 'perfect5']\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=metric_list)\n",
        "\n",
        "bigbatch = batch_size * 32\n",
        "big_text = np.array(big_text)\n",
        "big_haiku = np.array(big_haiku)\n",
        "text5arr = []\n",
        "haiku5mean = None\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  predicts = model.predict(big_haiku[0: bigbatch], batch_size=bigbatch)\n",
        "  rep = report(big_data[0: bigbatch], predicts)\n",
        "  print(\"short {}\".format(rep))\n",
        "  haiku5mean = rep['perfect5']\n",
        "  biglen = len(big_text)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(big_text[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(big_data[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    text5arr.append(rep['perfect5'])\n",
        "\n",
        "text5mean = np.mean(np.array(text5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BftiQv1HsRrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val5arr = []\n",
        "with tf.Session() as session:\n",
        "  #K.manual_variable_initialization(True)  \n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model = load_model(model_file)  \n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "    predicts = model.predict(x_test[i:i + bigbatch], batch_size=bigbatch)\n",
        "    rep = report(y_test[i:i + bigbatch], predicts)\n",
        "    print(\"[{}] {}\".format(i, rep))\n",
        "    val5arr.append(rep['perfect5'])\n",
        "\n",
        "val5mean = np.mean(np.array(val5arr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUYe3GyQkPt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Perfect5 for all haiku lines: {}, all mscoco lines: {}, validation mscoco: {}'.format(haiku5mean, text5mean, val5mean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6aZfdAnkCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}