{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USE TCN Deep Meter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTkEiZP3STNJ",
        "colab_type": "text"
      },
      "source": [
        "Use Keras model, use TCN, reuse haiku output from attention tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cclkF0rYcCj8",
        "colab_type": "code",
        "outputId": "0573de1c-fc99-4b46-a775-8c79c94e7329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip install keras-tcn\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.6/dist-packages (2.6.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.16.4)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "721941d7-9b53-4065-829d-e70e2bfe043f"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Nadam, Adam\n",
        "from tcn import TCN"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0613 03:14:36.423183 139994818779008 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vgi-xMRb-Xv",
        "colab_type": "code",
        "outputId": "73e61073-d56b-48ad-aedd-db915a5e5aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!pip uninstall -y deepmeter\n",
        "!pip install git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "from cmu.syllables_cmu import syllables\n",
        "print(syllables['therefore'])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling DeepMeter-0.0.1:\n",
            "  Successfully uninstalled DeepMeter-0.0.1\n",
            "Collecting deepmeter from git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
            "  Cloning https://github.com/LanceNorskog/deep_meter_2 to /tmp/pip-install-7e9r2gha/deepmeter\n",
            "  Running command git clone -q https://github.com/LanceNorskog/deep_meter_2 /tmp/pip-install-7e9r2gha/deepmeter\n",
            "Building wheels for collected packages: deepmeter\n",
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fg6p_zfw/wheels/80/6c/4f/1c43367a928a82b45cb3d36b4f23720b249748714869e28f13\n",
            "Successfully built deepmeter\n",
            "Installing collected packages: deepmeter\n",
            "Successfully installed deepmeter-0.0.1\n",
            "['DH EH R', 'F AO R']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLqizwZYZ3Cs",
        "colab_type": "code",
        "outputId": "a02160e8-766c-49cf-f337-fd613fbf09e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Mine\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/test_data/master/haiku_5.txt\n",
        "path_to_file = 'haiku_5.txt'\n",
        "!mkdir -p training_checkpoints\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "492dX33PaBYW",
        "colab_type": "text"
      },
      "source": [
        "Load Haiku data. Filter for bogus inputs. Create 2-directional map of known syllables, and matching arrays of ['A sentence for you',...] and syllables indexes of [['AH', 'S EH N', 'T EH NS', 'F OR', 'YU']]. Use CMU Pronunciation Dictionary for syllables (some guy's syllabized version)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5__pr1y6YJxu",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hh52qpUdYJV9",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [Long clause, Haiku line]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9xbqO7Iie9bb",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for word in self.lang:\n",
        "        #print(type(word))\n",
        "        #print(word)\n",
        "        if type(word) == type(''):\n",
        "            #if not word in self.vocab:\n",
        "             #   print('. adding: {}'.format(word))\n",
        "            #print(word)\n",
        "            self.vocab.update([word])\n",
        "        else:\n",
        "            for part in word:\n",
        "               #if not part in self.vocab:\n",
        "               #    print('. adding: {}'.format(part))\n",
        "               self.vocab.update([part])\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      #if index < 20:\n",
        "      #    print('Vocab[{}]: \"{}\"'.format(index, word))\n",
        "      if word in ['<start>', 'B AE K', 'big']:\n",
        "           print(\"Adding syllable: {} as index {}\".format(word, index))\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "    print(type(pairs))\n",
        "    print(len(pairs))\n",
        "    print(len(pairs[0]))\n",
        "    print(len(pairs[0][0]))\n",
        "    print(len(pairs[1][1]))\n",
        "    \n",
        "    # index language using the class defined above    \n",
        "    sylls = []\n",
        "    for parts in (hk.split(' ') for en, hk in pairs):\n",
        "        for word in parts:\n",
        "            for syll in syllables[word]:\n",
        "                sylls.append(syll)\n",
        "    print('sylls[0:3] {}'.format(sylls[0:3]))\n",
        "    print('# sylls: {}'.format(len(sylls)))\n",
        "    #targ_lang = LanguageIndex(enumerate(syllables[word] for word in (parts for parts in (hk.split(' ') for en, hk in pairs))))\n",
        "    targ_lang = LanguageIndex(sylls)\n",
        "    \n",
        "    print('Back[<start>]: {}'.format('<start>' in targ_lang.word2idx))\n",
        "    print('Back[B AE K]: {}'.format('B AE K' in targ_lang.word2idx))\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Raw input text, since that's what USE wants.\n",
        "    input_tensor = []\n",
        "    # Haiku lines\n",
        "    target_tensor = []\n",
        "    for i in range(len(pairs)):\n",
        "        hk = pairs[i][1]\n",
        "        syll_indexes = []\n",
        "        if i < 5:\n",
        "            print('haiku[{}]: {}'.format(i, hk))\n",
        "        syll_count = 0\n",
        "        for word in hk.split(' '):\n",
        "            #print('word[{}][0]: {}: '.format(i, word))\n",
        "            if word in syllables:\n",
        "                for syll in syllables[word]:\n",
        "                    # print('.  syll: ' + syll)\n",
        "                    if syll not in ['<start>','<end>', ',', '.']:\n",
        "                        syll_indexes.append(targ_lang.word2idx[syll])\n",
        "                        syll_count += 1\n",
        "            else:\n",
        "                print('text[{}]: word had no syllables: {}'.format(i, word))\n",
        "        if syll_count == 5:\n",
        "            input_tensor.append(pairs[i][0])\n",
        "            target_tensor.append(syll_indexes)\n",
        "        else:\n",
        "            print('wrong[{}] {}'.format(i, str(pairs[i][1])))\n",
        "            print('.....: ' + str(syll_indexes))\n",
        "                                                            \n",
        "    print(target_tensor[0])\n",
        "    \n",
        "    return input_tensor, target_tensor, targ_lang, 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTCeR2E15Yqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "syllables['<start>'] = ['<start>']\n",
        "syllables['<end>'] = ['<end>']\n",
        "syllables[','] = [',']\n",
        "syllables['.'] = ['.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "outputId": "7e6244e6-e2ad-451e-ab4f-60397d7635aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, targ_lang, max_length_targ = load_dataset(path_to_file, num_examples)\n",
        "num_syllables = len(targ_lang.idx2word)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "1000\n",
            "2\n",
            "37\n",
            "28\n",
            "sylls[0:3] ['<start>', 'B AE K', 'Y AA R D Z']\n",
            "# sylls: 7006\n",
            "Adding syllable: <start> as index 2\n",
            "Adding syllable: B AE K as index 39\n",
            "Back[<start>]: True\n",
            "Back[B AE K]: True\n",
            "haiku[0]: <start> backyards to stockyards <end>\n",
            "haiku[1]: <start> alameda street <end>\n",
            "haiku[2]: <start> alameda street <end>\n",
            "haiku[3]: <start> waiting for their ride <end>\n",
            "haiku[4]: <start> of toilet paper <end>\n",
            "wrong[575] <start> ready for use <end>\n",
            ".....: [462, 141, 170, 640]\n",
            "[40, 637, 595, 524, 637]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "79953759-1060-47ee-8d9e-0a763399df43"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print('{}, {}, {}, {}'.format(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)))\n",
        "print(target_tensor_train[0])\n",
        "# (x, 5, num_syllables)\n",
        "def fromdict_sparse(syllables, indexes, size=5):\n",
        "    tensor = np.zeros((len(indexes), size, num_syllables)) + 0.01\n",
        "    for i in range(len(indexes)):\n",
        "        for j in range(5):\n",
        "                tensor[i][j][indexes[i]] = 0.99\n",
        "    print(indexes[0])\n",
        "    print(tensor[0])\n",
        "    return tensor\n",
        "\n",
        "def fromdict(syllables, indexes, size=5):\n",
        "    tensor = np.zeros((len(indexes), size, 1)) + 0.01\n",
        "    for i in range(len(indexes)):\n",
        "        for j in range(5):\n",
        "            tensor[i][j][0] = indexes[i][j]\n",
        "    print(indexes[0])\n",
        "    print(tensor[0])\n",
        "    return tensor"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "799, 799, 200, 200\n",
            "[16, 105, 557, 166, 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "![alt text]### Create **model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "#BUFFER_SIZE = len(input_tensor_train)\n",
        "#BATCH_SIZE = 64\n",
        "#N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "tcn_embedding_dim = 256\n",
        "#units = 1024\n",
        "vocab_tar_size = len(targ_lang.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d06Q1yTNdAj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOlSpU9BdGwr",
        "colab_type": "code",
        "outputId": "39d6c28b-732a-4da6-a279-f5a68b9728bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "# size of embedding passed between Encoder and Decoder\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "print('USE embed size: {}'.format(embed_size))\n",
        "\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0613 03:14:52.325236 139994818779008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "USE embed size: 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avyJ_4VIUoHb",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  # keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)\n",
        "  if tf.test.is_gpu_available():\n",
        "    return layers.CuDNNGRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSGWKfUvc7Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e0cc55ee-b8b3-4cd5-d9ad-41039058315b"
      },
      "source": [
        "\n",
        "# slow\n",
        "num_epochs = 10\n",
        "adam_lr = 0.001\n",
        "adam_opt = Adam(lr=adam_lr)\n",
        "nadam_opt = tf.contrib.opt.NadamOptimizer(adam_lr)\n",
        "output_activation='sigmoid'\n",
        "dropout=0.5"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7uq0GEZ3MD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from compiled_tcn\n",
        "\n",
        "# https://github.com/keras-team/keras/pull/11373\n",
        "# It's now in Keras@master but still not available with pip.\n",
        "# TODO remove later.\n",
        "def accuracy(y_true, y_pred):\n",
        "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "            if K.ndim(y_true) == K.ndim(y_pred):\n",
        "                y_true = K.squeeze(y_true, -1)\n",
        "            # convert dense predictions to labels\n",
        "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "outputId": "bdfb06d8-ffc3-4369-d5f2-afe9e956de04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1704
        }
      },
      "source": [
        "\n",
        "# changed accuracy from 'choose your own accuracy'\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "embedding = layers.RepeatVector(5)(embedding)\n",
        "tcn = TCN(nb_filters=64, kernel_size=2, nb_stacks=1, dilations=[1,2,4,8], padding='same',\n",
        "            use_skip_connections=False, dropout_rate=0.05, return_sequences=True, name='tcn')(embedding)\n",
        "\n",
        "print('tcn.shape=', tcn.shape)\n",
        "output_layer = layers.Dense(vocab_tar_size)(tcn)\n",
        "output_layer = layers.Activation('softmax')(output_layer)\n",
        "\n",
        "model = Model(inputs=[input_text], outputs=output_layer)\n",
        "\n",
        "model.compile(nadam_opt, loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0613 03:14:54.837409 139994818779008 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0613 03:14:55.018860 139994818779008 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tcn.shape= (?, 5, 64)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "TF-Hub (Lambda)                 (None, 512)          0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 5, 512)       0           TF-Hub[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 5, 64)        32832       repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 5, 64)        8256        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 5, 64)        0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 5, 64)        0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 5, 64)        8256        spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 5, 64)        0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 5, 64)        4160        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2 (SpatialDro (None, 5, 64)        0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 5, 64)        0           conv1d_4[0][0]                   \n",
            "                                                                 spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 5, 64)        8256        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 5, 64)        0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_3 (SpatialDro (None, 5, 64)        0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 5, 64)        8256        spatial_dropout1d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 5, 64)        0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 5, 64)        4160        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_4 (SpatialDro (None, 5, 64)        0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 5, 64)        0           conv1d_7[0][0]                   \n",
            "                                                                 spatial_dropout1d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 5, 64)        8256        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 5, 64)        0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_5 (SpatialDro (None, 5, 64)        0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 5, 64)        8256        spatial_dropout1d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 5, 64)        0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 5, 64)        4160        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_6 (SpatialDro (None, 5, 64)        0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 5, 64)        0           conv1d_10[0][0]                  \n",
            "                                                                 spatial_dropout1d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 5, 64)        8256        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 5, 64)        0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_7 (SpatialDro (None, 5, 64)        0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 5, 64)        8256        spatial_dropout1d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 5, 64)        0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 5, 64)        4160        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_8 (SpatialDro (None, 5, 64)        0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 5, 64)        0           conv1d_13[0][0]                  \n",
            "                                                                 spatial_dropout1d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 5, 649)       42185       add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 5, 649)       0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 157,705\n",
            "Trainable params: 157,705\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSJAVTqa955",
        "colab_type": "code",
        "outputId": "790d4799-758b-4b22-e386-cf41a4dbdd56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    history = model.fit(np.array(input_tensor_train), \n",
        "            fromdict(syllables, target_tensor_train, 5),\n",
        "            #np.array(target_tensor_train),\n",
        "            validation_data=(np.array(input_tensor_val), fromdict(syllables, target_tensor_val, 5)),\n",
        "            #validation_data=(np.array(input_tensor_val), np.array(target_tensor_val)),\n",
        "            epochs=num_epochs,\n",
        "            callbacks = [EarlyStopping(patience=5)],\n",
        "            batch_size=32,\n",
        "            verbose=2\n",
        "    )\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 105, 557, 166, 103]\n",
            "[[ 16.]\n",
            " [105.]\n",
            " [557.]\n",
            " [166.]\n",
            " [103.]]\n",
            "[407, 394, 16, 78, 140]\n",
            "[[407.]\n",
            " [394.]\n",
            " [ 16.]\n",
            " [ 78.]\n",
            " [140.]]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0613 03:16:12.119290 139994818779008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0613 03:16:12.204053 139994818779008 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 799 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            " - 8s - loss: 5.4189 - accuracy: 0.1670 - val_loss: 4.9491 - val_accuracy: 0.1800\n",
            "Epoch 2/10\n",
            " - 1s - loss: 4.5511 - accuracy: 0.2133 - val_loss: 4.6903 - val_accuracy: 0.2110\n",
            "Epoch 3/10\n",
            " - 1s - loss: 4.1999 - accuracy: 0.2468 - val_loss: 4.5672 - val_accuracy: 0.2310\n",
            "Epoch 4/10\n",
            " - 1s - loss: 3.9591 - accuracy: 0.2578 - val_loss: 4.4128 - val_accuracy: 0.2340\n",
            "Epoch 5/10\n",
            " - 1s - loss: 3.7483 - accuracy: 0.2723 - val_loss: 4.2941 - val_accuracy: 0.2440\n",
            "Epoch 6/10\n",
            " - 1s - loss: 3.5765 - accuracy: 0.2826 - val_loss: 4.2394 - val_accuracy: 0.2650\n",
            "Epoch 7/10\n",
            " - 1s - loss: 3.4104 - accuracy: 0.2996 - val_loss: 4.2748 - val_accuracy: 0.2680\n",
            "Epoch 8/10\n",
            " - 1s - loss: 3.2616 - accuracy: 0.3169 - val_loss: 4.2040 - val_accuracy: 0.2820\n",
            "Epoch 9/10\n",
            " - 1s - loss: 3.1340 - accuracy: 0.3327 - val_loss: 4.1924 - val_accuracy: 0.3000\n",
            "Epoch 10/10\n",
            " - 1s - loss: 3.0361 - accuracy: 0.3379 - val_loss: 4.2040 - val_accuracy: 0.3160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-tPy_FTi_1O",
        "colab_type": "code",
        "outputId": "3bfb519e-05ac-477c-936f-a94ddf4fa932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  #model.load_weights('./model.h5')  \n",
        "  predicts = model.predict(np.array(input_tensor_train), batch_size=32)\n",
        "  print('shape: {}'.format(predicts.shape))\n",
        "\n",
        "print(len(predicts[0]))\n",
        "print(len(predicts[0][0]))\n",
        "print(predicts[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x7f527e2bf940>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1455, in __del__\n",
            "    self._session._session, self._handle, status)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\n",
            "    c_api.TF_GetCode(self.status.status))\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "shape: (799, 5, 649)\n",
            "5\n",
            "649\n",
            "[[0.00150616 0.00147162 0.00150401 ... 0.00160234 0.0015857  0.00154726]\n",
            " [0.00151585 0.00146712 0.00151706 ... 0.00158246 0.00158248 0.00154761]\n",
            " [0.00151213 0.00149036 0.00151312 ... 0.00159387 0.00155391 0.00157698]\n",
            " [0.00147679 0.00148687 0.00152369 ... 0.00156343 0.0015447  0.00154435]\n",
            " [0.00148117 0.00151662 0.00153047 ... 0.00157927 0.001538   0.00156293]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qy-mwJnkDl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "fb52aba5-61c1-4f96-c63b-783f0feadd70"
      },
      "source": [
        "def maxindx(pred):\n",
        "    maxi=0\n",
        "    maxv=pred[0]\n",
        "    for x in range(len(pred)-1):\n",
        "        if pred[x + 1] > maxv:\n",
        "            maxi = x + 1\n",
        "    return (maxi, maxv)\n",
        "\n",
        "for i in range(5):\n",
        "    (ind, v) = maxindx(predicts[0][i])\n",
        "    print('syll[{}] = {}, value={}'.format(i, ind, v))\n",
        "    print('... \"{}\"'.format(targ_lang.idx2word[ind]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "syll[0] = 648, value=0.0015061587328091264\n",
            "... \"ZH ER\"\n",
            "syll[1] = 648, value=0.0015158526366576552\n",
            "... \"ZH ER\"\n",
            "syll[2] = 648, value=0.0015121296746656299\n",
            "... \"ZH ER\"\n",
            "syll[3] = 648, value=0.0014767881948500872\n",
            "... \"ZH ER\"\n",
            "syll[4] = 648, value=0.0014811678556725383\n",
            "... \"ZH ER\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bngdTlpf3Sor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}