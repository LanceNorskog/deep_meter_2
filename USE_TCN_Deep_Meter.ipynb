{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "USE TCN Deep Meter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTkEiZP3STNJ",
        "colab_type": "text"
      },
      "source": [
        "Use Keras model, use TCN, reuse haiku output from attention tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cclkF0rYcCj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.layers as layers\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Nadam, Adam\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vgi-xMRb-Xv",
        "colab_type": "code",
        "outputId": "4a54c454-5c4c-4cb5-b965-85289e36c5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "!pip uninstall -y deepmeter\n",
        "!pip install git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "from cmu.syllables_cmu import syllables\n",
        "print(syllables['therefore'])\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling DeepMeter-0.0.1:\n",
            "  Successfully uninstalled DeepMeter-0.0.1\n",
            "Collecting deepmeter from git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
            "  Cloning https://github.com/LanceNorskog/deep_meter_2 to /tmp/pip-install-_jyh5gbn/deepmeter\n",
            "  Running command git clone -q https://github.com/LanceNorskog/deep_meter_2 /tmp/pip-install-_jyh5gbn/deepmeter\n",
            "Building wheels for collected packages: deepmeter\n",
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-c7hll74b/wheels/80/6c/4f/1c43367a928a82b45cb3d36b4f23720b249748714869e28f13\n",
            "Successfully built deepmeter\n",
            "Installing collected packages: deepmeter\n",
            "Successfully installed deepmeter-0.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cmu"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLqizwZYZ3Cs",
        "colab_type": "code",
        "outputId": "fceadb19-3842-46d7-99b1-4007380e8a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Mine\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/test_data/master/haiku_5.txt\n",
        "path_to_file = 'haiku_5.txt'\n",
        "!mkdir -p training_checkpoints\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "492dX33PaBYW",
        "colab_type": "text"
      },
      "source": [
        "Load Haiku data. Filter for bogus inputs. Create 2-directional map of known syllables, and matching arrays of ['A sentence for you',...] and syllables indexes of [['AH', 'S EH N', 'T EH NS', 'F OR', 'YU']]. Use CMU Pronunciation Dictionary for syllables (some guy's syllabized version)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5__pr1y6YJxu",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hh52qpUdYJV9",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [Long clause, Haiku line]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "    \n",
        "    return word_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9xbqO7Iie9bb",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "  def __init__(self, lang):\n",
        "    self.lang = lang\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.vocab = set()\n",
        "    \n",
        "    self.create_index()\n",
        "    \n",
        "  def create_index(self):\n",
        "    for word in self.lang:\n",
        "        #print(type(word))\n",
        "        #print(word)\n",
        "        if type(word) == type(''):\n",
        "            #if not word in self.vocab:\n",
        "             #   print('. adding: {}'.format(word))\n",
        "            #print(word)\n",
        "            self.vocab.update([word])\n",
        "        else:\n",
        "            for part in word:\n",
        "               #if not part in self.vocab:\n",
        "               #    print('. adding: {}'.format(part))\n",
        "               self.vocab.update([part])\n",
        "    \n",
        "    self.vocab = sorted(self.vocab)\n",
        "    \n",
        "    self.word2idx['<pad>'] = 0\n",
        "    for index, word in enumerate(self.vocab):\n",
        "      #if index < 20:\n",
        "      #    print('Vocab[{}]: \"{}\"'.format(index, word))\n",
        "      if word in ['<start>', 'B AE K', 'big']:\n",
        "           print(\"Adding syllable: {} as index {}\".format(word, index))\n",
        "      self.word2idx[word] = index + 1\n",
        "    \n",
        "    for word, index in self.word2idx.items():\n",
        "      self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples):\n",
        "    # creating cleaned input, output pairs\n",
        "    pairs = create_dataset(path, num_examples)\n",
        "    print(type(pairs))\n",
        "    print(len(pairs))\n",
        "    print(len(pairs[0]))\n",
        "    print(len(pairs[0][0]))\n",
        "    print(len(pairs[1][1]))\n",
        "    \n",
        "    # index language using the class defined above    \n",
        "    sylls = []\n",
        "    for parts in (hk.split(' ') for en, hk in pairs):\n",
        "        for word in parts:\n",
        "            for syll in syllables[word]:\n",
        "                sylls.append(syll)\n",
        "    print('sylls[0:3] {}'.format(sylls[0:3]))\n",
        "    print('# sylls: {}'.format(len(sylls)))\n",
        "    #targ_lang = LanguageIndex(enumerate(syllables[word] for word in (parts for parts in (hk.split(' ') for en, hk in pairs))))\n",
        "    targ_lang = LanguageIndex(sylls)\n",
        "    \n",
        "    print('Back[<start>]: {}'.format('<start>' in targ_lang.word2idx))\n",
        "    print('Back[B AE K]: {}'.format('B AE K' in targ_lang.word2idx))\n",
        "    \n",
        "    # Vectorize the input and target languages\n",
        "    \n",
        "    # Raw input text, since that's what USE wants.\n",
        "    input_tensor = []\n",
        "    # Haiku lines\n",
        "    target_tensor = []\n",
        "    for i in range(len(pairs)):\n",
        "        hk = pairs[i][1]\n",
        "        syll_indexes = []\n",
        "        if i < 5:\n",
        "            print('haiku[{}]: {}'.format(i, hk))\n",
        "        syll_count = 0\n",
        "        for word in hk.split(' '):\n",
        "            #print('word[{}][0]: {}: '.format(i, word))\n",
        "            if word in syllables:\n",
        "                for syll in syllables[word]:\n",
        "                    # print('.  syll: ' + syll)\n",
        "                    syll_indexes.append(targ_lang.word2idx[syll])\n",
        "                    if syll not in ['<start>','<end>', ',', '.']:\n",
        "                        syll_count += 1\n",
        "            else:\n",
        "                print('text[{}]: word had no syllables: {}'.format(i, word))\n",
        "        if syll_count == 5:\n",
        "            input_tensor.append(pairs[i][0])\n",
        "            target_tensor.append(syll_indexes)\n",
        "        else:\n",
        "            print('wrong[{}] {}'.format(i, str(pairs[i][1])))\n",
        "            print('.....: ' + str(syll_indexes))\n",
        "                                                            \n",
        "    print(target_tensor[0])\n",
        "    \n",
        "    return input_tensor, target_tensor, targ_lang, 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTCeR2E15Yqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "syllables['<start>'] = ['<start>']\n",
        "syllables['<end>'] = ['<end>']\n",
        "syllables[','] = [',']\n",
        "syllables['.'] = ['.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "outputId": "0088ff01-8c6a-41f1-a74b-01e60e45010c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1000\n",
        "input_tensor, target_tensor, targ_lang, max_length_targ = load_dataset(path_to_file, num_examples)\n",
        "num_syllables = len(targ_lang.idx2word)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "1000\n",
            "2\n",
            "37\n",
            "28\n",
            "sylls[0:3] ['<start>', 'B AE K', 'Y AA R D Z']\n",
            "# sylls: 7006\n",
            "Adding syllable: <start> as index 2\n",
            "Adding syllable: B AE K as index 39\n",
            "Back[<start>]: True\n",
            "Back[B AE K]: True\n",
            "haiku[0]: <start> backyards to stockyards <end>\n",
            "haiku[1]: <start> alameda street <end>\n",
            "haiku[2]: <start> alameda street <end>\n",
            "haiku[3]: <start> waiting for their ride <end>\n",
            "haiku[4]: <start> of toilet paper <end>\n",
            "wrong[575] <start> ready for use <end>\n",
            ".....: [3, 462, 141, 170, 640, 2]\n",
            "[3, 40, 637, 595, 524, 637, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "colab": {}
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n",
        "def fromdict(syllables, indexes, size=5):\n",
        "    tensor = np.zeros((len(indexes), num_syllables * size)) + 0.01\n",
        "    for i in range(len(indexes)):\n",
        "        for j in range(size):\n",
        "            tensor[i][indexes[j]] = 0.99\n",
        "    print(indexes[0])\n",
        "    print(tensor[0])\n",
        "    return tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "![alt text]### Create **model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "#BUFFER_SIZE = len(input_tensor_train)\n",
        "#BATCH_SIZE = 64\n",
        "#N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "tcn_embedding_dim = 256\n",
        "#units = 1024\n",
        "vocab_tar_size = len(targ_lang.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d06Q1yTNdAj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOlSpU9BdGwr",
        "colab_type": "code",
        "outputId": "08d1b44d-45ba-4406-f6be-119bb3465a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "# size of embedding passed between Encoder and Decoder\n",
        "embed_size = embed.get_output_info_dict()['default'].get_shape()[1].value\n",
        "print('USE embed size: {}'.format(embed_size))\n",
        "\n",
        "def UniversalEmbedding(x):\n",
        "    return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USE embed size: 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avyJ_4VIUoHb",
        "colab": {}
      },
      "source": [
        "def gru(units):\n",
        "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
        "  # the code automatically does that.\n",
        "  # keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False, stateful=False)\n",
        "  if tf.test.is_gpu_available():\n",
        "    return layers.CuDNNGRU(units, \n",
        "                                    return_sequences=True, \n",
        "                                    return_state=True, \n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "  else:\n",
        "    return layers.GRU(units, \n",
        "                               return_sequences=True, \n",
        "                               return_state=True, \n",
        "                               recurrent_activation='sigmoid', \n",
        "                               recurrent_initializer='glorot_uniform')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSGWKfUvc7Qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# slow\n",
        "num_epochs = 10\n",
        "adam_lr = 0.001\n",
        "adam_opt = Adam(lr=adam_lr)\n",
        "nadam_opt = tf.contrib.opt.NadamOptimizer(adam_lr)\n",
        "output_activation='sigmoid'\n",
        "dropout=0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "6e665c15-8309-4d05-8ce7-824a579c11b9"
      },
      "source": [
        "\n",
        "# changed accuracy from 'choose your own accuracy'\n",
        "input_text = layers.Input(shape=(1,), dtype=tf.string)\n",
        "embedding = layers.Lambda(UniversalEmbedding, output_shape=(embed_size,), name='TF-Hub')(input_text)\n",
        "embedding = layers.Dropout(0.5)(embedding)\n",
        "dense = layers.Dense(1024, activation='relu', name='Convoluted')(embedding)\n",
        "dense = layers.Dropout(0.5)(dense)\n",
        "pred = layers.Dense(5 * vocab_tar_size, activation=output_activation, name='Flatout')(dense)\n",
        "model = Model(inputs=[input_text], outputs=pred)\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=nadam_opt, \n",
        "              metrics=['binary_crossentropy']\n",
        "             )\n",
        "model.summary()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0612 02:29:48.620183 139867708565376 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "TF-Hub (Lambda)              (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "Convoluted (Dense)           (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "Flatout (Dense)              (None, 3245)              3326125   \n",
            "=================================================================\n",
            "Total params: 3,851,437\n",
            "Trainable params: 3,851,437\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELSJAVTqa955",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "e6bef3e9-e67e-4246-d54d-e6cdafd5e0c8"
      },
      "source": [
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists('./model.h5'):\n",
        "  with tf.Session() as session:\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    history = model.fit(np.array(input_tensor_train), \n",
        "            fromdict(syllables, target_tensor_train, 5),\n",
        "            validation_data=(np.array(input_tensor_val), fromdict(syllables, target_tensor_val, 5)),\n",
        "            epochs=num_epochs,\n",
        "            callbacks = [EarlyStopping(patience=5)],\n",
        "            batch_size=32,\n",
        "            verbose=2\n",
        "    )\n",
        "    model.save_weights('./model.h5')\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 24, 16, 497, 583, 334, 2]\n",
            "[0.01 0.01 0.99 ... 0.01 0.01 0.01]\n",
            "[3, 16, 58, 494, 286, 530, 2]\n",
            "[0.01 0.01 0.99 ... 0.01 0.01 0.01]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0612 02:29:59.328920 139867708565376 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 799 samples, validate on 200 samples\n",
            "Epoch 1/10\n",
            " - 11s - loss: 0.2716 - binary_crossentropy: 0.2716 - val_loss: 0.1072 - val_binary_crossentropy: 0.1072\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.0645 - binary_crossentropy: 0.0645 - val_loss: 0.1120 - val_binary_crossentropy: 0.1120\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.0596 - binary_crossentropy: 0.0596 - val_loss: 0.0977 - val_binary_crossentropy: 0.0977\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.0582 - binary_crossentropy: 0.0582 - val_loss: 0.0979 - val_binary_crossentropy: 0.0979\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.0581 - binary_crossentropy: 0.0581 - val_loss: 0.0978 - val_binary_crossentropy: 0.0978\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.0576 - binary_crossentropy: 0.0576 - val_loss: 0.0977 - val_binary_crossentropy: 0.0977\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.0578 - binary_crossentropy: 0.0578 - val_loss: 0.0977 - val_binary_crossentropy: 0.0977\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0575 - binary_crossentropy: 0.0575 - val_loss: 0.0975 - val_binary_crossentropy: 0.0975\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0575 - binary_crossentropy: 0.0575 - val_loss: 0.0978 - val_binary_crossentropy: 0.0978\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0573 - binary_crossentropy: 0.0573 - val_loss: 0.0976 - val_binary_crossentropy: 0.0976\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}