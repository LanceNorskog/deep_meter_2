{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZHG 2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRrnHfTpkale",
        "colab_type": "text"
      },
      "source": [
        "Use Cyber-ZHG's various Keras tools for NLP: Attention, Multi-head, etc. \n",
        "Use external model manager to allow freezing weights of basic model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsSmrtK06JSh",
        "outputId": "8cea2bc2-c5d1-47d7-86d6-91a5d82618c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#!pip install numpy==1.16.1\n",
        "!pip install keras==2.2.3\n",
        "!pip install keras-multi-head keras-pos-embd\n",
        "!wget -nc https://raw.githubusercontent.com/LanceNorskog/deep_meter_2/master/haiku_5.txt\n",
        "!cut -f2 < haiku_5.txt | sort | uniq > haiku_5_short.txt\n",
        "!wc -l haiku_5*.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: keras==2.2.3 in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.3) (1.0.8)\n",
            "Requirement already satisfied: keras-multi-head in /usr/local/lib/python3.6/dist-packages (0.20.0)\n",
            "Requirement already satisfied: keras-pos-embd in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (2.2.3)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (0.41.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-multi-head) (1.16.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-multi-head) (2.8.0)\n",
            "File ‘haiku_5.txt’ already there; not retrieving.\n",
            "\n",
            "   95631 haiku_5_short.txt\n",
            "  673680 haiku_5.txt\n",
            "  769311 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R1VL5d-HESw",
        "colab_type": "code",
        "outputId": "e2d1b93a-95f5-4f07-bfaa-a15c831fe1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip uninstall -qy git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n",
        "!pip install -q git+https://github.com/LanceNorskog/deep_meter_2#egg=deepmeter\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for deepmeter (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cih9auaZbpH",
        "colab_type": "code",
        "outputId": "73f0c910-69a0-4549-c48e-9217f5ec40f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import math\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from keras import layers \n",
        "from keras import metrics\n",
        "from keras.preprocessing import text\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from cmu.syllables_cmu import syllables as word2sylls\n",
        "from cmu.mappers import Decoder, trim_homynyms\n",
        "from cmu.full import FullSearch\n",
        "from cmu.topk import get_top_k, decodem, short_sentences\n",
        "from cmu.wordmap import Wordmap\n",
        "from cmu.readhaiku import Reader\n",
        "\n",
        "from keras_multi_head import MultiHeadAttention\n",
        "from keras_pos_embd import PositionEmbedding\n",
        "\n",
        "#from cmu.report import find_top_k_match, report\n",
        "from keras_stuff.loss import sparse_categorical_crossentropy as scc\n",
        "#from keras_stuff.loss import sparse_categorical_crossentropy_temporal as scct\n",
        "import keras_stuff.metrics as my_metrics\n",
        "from keras_stuff.modelmgr import ModelManager\n",
        "\n",
        "print(word2sylls['therefore'])\n",
        "\n",
        "# number of total samples to use\n",
        "max_data = 100000\n",
        "# number of words for hashing trick\n",
        "hash_mole = 20000\n",
        "# number of output syllables in short haiku\n",
        "max_features = 17000\n",
        "# longest output sentence\n",
        "num_sylls = 5\n",
        "# longest input sentence\n",
        "max_words = 10\n",
        "# what you think\n",
        "batch_size = 32\n",
        "# do not output the same haiku twice\n",
        "deduplicate_haiku=False\n",
        "# emit output as input\n",
        "duplicate_haiku=True\n",
        "# use long as input\n",
        "use_big_text=True\n",
        "\n",
        "model_base_file=\"/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_base_5.h5\"\n",
        "model_file=\"/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_5.h5\"\n",
        "print(model_file)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['DH EH R', 'F AO R']\n",
            "/content/gdrive/My Drive/Colab Notebooks/haiku_zhg_5.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JSlaFFPiT5w",
        "colab_type": "code",
        "outputId": "532706e2-e5fa-4a65-e7e6-5f0a0817e1d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!date\n",
        "print(word2sylls['door'])\n",
        "#word2sylls = trim_homynyms(word2sylls)\n",
        "print(word2sylls['door'])\n",
        "decoder = Decoder(word2sylls)\n",
        "syll2idx = decoder.syll2idx\n",
        "idx2syll = decoder.idx2syll\n",
        "\n",
        "print(syll2idx['DH EH R'], idx2syll[1])\n",
        "print('# features: ', len(idx2syll))\n",
        "\n",
        "for i in range(decoder.wordoff):\n",
        "    decoder.wordlist[i] = 'word{}'.format(i)\n",
        "    decoder.wordlength[i] = 1\n",
        "for i in range(decoder.sylloff):\n",
        "    decoder.idx2syll[i] = 'syll{}'.format(i)\n",
        "\n",
        "big_haiku_file = \"haiku_5.txt\"\n",
        "wordmap = Wordmap(len(decoder.wordlist))\n",
        "reader = Reader(word2sylls, decoder, wordmap)\n",
        "(big_text, big_haiku, big_data) = reader.readfile(big_haiku_file, max_words=max_words, \n",
        "    deduplicate_haiku=deduplicate_haiku, duplicate_haiku=duplicate_haiku, max_data=max_data)\n",
        "if use_big_text:\n",
        "    input_text = big_text\n",
        "else:\n",
        "    input_text = big_haiku\n",
        "big_hash = reader.gethash(input_text, max_words=max_words, hash_mole=hash_mole)\n",
        "haikuwordset = reader.haikuwordset\n",
        "print('{} -> {} : {}'.format(big_text[0], big_haiku[0], big_data[0]))\n",
        "\n",
        "print('Full length clauses: ', len(big_text))\n",
        "print('Wordmap total entries: ', wordmap.count())\n",
        "print('Wordmap length: ', wordmap.length())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Aug  1 03:10:42 UTC 2019\n",
            "['D AO R']\n",
            "['D AO R']\n",
            "2443 0\n",
            "# features:  15098\n",
            "a white sink and door -> a white sink and door : [[  156]\n",
            " [14238]\n",
            " [10115]\n",
            " [  125]\n",
            " [ 1844]]\n",
            "Full length clauses:  100001\n",
            "Wordmap total entries:  12523\n",
            "Wordmap length:  229463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6ffyQDaP4ot",
        "colab_type": "code",
        "outputId": "36ebd63b-093f-4dda-b93b-a6877cabbdcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Split multiple datasets across same index\n",
        "(train_i, test_i, _, _) = train_test_split(np.arange(len(big_data)), np.arange(len(big_data)))\n",
        "\n",
        "train_len=(len(train_i)//batch_size) * batch_size\n",
        "test_len=(len(test_i)//batch_size) * batch_size\n",
        "x_train = big_hash[train_i][:train_len]\n",
        "y_train = big_data[train_i][:train_len]\n",
        "x_test = big_hash[test_i][-test_len:]\n",
        "y_test = big_data[test_i][-test_len:]\n",
        "\n",
        "print(input_text[train_i[0]], x_train[0], str(y_test[0]))\n",
        "\n",
        "def get_lstm(size, return_sequences=True):\n",
        "    #return layers.LSTM(size, return_sequences=return_sequences)\n",
        "    return layers.CuDNNLSTM(size, return_sequences=return_sequences)\n",
        "\n",
        "#x_train = np.array(x_train)\n",
        "#x_test = np.array(x_test)\n",
        "#y_train = np.expand_dims(y_train, -1)\n",
        "#y_test = np.expand_dims(y_test, -1)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "print(y_test[0][0])\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "into a dog bed [10706.   309.  7877.   151.     0.     0.     0.     0.     0.     0.] [[ 8119]\n",
            " [ 2474]\n",
            " [13837]\n",
            " [ 4137]\n",
            " [13221]]\n",
            "x_train shape: (74976, 10)\n",
            "x_test shape: (24992, 10)\n",
            "y_train shape: (74976, 5, 1)\n",
            "y_test shape: (24992, 5, 1)\n",
            "[8119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWdU04vHTE61",
        "colab_type": "code",
        "outputId": "cb980d28-2a12-45d8-e4a9-e9a7fae7dc08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "modelmgr = ModelManager()\n",
        "\n",
        "embed_size=512\n",
        "units_k=embed_size\n",
        "units_v=embed_size\n",
        "units_v=embed_size//3\n",
        "units=512\n",
        "dropout=0.5\n",
        "\n",
        "model_params = {\n",
        "    'max_words':max_words , 'hash_mole': hash_mole, \n",
        "    'embed_size': embed_size, 'units': units, \n",
        "    'num_sylls': num_sylls, 'max_features': max_features\n",
        "    }\n",
        "\n",
        "metric_list = [my_metrics.sparse, my_metrics.perfect]\n",
        "metric_names = ['sparse', 'perfect']\n",
        "\n",
        "model = modelmgr.get_model(model_params, f=True)\n",
        "modelmgr.load_weights(model, model_base_file, freeze=True)\n",
        "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "\n",
        "print('Train...')\n",
        "history = None\n",
        "use_saved_model=False\n",
        "if not use_saved_model or not os.path.exists(model_base_file):\n",
        "  with tf.Session() as session:\n",
        "    K.manual_variable_initialization(False)\n",
        "    K.set_session(session)\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    if os.path.exists(model_base_file):\n",
        "        print(\"Reloading weights\")\n",
        "        model.load_weights(model_base_file, by_name=True)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=200,\n",
        "          callbacks=[EarlyStopping(monitor='val_perfect', mode='max', verbose=1, patience=10),\n",
        "            ModelCheckpoint(model_file, monitor='val_perfect', save_best_only=True, save_weights_only=True, mode='max', verbose=1)],\n",
        "          verbose=2,\n",
        "          validation_data=[x_test, y_test])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0801 03:10:50.213036 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0801 03:10:50.232673 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0801 03:10:50.236236 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0801 03:10:50.253185 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0801 03:10:50.264397 139794366359424 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0801 03:10:52.383780 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0801 03:10:53.588086 139794366359424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Layer name not found:  input_1\n",
            "Freezing layer:  embedding_1\n",
            "Layer name not found:  dropout_1\n",
            "Freezing layer:  bidirectional_1\n",
            "Layer name not found:  dropout_2\n",
            "Layer name not found:  repeat_vector_1\n",
            "Layer name not found:  dropout_3\n",
            "Freezing layer:  cu_dnnlstm_2\n",
            "Layer name not found:  dropout_4\n",
            "Layer name not found:  multi_head_attention_1\n",
            "Layer name not found:  dropout_5\n",
            "Freezing layer:  dense_1\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 512)           10240000  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1576960   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 5, 512)            2101248   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "multi_head_attention_1 (Mult (None, 5, 512)            1050624   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5, 17000)          8721000   \n",
            "=================================================================\n",
            "Total params: 23,689,832\n",
            "Trainable params: 1,050,624\n",
            "Non-trainable params: 22,639,208\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Reloading weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0801 03:10:54.133939 139794366359424 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 74976 samples, validate on 24992 samples\n",
            "Epoch 1/200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHy1cTDVP_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure()\n",
        "if history != None:\n",
        "  # summarize history for accuracy\n",
        "  for m in metric_names:\n",
        "      #plt.plot(history.history[m])\n",
        "      plt.plot(history.history['val_' + m])\n",
        "  plt.title('model accuracy (dropout={})'.format(dropout))\n",
        "  plt.xlabel('epoch')\n",
        "  sname = []\n",
        "  for m in metric_names:\n",
        "      sname.append('{}={:01.3f}'.format(m, history.history['val_' + m][-1]))\n",
        "  plt.legend(sname, loc='lower right')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCTpMmewvKjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  print('x_test.shape ', x_test.shape)\n",
        "  print('y_text.shape ', y_test.shape)\n",
        "  eval_small = model.evaluate(x_test, y_test)\n",
        "  print('model.evaluate on test data: ' ,model.metrics_names, eval_small)\n",
        "  print('history: ', history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PxN1Tm8gsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_top_k_match(data, prediction, top_k=5):\n",
        "        out = [-1] * len(data)\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            topind = topind[-top_k:]\n",
        "            for j in range(top_k):\n",
        "                #print(data[i][0], topind[j])\n",
        "                if data[i][0] == topind[j]:\n",
        "                    out[i] = topind[j]\n",
        "        return out\n",
        "    \n",
        "def report(data, prediction):\n",
        "    def match(data, prediction):\n",
        "        assert len(data.shape) == 2\n",
        "        assert len(prediction.shape) == 2\n",
        "        good = 0\n",
        "        top5 = 0\n",
        "        count = 0\n",
        "        for i in range(len(data)):\n",
        "            topind = np.argsort(prediction[i])\n",
        "            if data[i][0] == topind[-1]:\n",
        "                good += 1\n",
        "            topind = topind[-5:len(topind)]\n",
        "            for j in range(5):\n",
        "                if data[i][0] == topind[j]:\n",
        "                    top5 += 1\n",
        "                    break\n",
        "            count += 1\n",
        "        return (good, top5, count)\n",
        "\n",
        "    _sparse = 0.0\n",
        "    _perfect = 0.0\n",
        "    _sparse5 = 0.0\n",
        "    _perfect5 = 0.0\n",
        "    _total = 0\n",
        "    for n in range(len(data)):\n",
        "        #print(len(short[n]))\n",
        "        (good, top5, count) = match(data[n], predicts[n])\n",
        "        if count == 0:\n",
        "            continue\n",
        "        _sparse += good/count\n",
        "        _sparse5 += top5/count\n",
        "        if good == count:\n",
        "            _perfect += 1  \n",
        "        if top5 == count:\n",
        "            _perfect5 += 1\n",
        "        _total += 1\n",
        "    return {'sparse':_sparse/_total, 'perfect': _perfect/_total, 'sparse5': _sparse5/_total, 'perfect5': _perfect5/_total}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG3xh-E9HqfX",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "top_k=2\n",
        "\n",
        "try_indx=x_test\n",
        "try_text=input_text[test_i]\n",
        "try_indx=x_train\n",
        "try_text=input_text[train_i]\n",
        "   \n",
        "bigbatch = batch_size * 32\n",
        "with tf.Session() as session:\n",
        "  K.set_session(session)\n",
        "  session.run(tf.global_variables_initializer())\n",
        "  session.run(tf.tables_initializer())\n",
        "  model.load_weights(model_file)  \n",
        "  biglen = len(x_test)\n",
        "  for i in range(0, biglen, bigbatch):\n",
        "      predicts = model.predict(try_indx[i:i + bigbatch], batch_size=bigbatch)\n",
        "      for j in range(0, len(predicts)):\n",
        "          #f = find_top_k_match(y_test[i + j], predicts[j], 5)\n",
        "          #if np.min(f) > 0 and j == 0:\n",
        "          #    print('{} -> {}'.format(x_test[i + j], [decoder.idx2syll[k] for k in f]))\n",
        "          fs = FullSearch(num_sylls * 5, num_sylls, top_k)\n",
        "          (top_vals, top_paths) = get_top_k(predicts[j], top_k=top_k)\n",
        "          fs.mainloop(top_paths)\n",
        "          sentences = decodem(fs.scorepaths, top_paths, decoder, haikuwordset, wordmap)\n",
        "          if len(sentences) > 0:\n",
        "              for s in short_sentences(sentences, num_sylls):\n",
        "                    print('{} -> {}'.format(try_text[i + j], s))\n",
        "              #print('{} -> {}'.format(x_test[i + j], sentences[0]))\n",
        "              #for k in range(1, len(sentences)):\n",
        "              #      print('. -> {}'.format(sentences[k]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDSA8FtUUyju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}